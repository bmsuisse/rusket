"""Realistic 1B row benchmark: bootstrap-sample real-world transaction data.

Downloads two real-world basket datasets, learns their item frequency
distributions, then generates long-format (txn_id, item_id) arrays that
match the real item co-occurrence rates.  Feeds them to FPMiner chunk by
chunk — no dataset larger than one chunk ever enters Python RAM at once.

Usage:
    uv run python benchmarks/bench_fpminer_realistic.py

Targets: 300M → 500M → 800M → 1B rows
"""

from __future__ import annotations

import gc
import time
import urllib.request
from pathlib import Path

import numpy as np

from rusket import FPMiner

# ---------------------------------------------------------------------------
# Dataset download
# ---------------------------------------------------------------------------

DATA_DIR = Path(__file__).resolve().parent / "data"

DATASETS: dict[str, dict] = {
    "andi_data": {
        "url": "https://raw.githubusercontent.com/andi611/Apriori-and-Eclat-Frequent-Itemset-Mining/master/data/data.txt",
        "file": "andi_data.txt",
        "min_support": 0.02,
        "max_len": 3,
    },
    "andi_data2": {
        "url": "https://raw.githubusercontent.com/andi611/Apriori-and-Eclat-Frequent-Itemset-Mining/master/data/data2.txt",
        "file": "andi_data2.txt",
        "min_support": 0.02,
        "max_len": 3,
    },
}


def download_if_missing(name: str) -> Path:
    info = DATASETS[name]
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    path = DATA_DIR / info["file"]
    if not path.exists():
        print(f"  ⬇  Downloading {name} …", flush=True)
        urllib.request.urlretrieve(info["url"], path)
    return path


def load_transactions(path: Path) -> tuple[np.ndarray, np.ndarray, int]:
    """Parse txt basket file → integer (txn_id, item_id) arrays + n_items."""
    transactions: list[list[int]] = []
    item_to_idx: dict[str, int] = {}

    with open(path) as f:
        for line in f:
            items = line.strip().split()
            if not items:
                continue
            row = []
            for item in items:
                if item not in item_to_idx:
                    item_to_idx[item] = len(item_to_idx)
                row.append(item_to_idx[item])
            transactions.append(row)

    n_items = len(item_to_idx)
    txn_ids_list: list[int] = []
    item_ids_list: list[int] = []
    for tx_idx, items in enumerate(transactions):
        for item in items:
            txn_ids_list.append(tx_idx)
            item_ids_list.append(item)

    return (
        np.array(txn_ids_list, dtype=np.int64),
        np.array(item_ids_list, dtype=np.int32),
        n_items,
    )


def compute_item_probs(
    txn_ids: np.ndarray, item_ids: np.ndarray, n_items: int, n_txns: int
) -> np.ndarray:
    """Compute per-item occurrence probability from real data."""
    counts = np.bincount(item_ids, minlength=n_items).astype(np.float64)
    # Probability that a given item appears in a transaction
    probs = counts / n_txns
    return probs


# ---------------------------------------------------------------------------
# Bootstrap chunk generator
# ---------------------------------------------------------------------------


def generate_chunk(
    rng: np.random.Generator,
    item_probs: np.ndarray,
    avg_items_per_txn: float,
    n_items: int,
    chunk_txns: int,
    txn_id_offset: int,
) -> tuple[np.ndarray, np.ndarray]:
    """Generate one chunk of realistic (txn_id, item_id) pairs.

    Each synthetic transaction is generated by sampling items independently
    with their real-world occurrence probabilities.  Average basket size
    matches the real dataset.
    """
    # Draw basket sizes from Poisson(avg_items_per_txn)
    basket_sizes = rng.poisson(avg_items_per_txn, size=chunk_txns)
    total_pairs = int(basket_sizes.sum())

    # Repeat txn_id for each item in the basket
    txn_ids = np.repeat(
        np.arange(txn_id_offset, txn_id_offset + chunk_txns, dtype=np.int64),
        basket_sizes,
    )
    # Sample items with real-world frequency weights
    item_ids = rng.choice(n_items, size=total_pairs, p=item_probs).astype(np.int32)

    return txn_ids, item_ids


# ---------------------------------------------------------------------------
# Main benchmark
# ---------------------------------------------------------------------------

TARGETS = [300_000_000, 500_000_000, 800_000_000, 1_000_000_000]
CHUNK_TXNS = 500_000  # ~500k transactions per chunk — memory safe


def run_dataset(name: str) -> None:
    info = DATASETS[name]
    path = download_if_missing(name)

    print(f"\n{'=' * 70}")
    print(f"  Dataset: {name}")
    txn_ids_real, item_ids_real, n_items = load_transactions(path)
    n_txns_real = int(txn_ids_real.max()) + 1
    avg_items = len(txn_ids_real) / n_txns_real
    item_probs = compute_item_probs(txn_ids_real, item_ids_real, n_items, n_txns_real)
    print(
        f"  Real: {n_txns_real:,} txns × {n_items} items, avg {avg_items:.1f} items/txn"
    )
    print(f"  min_support={info['min_support']}, max_len={info['max_len']}")
    print(f"{'=' * 70}")
    del txn_ids_real, item_ids_real

    print(
        f"\n  {'target_rows':>14}  {'add_t':>8}  {'mine_t':>8}  {'total':>8}  {'itemsets':>10}"
    )

    for target_rows in TARGETS:
        # Derive target transaction count from target rows
        target_txns = int(target_rows / avg_items)
        rng = np.random.default_rng(42)
        miner = FPMiner(n_items=n_items)

        t_add_start = time.perf_counter()
        txn_offset = 0
        while txn_offset < target_txns:
            chunk_size = min(CHUNK_TXNS, target_txns - txn_offset)
            txn_ids, item_ids = generate_chunk(
                rng, item_probs, avg_items, n_items, chunk_size, txn_offset
            )
            miner.add_chunk(txn_ids, item_ids)
            txn_offset += chunk_size
            del txn_ids, item_ids
            gc.collect()
        add_t = time.perf_counter() - t_add_start
        actual_rows = miner.n_rows

        t0 = time.perf_counter()
        try:
            freq = miner.mine(
                min_support=info["min_support"],
                max_len=info["max_len"],
            )
            mine_t = time.perf_counter() - t0
            print(
                f"  {actual_rows:>14,}  {add_t:>7.1f}s  {mine_t:>7.1f}s  "
                f"{add_t + mine_t:>7.1f}s  {len(freq):>10,}",
                flush=True,
            )
        except Exception as e:
            print(f"  {actual_rows:>14,}  {add_t:>7.1f}s  ERROR: {e}", flush=True)

        del miner
        gc.collect()


def main() -> None:
    print("=" * 70)
    print("  FPMiner — Realistic scale benchmark (bootstrap from real data)")
    print("  Strategy: per-chunk sort + k-way merge (memory safe at 1B rows)")
    print("=" * 70)
    for name in DATASETS:
        run_dataset(name)
    print("\n✅ Done!")


if __name__ == "__main__":
    main()
