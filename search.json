{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"rusket","text":"","path":["Getting Started","rusket"],"tags":[]},{"location":"#rusket","level":1,"title":"rusket","text":"<p>Ultra-fast Recommender Engines &amp; Market Basket Analysis for Python, written in Rust.</p> <p>Made with ❤️ by the Data &amp; AI Team.</p> <p> </p>","path":["Getting Started","rusket"],"tags":[]},{"location":"#what-is-rusket","level":2,"title":"What is rusket?","text":"<p><code>rusket</code> turns raw transaction logs into revenue intelligence — \"frequently bought together\" rules, personalised recommendations, high-profit bundle discovery, and sequential customer journey analysis.</p> <p>The core algorithms run entirely in Rust (via PyO3) and accept Pandas, Polars, and Spark DataFrames natively with zero-copy Arrow transfers:</p> Input Rust path Notes Dense pandas DataFrame <code>fpgrowth_from_dense</code> Flat <code>uint8</code> buffer — zero-copy Sparse pandas DataFrame <code>fpgrowth_from_csr</code> Raw CSR arrays — zero-copy Polars DataFrame <code>fpgrowth_from_dense</code> Arrow-backed <code>numpy</code> buffer","path":["Getting Started","rusket"],"tags":[]},{"location":"#why-rusket","level":2,"title":"Why rusket?","text":"<p>Zero runtime dependencies. No TensorFlow, no PyTorch, no JVM — just <code>pip install rusket</code>. The entire engine is compiled Rust (~3 MB wheel).</p> Feature rusket LibRecommender Runtime deps 0 TF + PyTorch + gensim (~2 GB) ALS fit (ML-100k) 427 ms 1,324 ms (3.1× slower) BPR fit (ML-100k) 33 ms 681 ms (20.4× slower) ItemKNN fit (ML-100k) 55 ms 287 ms (5.2× slower) Polars / Spark support ✅ / ✅ ❌ / ❌ Pattern Mining FP-Growth, Eclat, HUPM, PrefixSpan ❌ <p>Benchmarks: <code>pytest-benchmark</code>, 5 rounds, warmed up, GC disabled. MovieLens 100k.</p>","path":["Getting Started","rusket"],"tags":[]},{"location":"#quick-example-frequently-bought-together","level":2,"title":"Quick Example — \"Frequently Bought Together\"","text":"<pre><code>import pandas as pd\nfrom rusket import AutoMiner\n\nreceipts = pd.DataFrame({\n    \"milk\":    [1, 1, 0, 1, 0, 1],\n    \"bread\":   [1, 0, 1, 1, 1, 0],\n    \"butter\":  [1, 0, 1, 0, 0, 1],\n    \"eggs\":    [0, 1, 1, 0, 1, 1],\n    \"coffee\":  [0, 1, 0, 0, 1, 1],\n}, dtype=bool)\n\nmodel = AutoMiner(receipts, min_support=0.4)\nfreq = model.mine(use_colnames=True)\nrules = model.association_rules(metric=\"confidence\", min_threshold=0.6)\nprint(rules[[\"antecedents\", \"consequents\", \"confidence\", \"lift\"]]\n      .sort_values(\"lift\", ascending=False).to_markdown(index=False))\n</code></pre> antecedents consequents confidence lift frozenset({'coffee'}) frozenset({'eggs'}) 1 1.5 frozenset({'eggs'}) frozenset({'coffee'}) 0.75 1.5 <ul> <li>Get Started — Install rusket and run your first analysis in minutes</li> <li>API Reference — Full parameter documentation for all functions</li> <li>GitHub — Source code, issues, and contributions</li> </ul>","path":["Getting Started","rusket"],"tags":[]},{"location":"1b-challenge/","level":1,"title":"The 1B Challenge","text":"<p>A story of memory management, algorithmic trade-offs, and the satisfying click of watching numbers improve.</p> <p>At the core of <code>rusket</code> is the belief that frequent itemset mining should scale on a single machine. No Spark cluster, no distributed coordination — just one process, tuned to be as efficient as possible.</p> <p>The 1 Billion Row Challenge is how we hold ourselves accountable.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#where-we-started-sorted-chunks-k-way-merge","level":2,"title":"Where We Started: Sorted Chunks + K-Way Merge","text":"<p>The first streaming design took inspiration from external-sort databases:</p> <ol> <li>Each call to <code>add_chunk()</code> receives <code>(txn_id, item_id)</code> arrays from Python.</li> <li>Rust sorts each chunk in-place using <code>rayon::par_sort_unstable()</code>.</li> <li>Chunks exceeding the RAM budget are spilled to anonymous <code>tempfile</code> files on disk.</li> <li>When <code>.mine()</code> is called, a k-way heap merge streams all sorted chunks in order, building the CSR matrix on the fly.</li> </ol> <pre><code>Memory: O(chunk_size)\nmine() memory: O(k cursors + CSR output)\n</code></pre>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#the-first-problem-disk-exhaustion","level":3,"title":"The first problem: disk exhaustion","text":"<p>Running the 500M → 1B targets, our machine hit 99% disk usage. The culprit: 500k transactions/chunk × 23 items/txn × 12 bytes/pair = ~138 MB per chunk. At 1B rows that's ~2,000 chunks = ~276 GB of tempfiles. Oops.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#the-second-problem-mine_t-grows-super-linearly","level":3,"title":"The second problem: <code>mine_t</code> grows super-linearly","text":"target_rows add_t mine_t total 300M 36.1s 516.8s 552.9s 500M 65.2s 1543.5s 1608.7s <p>The heap with 1,000+ cursors causes cache thrashing — a fundamental problem with the architecture.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#the-insight-hashmap-aggregation","level":2,"title":"The Insight: HashMap Aggregation","text":"<p>The key observation: the sorted-chunk approach stores every pair from every chunk. The real data that matters is <code>unique_transactions × avg_items_per_transaction</code>. For 1B rows with ~43M unique transactions × 23 items: that's only ~5GB vs the sorted approach's ~12GB.</p> <p>We replaced the entire chunk + merge system with a single <code>AHashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>:</p> <pre><code>pub fn add_chunk(&amp;mut self, txn_ids: ..., item_ids: ...) {\n    for (&amp;t, &amp;i) in txns.iter().zip(items.iter()) {\n        self.txns.entry(t).or_default().push(i);\n    }\n}\n</code></pre> <p><code>mine()</code> now just:</p> <ol> <li>Collects <code>(txn_id, &amp;items)</code> from the HashMap</li> <li><code>par_sort</code> by txn_id</li> <li>Sort+dedup each transaction's item list</li> <li>Build CSR → feed to algorithm</li> </ol> <p>No k-way merge. No disk spill. No tempfiles.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#initial-numbers-100m-200m-fp-growth","level":3,"title":"Initial numbers (100M &amp; 200M, FP-Growth)","text":"target_rows add_t mine_t total 100M 5.7s 26.6s 32.3s 200M 10.4s 84.6s 95.0s <p>Compared to the old approach's 300M taking 299.5s — a 3× speedup just from the architecture change.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#what-we-tried-the-iteration-phase","level":2,"title":"What We Tried: The Iteration Phase","text":"","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#approach-smallvec-u16-items-regression","level":3,"title":"Approach: SmallVec + u16 items (❌ Regression)","text":"<p>Using <code>SmallVec&lt;[u16; 32]&gt;</code> to store items inline on the stack caused a 2× slowdown. The 32-element inline size is too large for the CPU stack, causes cache pressure on every HashMap lookup.</p> <p>Lesson: measure first, optimize second.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#knob-chunk-size-100k-vs-500k-vs-2m","level":3,"title":"Knob: Chunk size (100k vs 500k vs 2M)","text":"chunk add_t mine_t total 100k 10.0s 3.8s 13.8s 500k 12.1s 5.3s 17.4s 2M 12.5s 5.3s 17.8s <p>Chunk size barely matters.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#knob-algorithm-fp-growth-vs-eclat","level":3,"title":"Knob: Algorithm (FP-Growth vs Eclat)","text":"<p>This was the biggest discovery. At 100M rows:</p> method add_t mine_t total M rows/s fpgrowth 10.0s ~55s ~65s 1.5 eclat 10.0s 3.8s 13.8s 7.24 <p>Eclat is ~14× faster at mining for dense retail data. The reason: Eclat works with vertical tidlists — for dense datasets with many frequent 2-itemsets, intersection operations are extremely cache-friendly.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#final-results-the-road-to-1b","level":2,"title":"Final Results: The Road to 1B","text":"","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#dense-retail-data-avg-23-itemstxn","level":3,"title":"Dense retail data (avg 23 items/txn)","text":"target_rows add_t mine_t total M rows/s itemsets 100M 12.5s 6.2s 18.7s 5.35 15,218 200M 22.7s 13.4s 36.1s 5.53 15,226 500M 61.1s 38.7s 99.8s 5.01 15,234 1B 173.5s 208.5s 382.1s 2.62 15,233 <p>✅ 1 Billion rows. 382 seconds. 15,233 frequent itemsets. No OOM. No disk spill.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#sparse-catalogue-data-avg-44-itemstxn","level":3,"title":"Sparse catalogue data (avg 4.4 items/txn)","text":"target_rows add_t mine_t total 100M 20.2s 4.9s 25.1s 200M 26.2s 9.2s 35.4s 500M ❌ ❌ OOM 1B ❌ ❌ OOM <p>The 1B challenge for sparse datasets remains open.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#what-we-learned","level":2,"title":"What We Learned","text":"","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#architecture-beats-micro-optimisation","level":3,"title":"Architecture beats micro-optimisation","text":"<p>The jump from k-way merge → HashMap changed 5-minute runs into 30-second runs.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#eclat-vs-fp-growth-depends-on-density","level":3,"title":"Eclat vs FP-Growth depends on density","text":"data density winner why Dense (avg &gt;10 items/txn) Eclat Tidlist intersections are O(n), very cache-friendly Sparse (avg &lt;5 items/txn) FP-Growth or similar Fewer candidates, conditional bases are small","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"1b-challenge/#running-it-yourself","level":2,"title":"Running It Yourself","text":"<pre><code>from rusket import FPMiner\nimport numpy as np\n\nminer = FPMiner(n_items=your_n_items)\n\nfor txn_ids_chunk, item_ids_chunk in your_data_stream():\n    miner.add_chunk(\n        txn_ids_chunk.astype(np.int64),\n        item_ids_chunk.astype(np.int32),\n    )\n\nfreq = miner.mine(min_support=0.02, max_len=3, method=\"eclat\")\nprint(f\"Found {len(freq):,} frequent itemsets\")\n</code></pre> <p>The full benchmark script is in <code>benchmarks/bench_fpminer_realistic.py</code>.</p>","path":["User Guide","The 1B Challenge"],"tags":[]},{"location":"api-reference/","level":1,"title":"API Reference","text":"<p>This file is auto-generated by <code>scripts/gen_api_reference.py</code>.  Do not edit by hand — update the Python docstrings instead.</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#functional-api","level":2,"title":"Functional API","text":"<p>Convenience module-level functions.  For most use-cases these are the only entry points you need.</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#mine","level":3,"title":"<code>mine</code>","text":"<p>Mine frequent itemsets using the optimal algorithm.</p> <p>This module-level function relies on the Object-Oriented APIs. Automatically selects between FP-Growth and Eclat based on density, or falls back to FPMiner (streaming) if memory is low.</p> <pre><code>from rusket.mine import mine\n\nmine(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, method: 'str' = 'auto', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpgrowth","level":3,"title":"<code>fpgrowth</code>","text":"<p>Find frequent itemsets using the optimal algorithm (Eclat or FP-growth).</p> <p>This module-level function relies on the Object-Oriented APIs.</p> <pre><code>from rusket.fpgrowth import fpgrowth\n\nfpgrowth(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, method: 'str' = 'auto', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#eclat","level":3,"title":"<code>eclat</code>","text":"<p>Find frequent itemsets using the Eclat algorithm.</p> <p>This module-level function relies on the Object-Oriented APIs.</p> <pre><code>from rusket.eclat import eclat\n\neclat(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#association_rules","level":3,"title":"<code>association_rules</code>","text":"<pre><code>from rusket.association_rules import association_rules\n\nassociation_rules(df: 'pd.DataFrame | Any', num_itemsets: 'int | None' = None, df_orig: 'pd.DataFrame | None' = None, null_values: 'bool' = False, metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, support_only: 'bool' = False, return_metrics: 'list[str]' = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'representativity', 'leverage', 'conviction', 'zhangs_metric', 'jaccard', 'certainty', 'kulczynski']) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#prefixspan","level":3,"title":"<code>prefixspan</code>","text":"<p>Mine sequential patterns using the PrefixSpan algorithm.</p> <p>This function discovers frequent sequences of items across multiple users/sessions. Currently, this assumes sequences where each event consists of a single item (e.g., a sequence of page views or a sequence of individual products bought over time).</p> <pre><code>from rusket.prefixspan import prefixspan\n\nprefixspan(sequences: 'list[list[int]]', min_support: 'int | float', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description sequences list of list of int A list of sequences, where each sequence is a list of integers representing items. Example: <code>[[1, 2, 3], [1, 3], [2, 3]]</code>. min_support int | float The minimum absolute support (number of sequences a pattern must appear in), or float percent. max_len int, optional The maximum length of the sequential patterns to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'support' and 'sequence' columns.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#hupm","level":3,"title":"<code>hupm</code>","text":"<p>Mine high-utility itemsets.</p> <p>This function discovers combinations of items that generate a high total utility (e.g., profit) across all transactions, even if they aren't the most frequent.</p> <pre><code>from rusket.hupm import hupm\n\nhupm(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description transactions list of list of int A list of transactions, where each transaction is a list of item IDs. utilities list of list of float A list of identical structure to <code>transactions</code>, but containing the numeric utility (e.g., profit) of that item in that specific transaction. min_utility float The minimum total utility required to consider a pattern \"high-utility\". max_len int, optional The maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#sequences_from_event_log","level":3,"title":"<code>sequences_from_event_log</code>","text":"<p>Convert an event log DataFrame into the sequence format required by PrefixSpan.</p> <p>Accepts Pandas, Polars, or PySpark DataFrames. Data is grouped by <code>user_col</code>, ordered by <code>time_col</code>, and <code>item_col</code> values are collected into sequences.</p> <pre><code>from rusket.prefixspan import sequences_from_event_log\n\nsequences_from_event_log(df: 'Any', user_col: 'str', time_col: 'str', item_col: 'str') -&gt; 'tuple[list[list[int]], dict[int, Any]]'\n</code></pre> <p>Parameters</p> Parameter Type Description df pd.DataFrame | pl.DataFrame | pyspark.sql.DataFrame Event log containing users, timestamps, and items. user_col str Column name identifying the sequence (e.g., user_id or session_id). time_col str Column name for ordering events. item_col str Column name for the items. <p>Returns</p> Name Type Description tuple of (indptr, indices, item_mapping) - indptr: CSR-style index pointer list. - indices: Flattened item index list. - item_mapping: A dictionary mapping the integer IDs back to the original item labels.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#mine_hupm","level":3,"title":"<code>mine_hupm</code>","text":"<p>Mine high-utility itemsets from a long-format DataFrame.</p> <p>Converts a Pandas or Polars DataFrame into the required list-of-lists format and runs the High-Utility Pattern Mining (HUPM) algorithm.</p> <pre><code>from rusket.hupm import mine_hupm\n\nmine_hupm(data: 'Any', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description data pd.DataFrame or pl.DataFrame A long-format DataFrame where each row represents an item in a transaction. transaction_col str Column name identifying the transaction ID. item_col str Column name identifying the item ID (must be numeric integers). utility_col str Column name identifying the numeric utility (e.g. price, profit) of the item. min_utility float The minimum total utility required to consider a pattern \"high-utility\". max_len int, optional Maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#mine_duckdb","level":3,"title":"<code>mine_duckdb</code>","text":"<p>Stream directly from a DuckDB query via Arrow RecordBatches.</p> <p>This is extremely memory efficient, bypassing Pandas entirely.</p> <pre><code>from rusket.streaming import mine_duckdb\n\nmine_duckdb(con: 'Any', query: 'str', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, chunk_size: 'int' = 1000000) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#mine_spark","level":3,"title":"<code>mine_spark</code>","text":"<p>Stream natively from a PySpark DataFrame on Databricks via Arrow.</p> <p>Uses <code>toLocalIterator()</code> to fetch Arrow chunks incrementally directly to the driver node, avoiding massive memory spikes.</p> <pre><code>from rusket.streaming import mine_spark\n\nmine_spark(spark_df: 'Any', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#from_transactions","level":3,"title":"<code>from_transactions</code>","text":"<p>Convert long-format transactional data to a one-hot boolean matrix.</p> <p>The return type mirrors the input type:</p> <ul> <li>Polars <code>DataFrame</code> → Polars <code>DataFrame</code></li> <li>Pandas <code>DataFrame</code> → Pandas <code>DataFrame</code></li> <li>Spark <code>DataFrame</code>  → Spark <code>DataFrame</code></li> <li><code>list[list[...]]</code>      → Pandas <code>DataFrame</code></li> </ul> <pre><code>from rusket.transactions import from_transactions\n\nfrom_transactions(data: 'DataFrame | Sequence[Sequence[str | int]] | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, min_item_count: 'int' = 1, verbose: 'int' = 0) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description data One of:  - Pandas / Polars / Spark DataFrame with (at least) two columns: one for the transaction identifier and one for the item. - List of lists where each inner list contains the items of a single transaction, e.g. <code>[[\"bread\", \"milk\"], [\"bread\", \"eggs\"]]</code>. transaction_col Name of the column that identifies transactions.  If <code>None</code> the first column is used.  Ignored for list-of-lists input. item_col Name of the column that contains item values.  If <code>None</code> the second column is used.  Ignored for list-of-lists input. min_item_count Minimum number of times an item must appear to be included in the resulting one-hot-encoded matrix. Default is 1. <p>Returns</p> Name Type Description DataFrame A boolean DataFrame (same type as input) ready for :func:<code>rusket.fpgrowth</code> or :func:<code>rusket.eclat</code>. Column names correspond to the unique items. <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"order_id\": [1, 1, 1, 2, 2, 3],\n...     \"item\": [3, 4, 5, 3, 5, 8],\n... })\n&gt;&gt;&gt; ohe = rusket.from_transactions(df)\n&gt;&gt;&gt; freq = rusket.fpgrowth(ohe, min_support=0.5, use_colnames=True)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#from_transactions_csr","level":3,"title":"<code>from_transactions_csr</code>","text":"<p>Convert long-format transactional data to a CSR matrix + column names.</p> <p>Unlike :func:<code>from_transactions</code>, this returns a raw <code>scipy.sparse.csr_matrix</code> that can be passed directly to :func:<code>rusket.fpgrowth</code> or :func:<code>rusket.eclat</code> — no pandas overhead.</p> <p>For billion-row datasets, this processes data in chunks of <code>chunk_size</code> rows, keeping peak memory to one chunk + the running CSR.</p> <pre><code>from rusket.transactions import from_transactions_csr\n\nfrom_transactions_csr(data: 'DataFrame | str | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, chunk_size: 'int' = 10000000) -&gt; 'tuple[Any, list[str]]'\n</code></pre> <p>Parameters</p> Parameter Type Description data One of:  - Pandas DataFrame with (at least) two columns. - Polars DataFrame or Spark DataFrame (converted internally). - File path (str / Path) to a Parquet file — read in chunks. transaction_col Name of the transaction-id column. Defaults to the first column. item_col Name of the item column. Defaults to the second column. chunk_size Number of rows per chunk. Lower values use less memory. Default: 10 million rows. <p>Returns</p> Name Type Description tuple[scipy.sparse.csr_matrix, list[str]] A CSR matrix and the list of column (item) names.  Pass directly::  csr, names = from_transactions_csr(df) freq = fpgrowth(csr, min_support=0.001, use_colnames=True, column_names=names) <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; csr, names = rusket.from_transactions_csr(\"orders.parquet\")\n&gt;&gt;&gt; freq = rusket.fpgrowth(csr, min_support=0.001,\n...                        use_colnames=True, column_names=names)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#from_pandas","level":3,"title":"<code>from_pandas</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_pandas\n\nfrom_pandas(df: 'pd.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, min_item_count: 'int' = 1, verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#from_polars","level":3,"title":"<code>from_polars</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_polars\n\nfrom_polars(df: 'pl.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, min_item_count: 'int' = 1, verbose: 'int' = 0) -&gt; 'pl.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#from_spark","level":3,"title":"<code>from_spark</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_spark\n\nfrom_spark(df: 'SparkDataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, min_item_count: 'int' = 1, verbose: 'int' = 0) -&gt; 'SparkDataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#oop-mining-api","level":2,"title":"OOP Mining API","text":"<p>All mining classes share a common <code>Miner.from_transactions()</code> / <code>.mine()</code> interface. <code>FPGrowth</code>, <code>Eclat</code>, <code>AutoMiner</code>, and <code>HUPM</code> also inherit <code>RuleMinerMixin</code> which adds <code>.association_rules()</code> and <code>.recommend_items()</code> helpers.</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpgrowth_1","level":3,"title":"<code>FPGrowth</code>","text":"<p>FP-Growth frequent itemset miner.</p> <p>This class wraps the fast, core Rust FP-Growth implementation.</p> <pre><code>from rusket.fpgrowth import FPGrowth\n\nFPGrowth(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpgrowthmine","level":4,"title":"<code>FPGrowth.mine</code>","text":"<p>Execute the FP-growth algorithm on the stored data.</p> <pre><code>from rusket.fpgrowth import FPGrowth.mine\n\nFPGrowth.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#eclat_1","level":3,"title":"<code>Eclat</code>","text":"<p>Eclat frequent itemset miner.</p> <p>Eclat is typically faster than FP-growth on dense datasets due to efficient vertical bitset intersection logic.</p> <pre><code>from rusket.eclat import Eclat\n\nEclat(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#eclatmine","level":4,"title":"<code>Eclat.mine</code>","text":"<p>Execute the Eclat algorithm on the stored data.</p> <pre><code>from rusket.eclat import Eclat.mine\n\nEclat.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#autominer","level":3,"title":"<code>AutoMiner</code>","text":"<p>Automatic frequent itemset miner.</p> <p>Selects the optimal miner (FP-Growth or Eclat) based on matrix density. Automatically falls back to streaming (FPMiner) if the dataset exceeds available memory.</p> <pre><code>from rusket.mine import AutoMiner\n\nAutoMiner(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = True, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#autominermine","level":4,"title":"<code>AutoMiner.mine</code>","text":"<p>Execute the optimal algorithm on the stored data.</p> <pre><code>from rusket.mine import AutoMiner.mine\n\nAutoMiner.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#prefixspan_1","level":3,"title":"<code>PrefixSpan</code>","text":"<p>Sequential Pattern Mining (PrefixSpan) model.</p> <p>This class discovers frequent sequences of items across multiple users/sessions.</p> <pre><code>from rusket.prefixspan import PrefixSpan\n\nPrefixSpan(data: 'list[list[int]]', min_support: 'int | float', max_len: 'int | None' = None, item_mapping: 'dict[int, Any] | None' = None)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#prefixspanmine","level":4,"title":"<code>PrefixSpan.mine</code>","text":"<p>Mine sequential patterns using PrefixSpan.</p> <pre><code>from rusket.prefixspan import PrefixSpan.mine\n\nPrefixSpan.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'support' and 'sequence' columns. Sequences are mapped back to original item names if <code>from_transactions</code> was used.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#hupm_1","level":3,"title":"<code>HUPM</code>","text":"<p>High-Utility Pattern Mining (HUPM) model.</p> <p>This class discovers combinations of items that generate a high total utility (e.g., profit) across all transactions, even if they aren't the most frequent.</p> <pre><code>from rusket.hupm import HUPM\n\nHUPM(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#hupmmine","level":4,"title":"<code>HUPM.mine</code>","text":"<p>Mine high-utility itemsets.</p> <pre><code>from rusket.hupm import HUPM.mine\n\nHUPM.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpminer","level":3,"title":"<code>FPMiner</code>","text":"<p>Streaming FP-Growth / Eclat accumulator for billion-row datasets.</p> <p>Feeds (transaction_id, item_id) integer arrays to Rust one chunk at a time.  Rust accumulates per-transaction item lists in a <code>HashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>.  Peak Python memory = one chunk.</p> <pre><code>from rusket.streaming import FPMiner\n\nFPMiner(n_items: 'int', max_ram_mb: 'int | None' = -1, hint_n_transactions: 'int | None' = None) -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description n_items int Number of distinct items (column count).  All item IDs fed via :meth:<code>add_chunk</code> must be in <code>[0, n_items)</code>. <p>Examples</p> <pre><code>Process a Parquet file 10 M rows at a time:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from rusket import FPMiner\n&gt;&gt;&gt; miner = FPMiner(n_items=500_000)\n&gt;&gt;&gt; for chunk in pd.read_parquet(\"orders.parquet\", chunksize=10_000_000):\n...     txn = chunk[\"txn_id\"].to_numpy(dtype=\"int64\")\n...     item = chunk[\"item_idx\"].to_numpy(dtype=\"int32\")\n...     miner.add_chunk(txn, item)\n&gt;&gt;&gt; freq = miner.mine(min_support=0.001, max_len=3, use_colnames=True)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpmineradd_arrow_batch","level":4,"title":"<code>FPMiner.add_arrow_batch</code>","text":"<p>Feed a PyArrow RecordBatch directly into the miner. Zero-copy extraction is used if types match (Int64/Int32).</p> <pre><code>from rusket.streaming import FPMiner.add_arrow_batch\n\nFPMiner.add_arrow_batch(batch: 'Any', txn_col: 'str', item_col: 'str') -&gt; 'FPMiner'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpmineradd_chunk","level":4,"title":"<code>FPMiner.add_chunk</code>","text":"<p>Feed a chunk of (transaction_id, item_id) pairs.</p> <pre><code>from rusket.streaming import FPMiner.add_chunk\n\nFPMiner.add_chunk(txn_ids: 'np.ndarray', item_ids: 'np.ndarray') -&gt; 'FPMiner'\n</code></pre> <p>Parameters</p> Parameter Type Description txn_ids np.ndarray[int64] 1-D array of transaction identifiers (arbitrary 64-bit integers). item_ids np.ndarray[int32] 1-D array of item column indices (0-based). <p>Returns</p> Name Type Description self  (for chaining)","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpminermine","level":4,"title":"<code>FPMiner.mine</code>","text":"<p>Mine frequent itemsets from all accumulated transactions.</p> <pre><code>from rusket.streaming import FPMiner.mine\n\nFPMiner.mine(min_support: 'float' = 0.5, max_len: 'int | None' = None, use_colnames: 'bool' = True, column_names: 'list[str] | None' = None, method: \"typing.Literal['fpgrowth', 'eclat', 'auto']\" = 'auto', verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description min_support float Minimum support threshold in <code>(0, 1]</code>. max_len int | None Maximum itemset length. use_colnames bool If <code>True</code>, itemsets contain column names instead of indices. column_names list[str] | None Column names to use when <code>use_colnames=True</code>. method \"fpgrowth\" | \"eclat\" | \"auto\" Mining algorithm to use.  <code>\"auto\"</code> (default) picks the best algorithm automatically based on data density after pre-filtering rare items (Borgelt 2003 heuristic: density &lt; 15% → Eclat, else FPGrowth). verbose int Level of verbosity: &gt;0 prints progress logs and times. <p>Returns</p> Name Type Description pd.DataFrame Columns <code>support</code> and <code>itemsets</code>.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#fpminerreset","level":4,"title":"<code>FPMiner.reset</code>","text":"<p>Free all accumulated data.</p> <pre><code>from rusket.streaming import FPMiner.reset\n\nFPMiner.reset() -&gt; 'None'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#ruleminermixin-shared-miner-interface","level":2,"title":"<code>RuleMinerMixin</code> — Shared Miner Interface","text":"<p><code>FPGrowth</code>, <code>Eclat</code>, <code>AutoMiner</code>, and <code>HUPM</code> all inherit these methods from <code>RuleMinerMixin</code>.  You do not construct <code>RuleMinerMixin</code> directly.</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#ruleminermixinassociation_rules","level":3,"title":"<code>RuleMinerMixin.association_rules</code>","text":"<p>Generate association rules from the mined frequent itemsets.</p> <pre><code>from rusket.model import RuleMinerMixin.association_rules\n\nRuleMinerMixin.association_rules(metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, return_metrics: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description metric str, default='confidence' The metric to evaluate if a rule is of interest. min_threshold float, default=0.8 The minimum threshold for the evaluation metric. return_metrics list[str] | None, default=None List of metrics to include in the resulting DataFrame. Defaults to all available metrics. <p>Returns</p> Name Type Description pd.DataFrame DataFrame of strong association rules.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#ruleminermixinrecommend_items","level":3,"title":"<code>RuleMinerMixin.recommend_items</code>","text":"<p>Deprecated: use :meth:<code>recommend_for_cart</code> instead.</p> <pre><code>from rusket.model import RuleMinerMixin.recommend_items\n\nRuleMinerMixin.recommend_items(items: 'list[Any]', n: 'int' = 5) -&gt; 'list[Any]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#ruleminermixin_invalidate_rules_cache","level":3,"title":"<code>RuleMinerMixin._invalidate_rules_cache</code>","text":"<p>Clear the cached association rules (call after re-mining).</p> <pre><code>from rusket.model import RuleMinerMixin._invalidate_rules_cache\n\nRuleMinerMixin._invalidate_rules_cache() -&gt; 'None'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#recommenders","level":2,"title":"Recommenders","text":"","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#als","level":3,"title":"<code>ALS</code>","text":"<p>Implicit ALS collaborative filtering model.</p> <pre><code>from rusket.als import ALS\n\nALS(factors: 'int' = 64, regularization: 'float' = 0.01, alpha: 'float' = 40.0, iterations: 'int' = 15, seed: 'int' = 42, verbose: 'int' = 0, cg_iters: 'int' = 10, use_cholesky: 'bool' = False, anderson_m: 'int' = 0, **kwargs: 'Any') -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description factors int Number of latent factors. regularization float L2 regularisation weight. alpha float Confidence scaling: <code>confidence = 1 + alpha * r</code>. iterations int Number of ALS outer iterations. seed int Random seed. cg_iters int Conjugate Gradient iterations per user/item solve (ignored when <code>use_cholesky=True</code>).  Reduce to 3 for very large datasets. use_cholesky bool Use a direct Cholesky solve instead of iterative CG. Exact solution; faster when users have many interactions relative to <code>factors</code>. anderson_m int History window for Anderson Acceleration of the outer ALS loop (default 0 = disabled).  Recommended value: 5.  ALS is a fixed-point iteration <code>(U,V) → F(U,V)</code>.  Anderson mixing extrapolates over the last <code>m</code> residuals to reach the fixed point faster, typically reducing the number of outer iterations by 30–50 % at identical recommendation quality::  # Baseline: 15 iterations model = ALS(iterations=15, cg_iters=3)  # Anderson-accelerated: 10 iterations, ~2.5× faster, same quality model = ALS(iterations=10, cg_iters=3, anderson_m=5)  Memory overhead: <code>m</code> copies of the full <code>(U ∥ V)</code> matrix (~57 MB per copy at 25M ratings, k=64).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#alsbatch_recommend","level":4,"title":"<code>ALS.batch_recommend</code>","text":"<p>Top-N items for all users efficiently computed in parallel.</p> <pre><code>from rusket.als import ALS.batch_recommend\n\nALS.batch_recommend(n: 'int' = 10, exclude_seen: 'bool' = True, format: 'str' = 'polars') -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description n int, default=10 The number of items to recommend per user. exclude_seen bool, default=True Whether to exclude items the user has already interacted with. format str, default=\"polars\" The DataFrame format to return. One of \"pandas\", \"polars\", or \"spark\". <p>Returns</p> Name Type Description DataFrame A DataFrame with columns <code>user_id</code>, <code>item_id</code>, and <code>score</code>.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#alsfit","level":4,"title":"<code>ALS.fit</code>","text":"<p>Fit the model to the user-item interaction matrix.</p> <pre><code>from rusket.als import ALS.fit\n\nALS.fit(interactions: 'Any') -&gt; 'ALS'\n</code></pre> <p>Raises</p> Exception Condition RuntimeError TypeError","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#alsrecommend_items","level":4,"title":"<code>ALS.recommend_items</code>","text":"<p>Top-N items for a user. Set exclude_seen=False to include already-seen items.</p> <pre><code>from rusket.als import ALS.recommend_items\n\nALS.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -&gt; 'tuple[Any, Any]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#alsrecommend_users","level":4,"title":"<code>ALS.recommend_users</code>","text":"<p>Top-N users for an item.</p> <pre><code>from rusket.als import ALS.recommend_users\n\nALS.recommend_users(item_id: 'int', n: 'int' = 10) -&gt; 'tuple[Any, Any]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#bpr","level":3,"title":"<code>BPR</code>","text":"<p>Bayesian Personalized Ranking (BPR) model for implicit feedback.</p> <p>BPR optimizes for ranking rather than reconstruction error (like ALS). It works by drawing positive items the user interacted with, and negative items they haven't, and adjusting latent factors to ensure the positive item scores higher.</p> <pre><code>from rusket.bpr import BPR\n\nBPR(factors: 'int' = 64, learning_rate: 'float' = 0.05, regularization: 'float' = 0.01, iterations: 'int' = 150, seed: 'int' = 42, verbose: 'int' = 0, **kwargs: 'Any') -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description factors int Number of latent factors (default: 64). learning_rate float SGD learning rate (default: 0.05). regularization float L2 regularization weight (default: 0.01). iterations int Number of passes over the entire interaction dataset (default: 150). seed int Random seed for Hogwild! SGD sampling (default: 42).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#bprfit","level":4,"title":"<code>BPR.fit</code>","text":"<p>Fit the BPR model to the user-item interaction matrix.</p> <pre><code>from rusket.bpr import BPR.fit\n\nBPR.fit(interactions: 'Any') -&gt; 'BPR'\n</code></pre> <p>Raises</p> Exception Condition RuntimeError TypeError","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#bprrecommend_items","level":4,"title":"<code>BPR.recommend_items</code>","text":"<p>Top-N items for a user.</p> <pre><code>from rusket.bpr import BPR.recommend_items\n\nBPR.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -&gt; 'tuple[Any, Any]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#recommender","level":3,"title":"<code>Recommender</code>","text":"<p>Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.</p> <pre><code>from rusket.recommend import Recommender\n\nRecommender(model: 'Any | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#recommenderpredict_next_chunk","level":4,"title":"<code>Recommender.predict_next_chunk</code>","text":"<p>Batch-rank the next best products for every user in user_history_df.</p> <pre><code>from rusket.recommend import Recommender.predict_next_chunk\n\nRecommender.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#recommenderrecommend_for_cart","level":4,"title":"<code>Recommender.recommend_for_cart</code>","text":"<p>Suggest items to add to an active cart using association rules.</p> <pre><code>from rusket.recommend import Recommender.recommend_for_cart\n\nRecommender.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -&gt; 'list[int]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#recommenderrecommend_for_user","level":4,"title":"<code>Recommender.recommend_for_user</code>","text":"<p>Top-N recommendations for a user via Hybrid ALS + Semantic.</p> <pre><code>from rusket.recommend import Recommender.recommend_for_user\n\nRecommender.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -&gt; 'tuple[np.ndarray, np.ndarray]'\n</code></pre> <p>Parameters</p> Parameter Type Description user_id int The user ID to generate recommendations for. n int, default=5 Number of items to return. alpha float, default=0.5 Weight blending CF vs Semantic. <code>alpha=1.0</code> is pure CF. <code>alpha=0.0</code> is pure semantic. target_item_for_semantic int | None, default=None If provided, semantic similarity is computed against this item. If None, and alpha &lt; 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#nextbestaction","level":3,"title":"<code>NextBestAction</code>","text":"<p>Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.</p> <pre><code>from rusket.recommend import NextBestAction\n\nNextBestAction(model: 'Any | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#nextbestactionpredict_next_chunk","level":4,"title":"<code>NextBestAction.predict_next_chunk</code>","text":"<p>Batch-rank the next best products for every user in user_history_df.</p> <pre><code>from rusket.recommend import NextBestAction.predict_next_chunk\n\nNextBestAction.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'pd.DataFrame'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#nextbestactionrecommend_for_cart","level":4,"title":"<code>NextBestAction.recommend_for_cart</code>","text":"<p>Suggest items to add to an active cart using association rules.</p> <pre><code>from rusket.recommend import NextBestAction.recommend_for_cart\n\nNextBestAction.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -&gt; 'list[int]'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#nextbestactionrecommend_for_user","level":4,"title":"<code>NextBestAction.recommend_for_user</code>","text":"<p>Top-N recommendations for a user via Hybrid ALS + Semantic.</p> <pre><code>from rusket.recommend import NextBestAction.recommend_for_user\n\nNextBestAction.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -&gt; 'tuple[np.ndarray, np.ndarray]'\n</code></pre> <p>Parameters</p> Parameter Type Description user_id int The user ID to generate recommendations for. n int, default=5 Number of items to return. alpha float, default=0.5 Weight blending CF vs Semantic. <code>alpha=1.0</code> is pure CF. <code>alpha=0.0</code> is pure semantic. target_item_for_semantic int | None, default=None If provided, semantic similarity is computed against this item. If None, and alpha &lt; 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#analytics-utilities","level":2,"title":"Analytics &amp; Utilities","text":"","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#score_potential","level":3,"title":"<code>score_potential</code>","text":"<p>Cross-selling potential scores — shape <code>(n_users, n_items)</code> or <code>(n_users, len(target_categories))</code>.</p> <p>Items the user has already interacted with are masked to <code>-inf</code>.</p> <pre><code>from rusket.recommend import score_potential\n\nscore_potential(user_history: 'list[list[int]]', model: 'Any', target_categories: 'list[int] | None' = None) -&gt; 'np.ndarray'\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#similar_items","level":3,"title":"<code>similar_items</code>","text":"<p>Find the most similar items to a given item ID based on latent factors.</p> <p>Computes cosine similarity between the specified item's latent vector and all other item vectors in the <code>item_factors</code> matrix.</p> <pre><code>from rusket.similarity import similar_items\n\nsimilar_items(model: 'SupportsItemFactors', item_id: 'int', n: 'int' = 5) -&gt; 'tuple[np.ndarray, np.ndarray]'\n</code></pre> <p>Parameters</p> Parameter Type Description model SupportsItemFactors A fitted model instance with an <code>item_factors</code> property. item_id int The internal integer index of the target item. n int Number of most similar items to return. <p>Returns</p> Name Type Description tuple[np.ndarray, np.ndarray] <code>(item_ids, cosine_similarities)</code> sorted in descending order.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#find_substitutes","level":3,"title":"<code>find_substitutes</code>","text":"<p>Substitute/cannibalizing products via negative association rules.</p> <p>Items with high individual support but low co-occurrence (lift &lt; 1.0) likely cannibalize each other.</p> <pre><code>from rusket.analytics import find_substitutes\n\nfind_substitutes(rules_df: 'pd.DataFrame', max_lift: 'float' = 0.8) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description rules_df DataFrame output from <code>rusket.association_rules</code>. max_lift Upper bound for lift; lift &lt; 1.0 implies negative correlation. <p>Returns</p> Name Type Description pd.DataFrame sorted ascending by lift (most severe cannibalization first).","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#customer_saturation","level":3,"title":"<code>customer_saturation</code>","text":"<p>Customer saturation by unique items/categories bought, split into deciles.</p> <pre><code>from rusket.analytics import customer_saturation\n\ncustomer_saturation(df: 'pd.DataFrame', user_col: 'str', category_col: 'str | None' = None, item_col: 'str | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description df Interaction DataFrame. user_col Column identifying the user. category_col Category column (optional; at least one of category/item required). item_col Item column (optional). <p>Returns</p> Name Type Description pd.DataFrame with <code>unique_count</code>, <code>saturation_pct</code>, and <code>decile</code> columns.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#export_item_factors","level":3,"title":"<code>export_item_factors</code>","text":"<p>Exports latent item factors as a DataFrame for Vector DBs.</p> <p>This format is ideal for ingesting into FAISS, Pinecone, or Qdrant for Retrieval-Augmented Generation (RAG) and semantic search.</p> <pre><code>from rusket.export import export_item_factors\n\nexport_item_factors(model: 'SupportsItemFactors', include_labels: 'bool' = True, normalize: 'bool' = False, format: 'str' = 'pandas') -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description model SupportsItemFactors A fitted model instance with an <code>item_factors</code> property. include_labels bool, default=True Whether to include the string item labels (if available from the model's fitting method). normalize bool, default=False Whether to L2-normalize the factors before export. format str, default=\"pandas\" The DataFrame format to return. One of \"pandas\", \"polars\", or \"spark\". <p>Returns</p> Name Type Description Any A DataFrame where each row is an item with columns <code>item_id</code>, optionally <code>item_label</code>, and <code>vector</code> (a dense 1-D numpy array of the item's latent factors). <p>Examples</p> <pre><code>&gt;&gt;&gt; model = rusket.ALS(factors=32).fit(interactions)\n&gt;&gt;&gt; df = rusket.export_item_factors(model)\n&gt;&gt;&gt; # Ingest into FAISS / Pinecone / Qdrant\n&gt;&gt;&gt; vectors = np.stack(df[\"vector\"].values)\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#visualization-rusketviz","level":2,"title":"Visualization (<code>rusket.viz</code>)","text":"<p>Graph and visualization utilities.  Requires <code>networkx</code> (<code>pip install networkx</code>).</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketvizto_networkx","level":3,"title":"<code>rusket.viz.to_networkx</code>","text":"<p>Convert a Rusket association rules DataFrame into a NetworkX Directed Graph.</p> <p>Nodes represent individual items. Directed edges represent rules (antecedent → consequent). Edge weights are set by the <code>edge_attr</code> parameter (typically lift or confidence).</p> <p>This is extremely useful for running community detection algorithms (e.g., Louvain, Girvan-Newman) to automatically discover product clusters, or for visualising cross-selling patterns as a force-directed graph.</p> <pre><code>from rusket.viz import rusket.viz.to_networkx\n\nrusket.viz.to_networkx(rules_df: 'pd.DataFrame', source_col: 'str' = 'antecedents', target_col: 'str' = 'consequents', edge_attr: 'str' = 'lift') -&gt; 'networkx.DiGraph'\n</code></pre> <p>Parameters</p> Parameter Type Description rules_df pd.DataFrame A Pandas DataFrame generated by <code>rusket.association_rules()</code>. source_col str, default='antecedents' Column name containing antecedents (graph edge sources). target_col str, default='consequents' Column name containing consequents (graph edge targets). edge_attr str, default='lift' The metric to use as edge weight/thickness. <p>Returns</p> Name Type Description networkx.DiGraph A directed graph of the association rules. If <code>rules_df</code> is empty, returns an empty <code>DiGraph</code>. <p>Notes Requires the <code>networkx</code> package (<code>pip install networkx</code>). When multiple rules produce the same directed edge, only the highest-weight rule is retained.</p> <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; G = rusket.viz.to_networkx(rules_df, edge_attr=\"lift\")\n&gt;&gt;&gt; # Community detection with networkx\n&gt;&gt;&gt; import networkx.algorithms.community as nx_comm\n&gt;&gt;&gt; communities = nx_comm.greedy_modularity_communities(G.to_undirected())\n</code></pre>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#distributed-spark-api-rusketspark","level":2,"title":"Distributed Spark API (<code>rusket.spark</code>)","text":"<p>All functions in <code>rusket.spark</code> distribute computation across PySpark partitions using Apache Arrow (zero-copy) for maximum throughput.</p>","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkmine_grouped","level":3,"title":"<code>rusket.spark.mine_grouped</code>","text":"<p>Distribute Market Basket Analysis across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>rusket.mine</code> to each group concurrently across the cluster.</p> <p>It assumes the input PySpark DataFrame is formatted like a dense boolean matrix (One-Hot Encoded) per group, where rows are transactions.</p> <pre><code>from rusket.spark import rusket.spark.mine_grouped\n\nrusket.spark.mine_grouped(df: 'Any', group_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, method: 'str' = 'auto', use_colnames: 'bool' = True) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). min_support Minimum support threshold. max_len Maximum itemset length. method Algorithm to use: 'auto', 'fpgrowth', or 'eclat'. use_colnames If True, returns item names instead of column indices. Must be True for PySpark <code>applyInArrow</code> schema consistency. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>support</code> (float) - <code>itemsets</code> (array of strings)","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkrules_grouped","level":3,"title":"<code>rusket.spark.rules_grouped</code>","text":"<p>Distribute Association Rule Mining across PySpark partitions.</p> <p>This takes the frequent itemsets DataFrame (output of <code>mine_grouped</code>) and applies <code>association_rules</code> uniformly across the groups.</p> <pre><code>from rusket.spark import rusket.spark.rules_grouped\n\nrusket.spark.rules_grouped(df: 'Any', group_col: 'str', num_itemsets: 'dict[Any, int] | int', metric: 'str' = 'confidence', min_threshold: 'float' = 0.8) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The PySpark <code>DataFrame</code> containing frequent itemsets. group_col The column to group by. num_itemsets A dictionary mapping group IDs to their total transaction count, or a single integer if all groups have the same number of transactions. metric The metric to filter by (e.g. \"confidence\", \"lift\"). min_threshold The minimal threshold for the evaluation metric. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A DataFrame containing antecedents, consequents, and all rule metrics, prepended with the <code>group_col</code>.","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkprefixspan_grouped","level":3,"title":"<code>rusket.spark.prefixspan_grouped</code>","text":"<p>Distribute Sequential Pattern Mining (PrefixSpan) across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>PrefixSpan.from_transactions</code> to each group concurrently across the cluster.</p> <pre><code>from rusket.spark import rusket.spark.prefixspan_grouped\n\nrusket.spark.prefixspan_grouped(df: 'Any', group_col: 'str', user_col: 'str', time_col: 'str', item_col: 'str', min_support: 'int' = 1, max_len: 'int | None' = None) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). user_col The column identifying the sequence within each group (e.g., <code>user_id</code> or <code>session_id</code>). time_col The column used for ordering events within a sequence. item_col The column containing the items. min_support The minimum absolute support (number of sequences a pattern must appear in). max_len Maximum length of the sequential patterns to mine. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>support</code> (long/int64) - <code>sequence</code> (array of strings)","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkhupm_grouped","level":3,"title":"<code>rusket.spark.hupm_grouped</code>","text":"<p>Distribute High-Utility Pattern Mining (HUPM) across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>HUPM.from_transactions</code> to each group concurrently across the cluster.</p> <pre><code>from rusket.spark import rusket.spark.hupm_grouped\n\nrusket.spark.hupm_grouped(df: 'Any', group_col: 'str', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). transaction_col The column identifying the transaction within each group. item_col The column containing the numeric item IDs. utility_col The column containing the numeric utility (e.g., profit) of the item in the transaction. min_utility The minimum total utility required to consider a pattern \"high-utility\". max_len Maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>utility</code> (double/float64) - <code>itemset</code> (array of longs/int64)","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkrecommend_batches","level":3,"title":"<code>rusket.spark.recommend_batches</code>","text":"<p>Distribute Batch Recommendations across PySpark partitions.</p> <p>This function uses <code>mapInArrow</code> to process partitions of users concurrently, applying a pre-fitted <code>Recommender</code> (or <code>ALS</code>) to each chunk.</p> <pre><code>from rusket.spark import rusket.spark.recommend_batches\n\nrusket.spark.recommend_batches(df: 'Any', model: 'Any', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The PySpark <code>DataFrame</code> containing user histories (must contain <code>user_col</code>). model The pre-trained <code>Recommender</code> or <code>ALS</code> model instance to use for scoring. user_col The column identifying the user. k The number of top recommendations to return per user. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A DataFrame with two columns: - <code>user_col</code> - <code>recommended_items</code> (array of longs/int64)","path":["User Guide","API Reference"],"tags":[]},{"location":"api-reference/#rusketsparkto_spark","level":3,"title":"<code>rusket.spark.to_spark</code>","text":"<p>Convert a Pandas or Polars DataFrame into a PySpark DataFrame.</p> <pre><code>from rusket.spark import rusket.spark.to_spark\n\nrusket.spark.to_spark(spark_session: 'Any', df: 'Any') -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description spark_session The active PySpark <code>SparkSession</code>. df The <code>pd.DataFrame</code> or <code>pl.DataFrame</code> to convert. <p>Returns</p> Name Type Description pyspark.sql.DataFrame The resulting PySpark DataFrame.","path":["User Guide","API Reference"],"tags":[]},{"location":"architecture/","level":1,"title":"Architecture","text":"<p>How rusket relies on Rust, PyO3, and Rayon for zero-copy, zero-allocation Python bindings.</p> <p>rusket is structured as a thin Python layer over a Rust core, compiled as a native extension module via PyO3 and maturin.</p>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#class-hierarchy-api-conventions","level":2,"title":"Class hierarchy &amp; API conventions","text":"<p>rusket has two class families: Miners (frequent pattern discovery) and Recommenders (collaborative filtering). All share common API patterns.</p>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#base-classes","level":3,"title":"Base classes","text":"<pre><code>BaseModel (ABC)\n├── Miner ─── FPGrowth, Eclat, FIN, LCM, AutoMiner (+ RuleMinerMixin)\n│             PrefixSpan, HUPM (specialized miners)\n├── ImplicitRecommender ─── ALS, BPR, EASE, ItemKNN, SVD, LightGCN\n├── SequentialRecommender ─── FPMC, SASRec\n└── FM (standalone, uses explicit feature matrices)\n</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#common-conventions","level":3,"title":"Common conventions","text":"Convention Rule <code>from_transactions(data, transaction_col, item_col, verbose, **kwargs)</code> Class method on every class for DataFrame-based init <code>from_pandas()</code>, <code>from_polars()</code>, <code>from_spark()</code> Aliases delegating to <code>from_transactions()</code> <code>verbose: int</code> <code>0</code> = silent, <code>1</code>+ = progress logs <code>seed: int</code> Random-seed parameter (never <code>random_state</code>) <code>fitted: bool</code> Attribute set to <code>True</code> after <code>.fit()</code> or <code>.mine()</code> <code>__repr__</code> Every class has a <code>__repr__</code> showing key hyperparameters Docstrings NumPy-style (<code>Parameters</code>, <code>Returns</code>, <code>Raises</code>)","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#miner-interface","level":3,"title":"Miner interface","text":"<pre><code>miner = FPGrowth.from_transactions(df, transaction_col=\"tid\", item_col=\"item\")\nfreq  = miner.mine()                        # → pd.DataFrame (support, itemsets)\nrules = miner.association_rules()            # → pd.DataFrame (metrics)\nrecs  = miner.recommend_for_cart(items, n=5) # → list[Any]\n</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#recommender-interface","level":3,"title":"Recommender interface","text":"<pre><code>model = ALS.from_transactions(df, user_col=\"user\", item_col=\"item\", factors=64)\nids, scores = model.recommend_items(user_id=0, n=10, exclude_seen=True)\n</code></pre> Method Available on <code>fit(interactions)</code> All recommenders <code>recommend_items(user_id, n, exclude_seen)</code> All recommenders <code>recommend_users(item_id, n)</code> ALS, SVD (others raise <code>NotImplementedError</code>) <code>batch_recommend(n, exclude_seen, format)</code> ALS, SVD <code>user_factors</code> / <code>item_factors</code> ALS, BPR, SVD, LightGCN","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#sequential-recommenders-fpmc-sasrec","level":3,"title":"Sequential recommenders (FPMC, SASRec)","text":"<p>Work on ordered sequences. SASRec also accepts ad-hoc sequences:</p> <pre><code>model = SASRec.from_transactions(df, user_col=\"user\", item_col=\"item\", timestamp_col=\"ts\")\nids, scores = model.recommend_items([1, 2, 3], n=10)\n</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#repository-layout","level":2,"title":"Repository layout","text":"<pre><code>rusket/\n├── src/                          # Rust (PyO3)\n│   ├── lib.rs                    # Module root — exports to Python\n│   ├── fpgrowth.rs               # FP-Tree + FP-Growth algorithm\n│   ├── association_rules.rs      # Rule generation + 12 metrics\n│   └── common.rs                 # Shared helpers\n├── python/\n│   ├── rusket/                  # Primary Python package (pyproject.toml name)\n│   │   ├── __init__.py\n│   │   ├── fpgrowth.py           # Dispatch + numpy conversion\n│   │   ├── association_rules.py  # Label mapping + Rust call\n│   │   └── _validation.py        # Input validation helpers\n│   └── fpgrowth_pyo3/            # Legacy compat package\n│       └── ...\n└── tests/\n    ├── conftest.py\n    ├── test_fpbase.py            # Shared base test classes\n    ├── test_fpgrowth.py          # FP-Growth tests\n    ├── test_association_rules.py # Association rules tests\n    └── test_benchmark.py         # Performance benchmarks\n</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#data-flow","level":2,"title":"Data flow","text":"<pre><code>graph TD\n    classDef python fill:#4B8BBE,stroke:#306998,stroke-width:2px,color:white;\n    classDef rust fill:#DEA584,stroke:#000,stroke-width:2px,color:black;\n    classDef data fill:#FFD43B,stroke:#306998,stroke-width:2px,color:black;\n\n    A[\"Python Caller&lt;br/&gt;rusket.mine(df)\"]:::python --&gt; B{\"Input Data Type\"}:::python\n\n    B --&gt;|Dense Pandas| C[\"C-Contiguous Array&lt;br/&gt;(uint8)\"]:::data\n    B --&gt;|Sparse Pandas| D[\"CSR Matrix&lt;br/&gt;(indptr, indices)\"]:::data\n    B --&gt;|Polars| E[\"Numpy View&lt;br/&gt;(Zero-copy)\"]:::data\n\n    C --&gt; F[\"Rust FFI&lt;br/&gt;fpgrowth_from_dense\"]:::rust\n    D --&gt; G[\"Rust FFI&lt;br/&gt;fpgrowth_from_csr\"]:::rust\n    E --&gt; F\n\n    F --&gt; H[\"Tree Construction&lt;br/&gt;(Single Pass)\"]:::rust\n    G --&gt; H\n\n    H --&gt; I[\"Recursive Mining&lt;br/&gt;(Rayon Parallel)\"]:::rust\n    I --&gt; J[\"Raw Vectors&lt;br/&gt;Vec&lt;(count, Vec&lt;usize&gt;)&gt;\"]:::data\n\n    J --&gt; K[\"Python Transformation&lt;br/&gt;Build pd.DataFrame\"]:::python</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#fp-growth-algorithm","level":2,"title":"FP-Growth algorithm","text":"<p>The Rust implementation follows the classic Han et al. (2000) FP-Growth algorithm:</p> <ol> <li>Header table scan — count item frequencies; prune items below <code>min_count</code>.</li> <li>FP-Tree construction — single-pass over transactions; compress into a prefix-tree structure.</li> <li>Recursive mining — for each frequent item, extract the conditional pattern base, build a conditional FP-Tree, and mine it recursively.</li> <li>Output — each leaf path materialises as one frequent itemset <code>(count, items)</code>.</li> </ol>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#dispatch-paths","level":3,"title":"Dispatch paths","text":"Path Rust function Input shape Notes Dense pandas <code>fpgrowth_from_dense</code> <code>[n_rows × n_cols]</code> uint8 Contiguous C array Sparse pandas <code>fpgrowth_from_csr</code> CSR <code>indptr + indices</code> Zero-copy scipy CSR Polars <code>fpgrowth_from_dense</code> same as dense Arrow → NumPy view","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#association-rules","level":2,"title":"Association rules","text":"<p>Rule generation is vectorised in Rust:</p> <ol> <li>For each frequent itemset of length ≥ 2, enumerate all non-empty antecedent / consequent splits.</li> <li>Look up antecedent and consequent supports from a pre-built hash map.</li> <li>Compute all 12 metrics in a single pass; filter by <code>(metric, min_threshold)</code>.</li> <li>Return raw integer index lists to Python; Python maps back to column names / frozensets.</li> </ol>","path":["User Guide","Architecture"],"tags":[]},{"location":"architecture/#building-from-source","level":2,"title":"Building from source","text":"<pre><code># Prerequisites: Rust 1.83+, Python 3.10+, uv\nrustup update\nuv sync\n\n# Debug build (fast compile, slower runtime)\nuv run maturin develop\n\n# Release build (optimised)\nuv run maturin develop --release\n\n# Type checking\nuv run basedpyright\n\n# Tests\nuv run pytest tests/ -x -q\n\n# Cargo lint\ncargo check\ncargo clippy\n</code></pre>","path":["User Guide","Architecture"],"tags":[]},{"location":"benchmarks/","level":1,"title":"Benchmarks","text":"<p>Performance comparisons: rusket vs mlxtend, scaling to 1 Billion rows, and FPMiner throughput.</p> <p><code>rusket</code> includes two mining algorithms (FP-Growth and Eclat), both implemented in Rust. These benchmarks compare <code>rusket</code> against mlxtend (pure Python) on synthetic and real-world datasets.</p> <p>Benchmark environment: Apple Silicon MacBook Air (M-series, arm64, 8 GB RAM). All timings are single-run wall-clock measurements. Your results may vary depending on hardware, thermal throttling, and background load.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#scale-benchmarks-1m-1b-rows","level":2,"title":"Scale Benchmarks: 1M → 1B Rows 🚀","text":"","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#interactive-chart","level":3,"title":"Interactive Chart","text":"","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#three-input-paths","level":3,"title":"Three Input Paths","text":"<p>rusket supports three ways to ingest data at scale:</p> <ol> <li><code>from_transactions</code> → sparse DataFrame — returns a pandas DataFrame, easy API</li> <li>Direct CSR → Rust — pass <code>scipy.sparse.csr_matrix</code> directly to <code>fpgrowth()</code>, skips pandas entirely</li> <li><code>FPMiner</code> Streaming — memory-safe accumulator for 100M+ rows that don't fit in RAM</li> </ol>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#in-memory-scale-fpgrowth","level":4,"title":"In-Memory Scale (fpgrowth)","text":"Scale <code>from_transactions</code> → fpgrowth Direct CSR → fpgrowth Speedup 1M rows (200k txns × 10k items) 5.0s 0.1s 50× 10M rows (2M txns × 50k items) 24.4s 1.2s 20× 50M rows (10M txns × 100k items) 63.1s 4.0s 15× 100M rows (20M txns × 200k items) 134.2s 10.1s 13× 200M rows (40M txns × 200k items) 246.8s 17.6s 14× <p>Direct CSR is the power-user path</p> <p>At 100M rows, direct CSR mining takes 1.2 seconds — the bottleneck is entirely the CSR build (24.5s). Compare to the pandas sparse path where mining alone takes 9.0s due to <code>sparse.to_coo().tocsr()</code> overhead.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#out-of-core-scale-fpminer-streaming","level":4,"title":"Out-of-Core Scale (FPMiner Streaming)","text":"<p>For real-world retail datasets scaling to 1 Billion rows, <code>FPMiner</code> uses a memory-safe chunks approach (per-chunk sort + k-way merge).</p> <p>Benchmark: 2,603 retail items, avg 4.4 items/basket, min_support = 0.1%</p> Scale add_chunk() mine() Total Time Itemsets Found 50M rows 4.8s 5.6s 10.4s 1,260 100M rows 10.6s 13.9s 24.6s 1,254 200M rows 22.7s 33.2s 55.9s 1,261 300M rows 30.0s 55.4s 85.4s 1,259","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#vs-mlxtend","level":2,"title":"vs mlxtend","text":"Dataset <code>rusket</code> (prep + mine) <code>mlxtend</code> (prep + mine) Speedup 50k rows (10k txns, 100 items) 0.0 s 0.1 s ~5× 500k rows (50k txns, 500 items) 0.2 s 1.8 s ~9× 2M rows (500k txns, 2k items) 0.2 s 16.0 s 80×","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#the-auto-routine-algorithm","level":2,"title":"The \"Auto\" Routine Algorithm","text":"<p><code>rusket.mine(method=\"auto\")</code> dynamically selects the algorithm that performs best based on the dataset density (Borgelt 2003 heuristic).</p> <ul> <li>Density &gt; 0.15 (Dense): Automatically routes to FP-Growth.</li> <li>Density &lt; 0.15 (Sparse): Automatically routes to Eclat. On sparse data (like retail baskets), traversing an enormous tree is memory-intensive. Eclat directly uses hardware SIMD array-intersections (<code>popcnt</code>) on the TID-lists, resulting in massive speedups (often 5× to 15× faster on sparse arrays).</li> </ul>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#real-world-datasets","level":2,"title":"Real-World Datasets","text":"Dataset Transactions Items <code>rusket</code> <code>mlxtend</code> Speedup andi_data.txt 8,416 119 9.7 s (22.8M itemsets) TIMEOUT 💥 ∞ andi_data2.txt 540,455 2,603 7.9 s 16.2 s 2× <p>Dense data</p> <p>On <code>andi_data.txt</code> (~23 items/basket), <code>mlxtend</code> can't finish in 60s. <code>rusket</code> mines 22.8M itemsets in under 10s.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#the-power-user-pipeline","level":2,"title":"The Power-User Pipeline","text":"<pre><code>import numpy as np\nfrom scipy import sparse as sp\nfrom rusket import AutoMiner\n\ncsr = sp.csr_matrix(\n    (np.ones(len(txn_ids), dtype=np.int8), (txn_ids, item_ids)),\n    shape=(n_transactions, n_items),\n)\n\nfreq = AutoMiner(csr).mine(min_support=0.001, max_len=3, column_names=item_names)\n</code></pre> <p>At 100M rows, the mining step takes 1.2 seconds (not a typo).</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#conquering-the-1-billion-row-challenge","level":2,"title":"🏆 Conquering the 1 Billion Row Challenge","text":"","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#bottleneck-1-memory-exhaustion-during-ingestion","level":3,"title":"Bottleneck 1: Memory Exhaustion during Ingestion","text":"<p>The Solution: The <code>FPMiner</code> class provides an out-of-core streaming API. It accepts chunks of <code>(txn_id, item_id)</code> pairs, performs a fast \\(O(k \\log k)\\) sort in Rust, buffers them, and uses a k-way merge to stream directly into the final compressed CSR memory block.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#bottleneck-2-algorithmic-memory-thrashing","level":3,"title":"Bottleneck 2: Algorithmic Memory Thrashing","text":"<p>The Solution: <code>rusket</code> employs a zero-allocation <code>intersect_count_into()</code> kernel. It pre-allocates a thread-local scratch buffer, intersected in-place, with an early-exit heuristic that aborts the memory scan the moment it proves the remaining bits cannot satisfy <code>min_support</code>.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#bottleneck-3-sequential-seriality","level":3,"title":"Bottleneck 3: Sequential Seriality","text":"<p>The Solution: <code>rusket</code> merges tree construction into the parallel worker loop. Conditional trees are collected and mined concurrently inside the rayon thread pool.</p>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#running-the-benchmarks","level":2,"title":"Running the benchmarks","text":"<pre><code>uv run maturin develop --release\nuv run python benchmarks/bench_scale.py    # Scale benchmark + Plotly chart\nuv run python benchmarks/bench_realworld.py\nuv run python benchmarks/bench_vs_mlxtend.py\nuv run pytest tests/test_benchmark.py -v -s\n</code></pre>","path":["User Guide","Benchmarks"],"tags":[]},{"location":"benchmarks/#why-is-rusket-faster","level":2,"title":"Why is rusket faster?","text":"Technique Description Zero-copy CSR <code>indptr</code>/<code>indices</code> passed to Rust as pointer hand-offs Arena FP-Tree Flat children arena, incremental <code>is_path()</code> tracking Rayon Parallel conditional mining across CPU cores Eclat popcount <code>Vec&lt;u64&gt;</code> bitsets + hardware <code>popcnt</code> for support No Python loops FP-Tree, mining, and metrics all in Rust <code>pd.factorize</code> O(n) integer encoding, faster than <code>pd.Categorical</code> at scale","path":["User Guide","Benchmarks"],"tags":[]},{"location":"changelog/","level":1,"title":"Changelog","text":"<p>All notable changes are documented here. This project follows Semantic Versioning.</p>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> <li>auto-format code with Ruff [skip ci]</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features","level":3,"title":"🚀 Features","text":"<ul> <li>add Kosarak regression test, Rust unit tests for association_rules, and benchmark vs arm-rs</li> <li>Introduce SVD model, standardize <code>verbose</code> and <code>seed</code> parameters, and optimize imports across modules.</li> <li>Refactor benchmarks, add new comparison benchmarks, enhance SVD API with type hints and <code>fitted</code> property, and expand documentation and test coverage.</li> <li>add SVD model, LibRecommender benchmarks, zero-dep messaging</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>als_model references in Recommender docs and test setups</li> <li>add tabulate to dev dependencies for mkdocs</li> <li>keep _orig_type resolution before to_dataframe data coercion</li> <li>lazy evaluation of num_itemsets to support PySpark dfs</li> <li>preserve spark dataframe column order after inner join in from_transactions</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_1","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> <li>bump version to 0.1.46</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_2","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#style","level":3,"title":"Style","text":"<ul> <li>apply ruff formatting</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_3","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>add remaining unstaged files from main</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd","level":3,"title":"🔄 CI/CD","text":"<ul> <li>enforce regression and benchmark job pass before PyPI release</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#test","level":3,"title":"Test","text":"<ul> <li>extract benchmark and regression tests into a separate workflow</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_4","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_1","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>resolve E0432 unresolved import and E0308 type mismatch in FPGrowth and ALS</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation","level":3,"title":"📖 Documentation","text":"<ul> <li>promote OOP API, update Ferris logo color to blue, fix pyright errors</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_5","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> <li>auto-format code with Ruff [skip ci]</li> <li>updating docs and testing</li> <li>auto-format code with Ruff [skip ci]</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_1","level":3,"title":"🚀 Features","text":"<ul> <li>handle miner kwargs and preserve DataFrame return types</li> <li>use labels by default in mining algorithms</li> <li>Enhance ALS and FPGrowth algorithms, update ALS benchmarks, add Databricks cookbook, refresh project logos, and remove <code>test_colnames.py</code>.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_6","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd_1","level":3,"title":"🔄 CI/CD","text":"<ul> <li>skip git-auto-commit on tags to prevent race conditions and bump to v0.1.39</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#optimizer","level":3,"title":"Optimizer","text":"<ul> <li>enhance PySpark toArrow to utilize pandas ArrowDtype</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_7","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>auto-format code with Ruff [skip ci]</li> <li>resolve u.vlock conflict</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd_2","level":3,"title":"🔄 CI/CD","text":"<ul> <li>fix detached head for auto-commit and bump to v0.1.37</li> <li>remove experimental python 3.14 to fix pipeline hang and bump to v0.1.38</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd_3","level":3,"title":"🔄 CI/CD","text":"<ul> <li>add git-auto-commit for ruff format and bump to v0.1.36</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_8","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix ruff format issues from type ignores and bump to v0.1.35</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_9","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>bypass pandas-stubs typing issues for older Python versions and bump to v0.1.34</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_10","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix Ruff trailing whitespace formatting error in test_fpbase.py, bump to v0.1.33</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_11","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix PrefixSpan KeyError in test_spark_prefixspan and bump to v0.1.32</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_12","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix pandas FutureWarning in tests and bump to v0.1.31</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_13","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix PySpark assertion and Pytest deprecation warnings, bump v0.1.30</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#benchmarks","level":3,"title":"Benchmarks","text":"<ul> <li>add comprehensive benchmark scripts and final report against Python libraries</li> <li>fix missing imports and numpy compatibility, fix ruff lints</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#mining","level":3,"title":"Mining","text":"<ul> <li>optimize prefixspan removing hashmaps and pyo3 object lists for 1.15x speedup</li> <li>optimize prefixspan with zero-copy numpy ffi over pyo3 for 2.05x speedup</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_14","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>untrack benchmarking, profile, and recbole test artifacts</li> <li>fix pytest warnings/pyright errors and bump version to v0.1.29</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_15","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>bump to v0.1.28, fix typing issues in tests</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#style_1","level":3,"title":"Style","text":"<ul> <li>run ruff format and fix lints</li> <li>Auto-format with Ruff</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_2","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>ensure Spark input is handled before Polars coercion in from_transactions</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_1","level":3,"title":"📖 Documentation","text":"<ul> <li>update logo asset path to <code>logo_single.svg</code> in documentation and configuration.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_16","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>update uv.lock for 0.1.26</li> <li>exclude non-essential files from sdist</li> <li>exclude dev/docs from sdist and wheels</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_2","level":3,"title":"🚀 Features","text":"<ul> <li>Add strict UI typings (SupportsItemFactors), classes API filtering, and generated Schema</li> <li>add natively rust-backed evaluation metrics and model selection splitters</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_2","level":3,"title":"📖 Documentation","text":"<ul> <li>sync changelog and api reference for 0.1.26</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_17","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>bump version to 0.1.26</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_3","level":3,"title":"🚀 Features","text":"<ul> <li><code>from_transactions</code> now preserves input DataFrame type for Pandas, Polars, and Spark with updated type hints and tests.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#benchmark","level":3,"title":"Benchmark","text":"<ul> <li>add script comparing eALS vs iALS</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#debug","level":3,"title":"Debug","text":"<ul> <li>re-raise exception in als_grouped worker to reveal root cause</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#merge","level":3,"title":"Merge","text":"<ul> <li>feature/fin-lcm-miner into main (FIN/LCM algorithms, FM/FPMC)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance","level":3,"title":"⚡ Performance","text":"<ul> <li>SIMD unrolling for dot and axpy hot-loops in ALS solver</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_3","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>auto-coerce 0/1 pandas DataFrames to bool in dispatch, silence non-bool DeprecationWarning</li> <li>add criterion dev-dependency for bench targets</li> <li>validate DataFrame before coercing to bool so invalid values (e.g. 2) raise ValueError</li> <li>add fitted property to ItemKNN</li> <li>suppress DeprecationWarning in als_grouped Spark worker</li> <li>use internal model indices in als_grouped worker to correctly map user_labels</li> <li>resolve all pyright errors and ruff format/lint failures for CI</li> <li>resolve all ruff format/lint and pyright CI failures</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_3","level":3,"title":"📖 Documentation","text":"<ul> <li>fix MDX parsing errors for Mintlify</li> <li>add business-oriented LightGCN and SASRec example notebooks</li> <li>migrate to Zensical for GitHub Pages deployment</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_18","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>Remove Python profiling, benchmarking, and RecBole testing scripts, and update Cargo.toml and .gitignore.</li> <li>untrack generated artifacts (tensorboard logs, dSYM, recbole_data, saved)</li> <li>untrack ai slop benches/ directory</li> <li>bump version to 0.1.25</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_4","level":3,"title":"🚀 Features","text":"<ul> <li>implement FIN and LCM algorithms with fast bitset operations</li> <li>wip RecBole benchmarking and FM/FPMC algorithms</li> <li>add LightGCN and SASRec recommendation models</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bench","level":3,"title":"Bench","text":"<ul> <li>fix unfair benchmark timing and optimize EASE with Cholesky</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#benchmark_1","level":3,"title":"Benchmark","text":"<ul> <li>add script comparing dEclat vs ECLAT</li> <li>add script comparing eALS vs iALS</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#style_2","level":3,"title":"Style","text":"<ul> <li>run ruff format</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance_1","level":3,"title":"⚡ Performance","text":"<ul> <li>SIMD unrolling for dot and axpy hot-loops in ALS solver</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_4","level":3,"title":"📖 Documentation","text":"<ul> <li>expose llm.txt in docs root and fix test_real_world.py sampling</li> <li>migrate to Mintlify</li> <li>auto-update API reference, changelog, and llm.txt</li> <li>fix MDX parsing errors for Mintlify</li> <li>auto-update API reference, changelog, and llm.txt</li> <li>add als 25m benchmark sweep chart</li> <li>update changelog for YOLO release</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_19","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>include Mintlify config and generated MDX docs</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_5","level":3,"title":"🚀 Features","text":"<ul> <li>Add ultra-fast Sparse ItemKNN algorithm using BM25 and Rust Rayon</li> <li>implement FIN and LCM algorithms with fast bitset operations</li> <li>wip RecBole benchmarking and FM/FPMC algorithms</li> <li>Add grouped PySpark support for ALS</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#style_3","level":3,"title":"Style","text":"<ul> <li>apply ruff formatting and fixes</li> <li>Update logo colors from purple to orange.</li> <li>refine logos with orange theme, update mkdocs palette and extra.css</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_4","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>resolve PySpark ChunkedArray fallback warning and implement BPR fit_transactions</li> <li>fix pyright errors reported on ci</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_5","level":3,"title":"📖 Documentation","text":"<ul> <li>add Polars/PySpark PrefixSpan tests and cookbook examples</li> <li>improve API documentation, update marketing copy, and setup PySpark skips</li> <li>enhance PrefixSpan and HUPM cookbook sections with clearer descriptions, business scenarios, and updated Python code examples.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_20","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>commit remaining unstaged files from previous sessions</li> <li>bump version to 0.1.21</li> <li>bump version to 0.1.22</li> <li>bump version to 0.1.23</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#refactoring","level":3,"title":"🔧 Refactoring","text":"<ul> <li>simplify BaseModel and remove implicit recommender duplication</li> <li>update logo SVG basket elements to use curved paths and refined wire details.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_6","level":3,"title":"🚀 Features","text":"<ul> <li>core algorithms via Faer, HUPM, Arrow Streams, and Hybrid Recommender</li> <li>complete PySpark and Polars integration for PrefixSpan via native PyArrow sequences</li> <li>implement recommend_items for association rule models</li> <li>Introduce new documentation notebooks, update PySpark integration documentation, and add a notebook conversion workflow.</li> <li>automated doc sync scripts (changelog, API ref, llm.txt)</li> <li>enhance recommender system documentation and examples, update core logic, and refresh logos.</li> <li>merge feature/fpgrowth-mlxtend-api</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance_2","level":3,"title":"⚡ Performance","text":"<ul> <li>Boost FPGrowth performance with a new architecture, update benchmarks and documentation, add new logos, and remove temporary test files.\"</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_5","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>skip mlxtend comparison at &gt;1M rows to prevent CI timeout</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_6","level":3,"title":"📖 Documentation","text":"<ul> <li>add genai and lancedb integration examples to cookbook</li> <li>add cookbook examples for ALS PCA visualization and Spark MLlib translation</li> <li>conquer 1 billion row challenge architecture and bump v0.1.20</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd_4","level":3,"title":"🔄 CI/CD","text":"<ul> <li>trigger Deploy Docs on benchmarks/** changes too</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#refactoring_1","level":3,"title":"🔧 Refactoring","text":"<ul> <li>clean Python layer — remove stale timing vars, dead code, AI-slop comments</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_6","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>Loosen numerical tolerance for parallel Hogwild! BPR test to fix CI</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_7","level":3,"title":"📖 Documentation","text":"<ul> <li>use relative path for logo in README</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_8","level":3,"title":"📖 Documentation","text":"<ul> <li>Comprehensive Interactive Cookbook with Real-World Datasets</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bench_1","level":3,"title":"Bench","text":"<ul> <li>add Cholesky to ALS benchmark script and fix pyright</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_9","level":3,"title":"📖 Documentation","text":"<ul> <li>feature rusket.mine as the primary public api endpoint across mkdocs and readme</li> <li>append comprehensive cookbook examples for prefixspan, hupm, bpr, similarity, and recommender modules</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_21","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>safe checkpoint</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_7","level":3,"title":"🚀 Features","text":"<ul> <li>add method='auto' routing to dynamically select eclat or fpgrowth based on dataset density</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_8","level":3,"title":"🚀 Features","text":"<ul> <li>YOLO release v0.1.16</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance_3","level":3,"title":"⚡ Performance","text":"<ul> <li>implement rayon multi-threading for FPMiner chunk ingestion</li> <li>revert SmallVec regression, clean HashMap FPMiner + scale to 1B benchmark</li> <li>item pre-filter + with_capacity hint in FPMiner</li> <li>fix freq-sort to ascending (Eclat-optimal: least-frequent items first)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_7","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>pyright unbound variables correctly initialized</li> <li>pyright complaints about unbound variables and missing als_fit_implicit argument</li> <li>benchmark now uses 8GB in-memory limit instead of disk-spilling at scale</li> <li>streaming.py cleanup + als_fit_implicit cg_iters stub + psutil available RAM strategy</li> <li>batched mining at 250M rows per batch to avoid OOM at 800M+</li> <li>SCALE_TARGETS scoping + launch 1B Eclat scale-up</li> <li>restore SEP in benchmark f-strings</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_10","level":3,"title":"📖 Documentation","text":"<ul> <li>add FPMiner out-of-core streaming section and 300M benchmark</li> <li>add ALS feature and market basket analysis to README</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_9","level":3,"title":"🚀 Features","text":"<ul> <li>add verbose mode to fpgrowth, eclat, and FPMiner for large-scale feedback</li> <li>implement hybrid memory/disk out-of-core FPMiner with dynamic RAM limit</li> <li>add verbose iteration timing + out-of-core 1B support</li> <li>comprehensive cookbook + ALS speed improvements</li> <li>HashMap FPMiner + creative benchmark (method × chunk-size × scale)</li> <li>frequency-sorted remap + mine_auto + hint_n_transactions (Borgelt 2003)</li> <li>Anderson Acceleration for ALS outer loop (anderson_m param)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_10","level":3,"title":"🚀 Features","text":"<ul> <li>FPMiner streaming accumulator v0.1.14</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_11","level":3,"title":"🚀 Features","text":"<ul> <li>direct scipy CSR support in fpgrowth/eclat + pd.factorize + scale benchmarks</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_12","level":3,"title":"🚀 Features","text":"<ul> <li>automated scale benchmark with Plotly chart (1M-500M rows)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_13","level":3,"title":"🚀 Features","text":"<ul> <li>sparse CSR from_transactions + million-scale benchmarks (66× faster)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bench_2","level":3,"title":"Bench","text":"<ul> <li>add real-world dataset benchmark (auto-downloads, with timeouts)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_11","level":3,"title":"📖 Documentation","text":"<ul> <li>add Eclat API, real-world benchmarks, and usage examples</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_14","level":3,"title":"🚀 Features","text":"<ul> <li>add from_transactions, from_pandas, from_polars, from_spark helpers</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#test_1","level":3,"title":"Test","text":"<ul> <li>add dedicated test_eclat.py for standalone eclat() function</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance_4","level":3,"title":"⚡ Performance","text":"<ul> <li>arena-based FPNode with flat children storage (7.8x speedup)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_8","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>add readme and license to pyproject.toml for PyPI, bump to 0.1.9</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_15","level":3,"title":"🚀 Features","text":"<ul> <li>add Eclat algorithm (method='eclat') with 2.4-2.8x speedup on sparse data</li> <li>make eclat the default method (faster in all benchmarks)</li> <li>expose eclat() as standalone public function</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_9","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>remove orphaned FPGrowth import after FP-TDA removal</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_22","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>remove FP-TDA implementation</li> <li>add MIT license</li> <li>add dependabot.yml to match httprx structure</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_16","level":3,"title":"🚀 Features","text":"<ul> <li>implement zero-copy slice algorithm for FP-TDA</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_23","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>remove tracked pycache / .pyc files</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_10","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>remove target-cpu=native from .cargo/config.toml to fix CI SIGILL crashes</li> <li>exclude test_benchmark.py from regular pytest run to prevent mlxtend timeouts</li> <li>increase CI timeout to 45min for slow free-threaded Python builds</li> <li>benchmark CI - conditional baseline compare + PyPI trusted publishing (OIDC)</li> <li>fptda iterative mining to avoid stack overflow on sparse data</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_12","level":3,"title":"📖 Documentation","text":"<ul> <li>compact logo, remove fast pattern mining subtitle</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_24","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>merge feat/regression-benchmarks into main</li> <li>bump version to 0.1.5</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#refactoring_2","level":3,"title":"🔧 Refactoring","text":"<ul> <li>extract FPBase, add FPTda class, FP-TDA in benchmarks</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_17","level":3,"title":"🚀 Features","text":"<ul> <li>regression benchmark tests + fix warnings</li> <li>add FP-TDA algorithm (IJISRT25NOV1256)\\n\\nImplements the Frequent-Pattern Two-Dimensional Array algorithm as a\\ndrop-in alternative to FP-Growth. Uses right-to-left column projection\\non sorted transaction lists instead of conditional subtree construction.\\n\\n- src/fptda.rs: Rust core (fptda_from_dense / fptda_from_csr)\\n- rusket/fptda.py: Python wrapper, identical API to fpgrowth()\\n- rusket/init.py: export rusket.fptda\\n- tests/test_fptda.py: 22 tests (mix-ins + cross-check vs fpgrowth)\\n- src/fpgrowth.rs: made process_item_counts/flatten_results pub(crate)\\n- src/lib.rs: register new pyfunction bindings</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#style_4","level":3,"title":"Style","text":"<ul> <li>apply ruff format and fix lint errors</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_11","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>remove tracked site/ dir, rename fpgrowth-pyo3→rusket, fix docs workflow</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_13","level":3,"title":"📖 Documentation","text":"<ul> <li>add CI/CD workflow guidance to AGENTS.md</li> <li>publish real benchmark numbers with Plotly interactive chart</li> <li>add GitHub Pages enable step to AGENTS.md</li> <li>replace cookbook notebook with clean markdown, simplify docs workflow</li> <li>add YOLO section to AGENTS.md; merge feat/regression-benchmarks</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_18","level":3,"title":"🚀 Features","text":"<ul> <li>add benchmark against efficient-apriori</li> <li>Bump version to 0.1.3, refine FPGrowth Arrow data type handling, update dependencies, and refactor test and project files.</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_12","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>add mkdocs-jupyter dependency for github pages</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_25","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>fix docs deployment and format readme</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#performance_5","level":3,"title":"⚡ Performance","text":"<ul> <li>zero-copy pyarrow backend implementation</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#bug-fixes_13","level":3,"title":"🐛 Bug Fixes","text":"<ul> <li>resolve SIGABRT panic in fpgrowth.rs and restore missing validation checks in python port</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_14","level":3,"title":"📖 Documentation","text":"<ul> <li>add comprehensive Jupyter cookbook with Plotly graphs and benchmark results</li> <li>add pyarrow zero-copy dataframe slicing examples</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_26","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>add pytest-timeout to dev dependencies</li> <li>bump version to 0.1.1</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#documentation_15","level":3,"title":"📖 Documentation","text":"<ul> <li>emphasize ultimate blazing speed in README</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#miscellaneous_27","level":3,"title":"📦 Miscellaneous","text":"<ul> <li>add maturin and pyright to dev dependencies for CI</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#cicd_5","level":3,"title":"🔄 CI/CD","text":"<ul> <li>configure automated pypi release and github tags workflow</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"changelog/#features_19","level":3,"title":"🚀 Features","text":"<ul> <li>optimised FP-Growth (mimalloc + SmallVec + PAR_ITEMS_CUTOFF=4 + parallel freq count + dedup)</li> </ul>","path":["Reference","Changelog"],"tags":[]},{"location":"cookbook/","level":1,"title":"Cookbook","text":"<p>A hands-on guide to every feature in <code>rusket</code> — from market basket analysis to billion-scale collaborative filtering.</p>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#setup","level":2,"title":"Setup","text":"<pre><code>pip install rusket\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport polars as pl\nfrom rusket import FPGrowth, Eclat, AutoMiner, association_rules\nfrom rusket import ALS, BPR, PrefixSpan, HUPM, Recommender\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#1-market-basket-analysis-grocery-retail","level":2,"title":"1. Market Basket Analysis — Grocery Retail","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#business-context","level":3,"title":"Business context","text":"<p>A supermarket chain wants to identify which product combinations appear most frequently in customer baskets. The output drives:</p> <ul> <li>\"Frequently Bought Together\" widgets on the self-checkout screen</li> <li>Shelf adjacency decisions (place high-lift pairs closer together)</li> <li>Promotional bundles (discount pairs with high confidence but low current margin)</li> </ul>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#prepare-the-basket-data-and-find-frequent-combinations","level":3,"title":"Prepare the basket data and find frequent combinations","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom rusket import FPGrowth\n\nnp.random.seed(42)\n\ncategories = {\n    \"Milk\": 0.55, \"Bread\": 0.52, \"Butter\": 0.36, \"Eggs\": 0.41,\n    \"Cheese\": 0.28, \"Yogurt\": 0.22, \"Coffee\": 0.31, \"Tea\": 0.18,\n    \"Sugar\": 0.20, \"Apples\": 0.25, \"Bananas\": 0.30, \"Oranges\": 0.15,\n    \"Chicken\": 0.35, \"Pasta\": 0.27, \"Tomato Sauce\": 0.26, \"Onions\": 0.40,\n}\n\nn_receipts = 10_000\ndf_long = pd.DataFrame(\n    [(receipt, product)\n     for receipt in range(n_receipts)\n     for product, prob in categories.items()\n     if np.random.rand() &lt; prob],\n    columns=[\"receipt_id\", \"product\"],\n)\n\nminer = FPGrowth.from_transactions(\n    df_long,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    min_support=0.05,\n    use_colnames=True,\n)\nfreq = miner.mine()\nprint(f\"Found {len(freq):,} frequent product combinations\")\ntop_combos = freq.sort_values(\"support\", ascending=False)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#generate-cross-sell-rules","level":3,"title":"Generate cross-sell rules","text":"<pre><code># Rules are now accessible directly from the miner instance\nrules = miner.association_rules(min_threshold=0.3)\nactionable = rules[(rules[\"confidence\"] &gt; 0.45) &amp; (rules[\"lift\"] &gt; 1.2)]\nprint(actionable.sort_values(\"lift\", ascending=False).head(10))\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#limit-itemset-length-for-large-catalogues","level":3,"title":"Limit itemset length for large catalogues","text":"<pre><code>miner_pairs = FPGrowth.from_transactions(\n    df_long,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    min_support=0.02,\n    max_len=2,\n    use_colnames=True,\n)\nfreq_pairs = miner_pairs.mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#2-eclat-when-to-use-vs-fpgrowth","level":2,"title":"2. ECLAT — When to Use vs FPGrowth","text":"<p>ECLAT uses a vertical bitset representation. It is faster than FPGrowth for sparse datasets.</p> <pre><code>from rusket import Eclat\n\nfreq_ec = Eclat.from_transactions(\n    df_long,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    min_support=0.05,\n    use_colnames=True,\n).mine()\n</code></pre> Condition Recommended class Dense dataset, few items <code>AutoMiner</code> Sparse dataset, many items, low support <code>Eclat</code> Very large dataset (100M+ rows) <code>FPMiner</code> with streaming","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#3-transaction-input-formats","level":2,"title":"3. Transaction Input Formats","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#from-a-pandas-dataframe","level":3,"title":"From a Pandas DataFrame","text":"<pre><code>import pandas as pd\nfrom rusket import FPGrowth\n\norders = pd.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\nfreq = FPGrowth.from_transactions(\n    orders,\n    transaction_col=\"order_id\",\n    item_col=\"item\",\n    min_support=0.3,\n    use_colnames=True,\n).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#from-a-polars-dataframe","level":3,"title":"From a Polars DataFrame","text":"<pre><code>import polars as pl\nfrom rusket import FPGrowth\n\norders_pl = pl.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\nfreq = FPGrowth.from_transactions(\n    orders_pl,\n    transaction_col=\"order_id\",\n    item_col=\"item\",\n    min_support=0.3,\n    use_colnames=True,\n).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#from-a-list-of-lists","level":3,"title":"From a list of lists","text":"<pre><code>from rusket import FPGrowth\n\nbaskets = [[\"Milk\", \"Bread\"], [\"Milk\", \"Eggs\", \"Butter\"], [\"Bread\", \"Eggs\"]]\nfreq = FPGrowth(baskets, min_support=0.5, use_colnames=True).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#4-collaborative-filtering-with-als","level":2,"title":"4. Collaborative Filtering with ALS","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#fit-from-purchase-history","level":3,"title":"Fit from purchase history","text":"<pre><code>import pandas as pd\nfrom rusket import ALS\n\npurchases = pd.DataFrame({\n    \"customer_id\": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1003],\n    \"sku\":         [\"A10\", \"B22\", \"C15\",  \"A10\", \"D33\",  \"B22\", \"C15\", \"E07\"],\n    \"revenue\":     [29.99, 49.00, 9.99,  29.99, 15.00, 49.00, 9.99, 22.00],\n})\n\nmodel = ALS.from_transactions(\n    purchases,\n    transaction_col=\"customer_id\",\n    item_col=\"sku\",\n    rating_col=\"revenue\",\n    factors=64,\n    iterations=15,\n    alpha=40.0,\n    cg_iters=3,\n)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#get-personalised-recommendations","level":3,"title":"Get personalised recommendations","text":"<pre><code>skus, scores = model.recommend_items(user_id=1002, n=5, exclude_seen=True)\ntop_customers, scores = model.recommend_users(item_id=\"B22\", n=100)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#access-latent-factors-item-embeddings-directly","level":3,"title":"Access latent factors (item embeddings) directly","text":"<pre><code># NumPy arrays (n_users x factors) and (n_items x factors)\nprint(model.user_factors.shape)  # (n_users, 64)\nprint(model.item_factors.shape)  # (n_items, 64)\n\n# Semantic alias for LLM/GenAI workflows\nembeddings = model.item_embeddings\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#5-out-of-core-als-for-1b-ratings","level":2,"title":"5. Out-of-Core ALS for 1B+ Ratings","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#build-the-out-of-core-csr-matrix","level":3,"title":"Build the out-of-core CSR matrix","text":"<pre><code>import numpy as np\nfrom scipy import sparse\nfrom pathlib import Path\n\ndata_dir = Path(\"data/ml-1b/ml-20mx16x32\")\nnpz_files = sorted(data_dir.glob(\"trainx*.npz\"))\n\n# ... (out of core logic) ...\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#fit-als-on-the-out-of-core-matrix","level":3,"title":"Fit ALS on the out-of-core matrix","text":"<pre><code>from rusket import ALS\n\nmat = sparse.csr_matrix((n_users, n_items))\nmat.indptr  = indptr\nmat.indices = mmap_indices\nmat.data    = mmap_data\n\nmodel = ALS(factors=64, iterations=5, alpha=40.0, verbose=True, cg_iters=3)\nmodel.fit(mat)\n</code></pre> <p>Tip</p> <p>On a machine with ≥ 32 GB RAM, each iteration completes in ~5 minutes. On 8 GB RAM, each iteration is disk-bound and takes hours.</p>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#6-bayesian-personalized-ranking-bpr","level":2,"title":"6. Bayesian Personalized Ranking (BPR)","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom rusket import BPR\n\npurchases = pd.DataFrame({\n    \"user_id\": np.random.randint(0, 1000, size=5000),\n    \"item_id\": np.random.randint(0, 500, size=5000),\n})\n\nmodel = BPR.from_transactions(\n    purchases,\n    transaction_col=\"user_id\",\n    item_col=\"item_id\",\n    factors=64,\n    learning_rate=0.01,\n    regularization=0.01,\n    iterations=100,\n    seed=42,\n)\n\nitems, scores = model.recommend_items(user_id=10, n=5)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#7-sequential-pattern-mining-prefixspan","level":2,"title":"7. Sequential Pattern Mining (PrefixSpan)","text":"<pre><code>import pandas as pd\nfrom rusket import PrefixSpan\n\nclickstream = pd.DataFrame({\n    \"session_id\": [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n    \"timestamp\":  [10, 20, 30, 15, 25, 5, 15, 35, 10, 18, 40],\n    \"page\": [\n        \"Home\", \"Pricing\", \"Checkout\",\n        \"Home\", \"Pricing\",\n        \"Features\", \"Pricing\", \"Checkout\",\n        \"Home\", \"Features\", \"Checkout\",\n    ],\n})\n\nminer = PrefixSpan.from_transactions(\n    clickstream,\n    user_col=\"session_id\",\n    time_col=\"timestamp\",\n    item_col=\"page\",\n    min_support=2,\n    max_len=4,\n)\npatterns_df = miner.mine()\nprint(patterns_df.head(10))\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#8-high-utility-pattern-mining-hupm","level":2,"title":"8. High-Utility Pattern Mining (HUPM)","text":"<pre><code>import pandas as pd\nfrom rusket import HUPM\n\nreceipts = pd.DataFrame({\n    \"receipt_id\": [1, 1, 1, 2, 2, 3, 3, 4, 4, 4],\n    \"product\":    [\"champagne\", \"foie_gras\", \"truffle_oil\",\n                   \"champagne\", \"truffle_oil\",\n                   \"foie_gras\", \"truffle_oil\",\n                   \"champagne\", \"foie_gras\", \"truffle_oil\"],\n    \"margin\":     [18.50, 14.00, 8.00, 18.50, 8.00, 14.00, 8.00, 18.50, 14.00, 8.00],\n})\n\nhigh_value = HUPM.from_transactions(\n    receipts,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    utility_col=\"margin\",\n    min_utility=30.0,\n).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#9-native-polars-integration","level":2,"title":"9. Native Polars Integration","text":"<p>All miners accept Polars DataFrames directly — no conversion needed:</p> <pre><code>import polars as pl\nfrom rusket import AutoMiner\n\ndf_pl = pl.read_parquet(\"transactions.parquet\")\n\nfreq = AutoMiner.from_transactions(\n    df_pl,\n    transaction_col=\"order_id\",\n    item_col=\"product_id\",\n    min_support=0.05,\n    use_colnames=True,\n).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#10-spark-databricks-integration","level":2,"title":"10. Spark / Databricks Integration","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#streaming-1b-rows-from-spark","level":3,"title":"Streaming 1B+ Rows from Spark","text":"<pre><code>from rusket import FPMiner\n\nspark_df = spark.table(\"silver_transactions\")\nfrequent_itemsets = FPMiner(\n    spark_df,\n    n_items=500_000,\n    txn_col=\"transaction_id\",\n    item_col=\"product_id\",\n    min_support=0.001,\n).mine()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#distributed-parallel-mining-grouped","level":3,"title":"Distributed Parallel Mining (Grouped)","text":"<pre><code>from rusket.spark import mine_grouped\n\nregional_rules_df = mine_grouped(spark_df, group_col=\"store_id\", min_support=0.05)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#collaborative-filtering-als-from-spark","level":3,"title":"Collaborative Filtering (ALS) from Spark","text":"<pre><code>from rusket import ALS\n\nmodel = ALS.from_transactions(\n    spark.table(\"implicit_ratings\"),\n    transaction_col=\"user_id\",\n    item_col=\"item_id\",\n    rating_col=\"clicks\",\n    factors=64,\n    iterations=10,\n)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#11-databricks-high-speed-cross-sell-generation","level":2,"title":"11. Databricks: High-Speed Cross-Sell Generation","text":"<p>When working in Databricks with millions of users, Python <code>for</code> loops are a massive bottleneck. Use <code>batch_recommend</code> to leverage Rust's parallel iterators (Rayon) and return native Spark or Polars DataFrames instantly.</p> <pre><code>from rusket import ALS\nimport polars as pl\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\npurchases = spark.table(\"bronze_layer.customer_transactions\")\n\n# 1. Train the model using the fast Polars bridge\nals = ALS(factors=128, iterations=15).from_transactions(\n    purchases.toPandas(), # Or pass Polars directly if memory allows\n    transaction_col=\"customer_id\",\n    item_col=\"product_id\",\n    rating_col=\"sales_amount\",\n)\n\n# 2. Score ALL users simultaneously across all CPU cores (Rust Rayon)\n#    Returns a fast Polars DataFrame: [user_id, item_id, score]\nrecommendations_pl = als.batch_recommend(n=10, format=\"polars\")\n\n# 3. Export L2-normalized item and user factors directly to Spark for Delta tables\nuser_factors_df = als.export_user_factors(normalize=True, format=\"spark\")\nitem_factors_df = als.export_factors(normalize=True, format=\"spark\")\n\n# 4. Save to Delta\nuser_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.user_embeddings\")\nitem_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.item_embeddings\")\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#12-tuning-guide","level":2,"title":"12. Tuning Guide","text":"","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#fpgrowth-eclat-autominer","level":3,"title":"FPGrowth / Eclat / AutoMiner","text":"Parameter Default Effect <code>min_support</code> required Lower → more itemsets, slower <code>max_len</code> None Cap itemset size — huge speedup on large catalogs <code>use_colnames</code> False Return column names instead of indices","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#als","level":3,"title":"ALS","text":"Parameter Default Notes <code>factors</code> 64 Higher → better quality, more RAM, slower <code>iterations</code> 15 5–15 is typical <code>alpha</code> 40.0 Higher → stronger signal <code>cg_iters</code> 3 CG solver steps <code>anderson_m</code> 0 Anderson acceleration history (5 recommended)","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#13-item-similarity-and-cross-selling-potential","level":2,"title":"13. Item Similarity and Cross-Selling Potential","text":"<pre><code># Now part of the Model class\nitem_ids, match_scores = model.similar_items(item_id=102, n=5)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#14-hybrid-recommender-als-association-rules","level":2,"title":"14. Hybrid Recommender (ALS + Association Rules)","text":"<pre><code>from rusket import Recommender\n\nrec = Recommender(model=model, rules_df=rules)\nitem_ids, scores = rec.recommend_for_user(user_id=125, n=5)\nsuggested_additions = rec.recommend_for_cart([10, 15], n=3)\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#15-genai-llm-stack-integration","level":2,"title":"15. GenAI / LLM Stack Integration","text":"<pre><code>import lancedb\n# Direct export from model\ndf_vectors = model.export_factors()\ndb = lancedb.connect(\"./lancedb\")\ntable = db.create_table(\"item_embeddings\", data=df_vectors, mode=\"overwrite\")\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"cookbook/#16-visualizing-latent-spaces-pca","level":2,"title":"16. Visualizing Latent Spaces (PCA)","text":"<pre><code># Built-in 3D PCA visualization\nfig = model.visualize_factors()\nfig.show()\n</code></pre>","path":["User Guide","Cookbook"],"tags":[]},{"location":"databricks_cookbook/","level":1,"title":"Databricks ALS Cross-Sell Cookbook","text":"<p>This guide outlines how to use <code>rusket</code> within a Databricks/PySpark scale environment. It walks through the end-to-end process of generating recommendations, extracting latent factors for semantic search, and clustering users by cross-sell potential.</p>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#1-setup-and-sample-data","level":2,"title":"1. Setup and Sample Data","text":"<p>We'll start by loading sample transaction data into spark.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pyspark.sql import SparkSession\nimport rusket\n\nspark = SparkSession.builder.getOrCreate()\n\n# Create dummy sample data for articles and purchases\npurchases = pd.DataFrame({\n    \"customer_id\": np.random.randint(0, 1000, size=5000),\n    \"article_id\": np.random.randint(0, 500, size=5000),\n    \"sales_amount\": np.random.exponential(50, size=5000),\n})\n\n# Read in PySpark (simulate loading from a bronze layer)\nspark_purchases = spark.createDataFrame(purchases)\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#2-model-training","level":2,"title":"2. Model Training","text":"<p>We fit the high-performance ALS model using implicit feedback (e.g., sales amounts).</p> <pre><code># Train the model, optionally passing pandas or polars dataframe directly \n# to avoid heavy JVM-to-Python serialization overhead when feasible\nals = rusket.ALS(\n    factors=64, \n    iterations=15, \n    alpha=40.0,\n    seed=42\n).from_transactions(\n    spark_purchases.toPandas(), \n    transaction_col=\"customer_id\",\n    item_col=\"article_id\",\n    rating_col=\"sales_amount\",\n)\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#3-extracting-and-normalizing-latent-space-embeddings","level":2,"title":"3. Extracting and Normalizing Latent Space (Embeddings)","text":"<p>Rather than keeping factors in memory or scoring one by one, we can export the trained underlying latent factors for semantic indexing or vector database lookups using LanceDB, FAISS, or Pinecone.</p> <pre><code># Export normalized vectors directly back to Spark DataFrames!\nuser_factors_df = als.export_user_factors(normalize=True, format=\"spark\")\nitem_factors_df = als.export_factors(normalize=True, format=\"spark\")\n\n# Save to your Delta Lake silver tier\nuser_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.user_embeddings\")\nitem_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.item_embeddings\")\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#4-dimensionality-reduction-visualization","level":2,"title":"4. Dimensionality Reduction &amp; Visualization","text":"<p>We can map our embeddings down to 3D and visualize them using Principal Component Analysis (PCA).</p> <pre><code># rusket has a built-in visualization utility utilizing sklearn PCA and Plotly\nfig = als.visualize_factors(labels=False)\nfig.show()\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#5-high-speed-batch-scoring","level":2,"title":"5. High-Speed Batch Scoring","text":"<p>We usually want to assign cross-sell scores to all users. Instead of a slow loop, <code>batch_recommend</code> accelerates this seamlessly across all cores using Rust.</p> <pre><code># Native Rust Rayon parallelism. Extremely fast.\nrecommendations = als.batch_recommend(n=20, exclude_seen=True, format=\"spark\")\n\n# Write out recommendations directly\nrecommendations.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_layer.cross_sell_predictions\")\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"databricks_cookbook/#6-business-value-potential-clustering","level":2,"title":"6. Business Value: \"Potential\" Clustering","text":"<p>Using Databricks SQL or DataFrame APIs, we can categorize these recommendations into actionable tiers for email marketing queues (e.g. High / Medium / Low potential):</p> <pre><code>import pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\n\n# Define quantiles over the `score` per item or overall\npercent rank window\nw = Window.partitionBy(\"item_id\").orderBy(F.col(\"score\").desc())\n\nclustered = recommendations.withColumn(\n    \"percent_rank\", F.percent_rank().over(w)\n).withColumn(\n    \"potential\", \n    F.when(F.col(\"percent_rank\") &lt;= 0.2, \"High\")\n     .when(F.col(\"percent_rank\") &lt;= 0.6, \"Medium\")\n     .otherwise(\"Low\")\n)\n\ndisplay(clustered.filter(F.col(\"potential\") == \"High\"))\n</code></pre>","path":["Databricks ALS Cross-Sell Cookbook"],"tags":[]},{"location":"polars/","level":1,"title":"Polars Support","text":"<p>rusket accepts <code>polars.DataFrame</code> natively alongside pandas, via the Arrow-backed zero-copy path.</p>","path":["User Guide","Polars Support"],"tags":[]},{"location":"polars/#installation","level":2,"title":"Installation","text":"<p>Install rusket with the Polars extra:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>This pins <code>polars&gt;=0.20</code>. If you already have Polars installed, you can also just <code>pip install rusket</code>.</p>","path":["User Guide","Polars Support"],"tags":[]},{"location":"polars/#usage","level":2,"title":"Usage","text":"<p>The <code>fpgrowth</code> function detects Polars DataFrames automatically — no extra parameters needed:</p> <pre><code>import polars as pl\nfrom rusket import AutoMiner\n\ndf = pl.DataFrame({\n    \"milk\":  [True, True,  False, True],\n    \"bread\": [True, False, True,  True],\n    \"eggs\":  [False, True, True,  True],\n})\n\nmodel = AutoMiner(df, min_support=0.5)\nfreq = model.mine(use_colnames=True)\nrules = model.association_rules(metric=\"lift\", min_threshold=1.0)\n</code></pre> <p>Note</p> <p><code>AutoMiner</code> always returns a pandas DataFrame, regardless of input type.</p>","path":["User Guide","Polars Support"],"tags":[]},{"location":"polars/#how-it-works","level":2,"title":"How it works","text":"<p>The Polars path uses <code>polars.DataFrame.to_numpy()</code> which returns an Arrow-backed NumPy buffer — zero-copy for numeric dtypes.</p> <pre><code>Polars DataFrame\n    │\n    ▼  df.to_numpy()  (zero-copy for bool/int dtypes)\nnumpy uint8 array\n    │\n    ▼  AutoMiner.from_transactions(...)  (Rust, PyO3 ReadonlyArray2&lt;u8&gt;)\nRust FP-Tree / Eclat mining\n    │\n    ▼\npandas DataFrame  [support, itemsets]\n</code></pre>","path":["User Guide","Polars Support"],"tags":[]},{"location":"polars/#supported-dtypes","level":2,"title":"Supported dtypes","text":"Polars dtype Supported <code>Boolean</code> ✅ <code>Int8 / Int16 / Int32 / Int64</code> ✅ (0/1 values) <code>UInt8 / UInt16 / UInt32 / UInt64</code> ✅ (0/1 values) <code>Float32 / Float64</code> ⚠️ (0.0/1.0 values, cast to uint8) Categorical / String ❌ (pre-encode with <code>get_dummies</code>) <p>Lazy frames</p> <p>Pass <code>.collect()</code> before supplying it to <code>AutoMiner</code> if you have a <code>LazyFrame</code>: <pre><code>freq = AutoMiner(lazy_df.collect(), min_support=0.3).mine(use_colnames=True)\n</code></pre></p>","path":["User Guide","Polars Support"],"tags":[]},{"location":"quickstart/","level":1,"title":"Quick Start","text":"<p>Install rusket and run your first Market Basket Analysis in minutes.</p>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#installation","level":2,"title":"Installation","text":"pipuvconda <pre><code>pip install rusket\n</code></pre> <pre><code>uv add rusket\n</code></pre> <pre><code>pip install rusket  # rusket is not on conda-forge yet\n</code></pre> <p>To also enable Polars support:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#business-scenario-supermarket-cross-selling","level":2,"title":"Business Scenario — Supermarket Cross-Selling","text":"","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#step-1-prepare-your-data","level":2,"title":"Step 1 — Prepare your data","text":"<p><code>AutoMiner</code> expects a one-hot encoded DataFrame where rows are transactions and columns are products.</p> <pre><code>import pandas as pd\nfrom rusket import AutoMiner\n\norders = pd.DataFrame({\n    \"receipt_id\": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1004],\n    \"product\":    [\"milk\", \"bread\", \"butter\",\n                   \"milk\", \"eggs\",\n                   \"bread\", \"butter\",\n                   \"milk\", \"bread\", \"eggs\", \"coffee\"],\n})\n\nmodel = AutoMiner.from_transactions(orders, transaction_col=\"receipt_id\", item_col=\"product\", min_support=0.4)\n</code></pre>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#step-2-mine-frequent-product-combinations","level":2,"title":"Step 2 — Mine frequent product combinations","text":"<pre><code>freq = model.mine(use_colnames=True)\nprint(freq.sort_values(\"support\", ascending=False))\n</code></pre> <p>Tip</p> <p><code>AutoMiner</code> picks <code>Eclat</code> for sparse data (density &lt; 0.15) and <code>FPGrowth</code> for dense data.</p>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#step-3-generate-frequently-bought-together-rules","level":2,"title":"Step 3 — Generate \"Frequently Bought Together\" rules","text":"<pre><code>rules = model.association_rules(metric=\"confidence\", min_threshold=0.6)\nprint(rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n</code></pre>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#recommendations","level":2,"title":"Recommendations","text":"<pre><code>basket_contents = [\"milk\", \"bread\"]\nsuggestions = model.recommend_items(basket_contents, n=3)\n</code></pre>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#billion-scale-streaming","level":2,"title":"Billion-Scale Streaming","text":"<pre><code>from rusket import FPMiner\n\nminer = FPMiner(n_items=500_000)\n\nfor chunk in pd.read_parquet(\"sales_fact.parquet\", chunksize=10_000_000):\n    txn  = chunk[\"receipt_id\"].to_numpy(dtype=\"int64\")\n    item = chunk[\"product_idx\"].to_numpy(dtype=\"int32\")\n    miner.add_chunk(txn, item)\n\nfreq  = miner.mine(min_support=0.001, max_len=3)\nrules = miner.association_rules()\n</code></pre> <p>Tip</p> <p>Peak Python memory = one chunk. Rust holds the per-transaction item lists. The final mining step passes CSR arrays directly — zero copies.</p>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#direct-csr-path","level":3,"title":"Direct CSR path","text":"<pre><code>from scipy import sparse as sp\nfrom rusket import AutoMiner\n\ncsr = sp.csr_matrix(\n    (np.ones(len(receipt_ids), dtype=np.int8), (receipt_ids, sku_indices)),\n    shape=(n_receipts, n_skus),\n)\nfreq = AutoMiner(csr).mine(min_support=0.001, column_names=sku_names)\n</code></pre>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"quickstart/#whats-next","level":2,"title":"What's Next?","text":"<ul> <li>API Reference</li> <li>Polars Support</li> <li>Recommender Workflows</li> </ul>","path":["Getting Started","Quick Start"],"tags":[]},{"location":"recommender/","level":1,"title":"Recommender Workflows","text":"<p>Three complementary recommendation strategies: cart add-ons, personalised \"For You\", and hybrid models.</p> <p><code>rusket</code> provides three complementary recommendation strategies that cover the most common revenue-generating use cases in e-commerce, retail, and content platforms.</p> Strategy Best for API \"Frequently Bought Together\" Cart add-ons, shelf placement <code>FPGrowth</code> / <code>AutoMiner</code> \"For You\" (Personalised) Homepage, email, loyalty <code>ALS</code> / <code>BPR</code> Hybrid Blend both signals <code>Recommender</code>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#frequently-bought-together-cart-recommendations","level":2,"title":"\"Frequently Bought Together\" — Cart Recommendations","text":"<pre><code>import pandas as pd\nfrom rusket import AutoMiner\n\ncheckouts = pd.DataFrame({\n    \"receipt_id\": [1, 1, 2, 2, 2, 3, 3, 4, 4, 4],\n    \"product\":    [\"espresso_beans\", \"grinder\",\n                   \"espresso_beans\", \"milk_frother\", \"travel_mug\",\n                   \"grinder\", \"milk_frother\",\n                   \"espresso_beans\", \"grinder\", \"descaler\"],\n})\n\nmodel = AutoMiner.from_transactions(\n    checkouts,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    min_support=0.3,\n)\n\nbasket   = [\"grinder\"]\nadd_ons  = model.recommend_items(basket, n=3)\nrules = model.association_rules(metric=\"lift\", min_threshold=1.0)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#for-you-personalised-recommendations-with-als-bpr","level":2,"title":"\"For You\" — Personalised Recommendations with ALS / BPR","text":"<ul> <li>ALS — best for score prediction and serendipitous discovery</li> <li>BPR — best when you care only about top-N ranking</li> </ul> <pre><code>from rusket import ALS, BPR\n\npurchases = pd.DataFrame({\n    \"customer_id\": [1001, 1001, 1001, 1002, 1002, 1003],\n    \"sku\":         [\"A10\", \"B22\", \"C15\",  \"A10\", \"D33\",  \"B22\"],\n    \"revenue\":     [29.99, 49.00, 9.99,  29.99, 15.00, 49.00],\n})\n\nals = ALS(factors=64, iterations=15, alpha=40.0).from_transactions(\n    purchases,\n    user_col=\"customer_id\",\n    item_col=\"sku\",\n    rating_col=\"revenue\",\n)\n\nbpr = BPR(factors=64, learning_rate=0.05, iterations=150).fit(user_item_csr)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#getting-recommendations","level":3,"title":"Getting recommendations","text":"<pre><code>items, scores = als.recommend_items(user_id=1001, n=5, exclude_seen=True)\ntop_customers, scores = als.recommend_users(item_id=\"D33\", n=100)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#the-hybrid-recommender","level":2,"title":"The Hybrid Recommender","text":"<pre><code>from rusket import ALS, AutoMiner, Recommender\n\nals  = ALS(factors=64, iterations=15).fit(user_item_csr)\nmodel = AutoMiner(basket_ohe, min_support=0.01)\nfreq  = model.mine()\nrules = model.association_rules()\nrec = Recommender(model=als, rules_df=rules)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#1-personalised-homepage-for-you","level":3,"title":"1. Personalised homepage (\"For You\")","text":"<pre><code>items, scores = rec.recommend_for_user(user_id=1001, n=5)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#2-hybrid-cf-product-embeddings","level":3,"title":"2. Hybrid — CF + product embeddings","text":"<pre><code>rec = Recommender(model=als, rules_df=rules, item_embeddings=product_vectors)\n\nitems, scores = rec.recommend_for_user(\n    user_id=1001, n=5, alpha=0.7,\n    target_item_for_semantic=\"B22\",\n)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#3-cart-based-frequently-bought-together","level":3,"title":"3. Cart-based \"Frequently Bought Together\"","text":"<pre><code>cart = [\"espresso_beans\", \"grinder\"]\nadd_ons = rec.recommend_for_cart(cart, n=3)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#4-databricks-batch-scoring-high-speed-cross-sell-generation","level":3,"title":"4. Databricks / Batch scoring — high speed cross-sell generation","text":"<p>Avoid slow Python <code>for</code> loops by using the Rust-accelerated <code>batch_recommend</code> to score all users across all CPU cores simultaneously.</p> <pre><code># Returns a native Polars DataFrame instantly: [user_id, item_id, score]\nrecommendations_pl = als.batch_recommend(n=10, format=\"polars\")\n\n# Need it in Delta Lake? Export factors and scores directly to Spark DataFrames!\nuser_factors_df = als.export_user_factors(normalize=True, format=\"spark\")\nitem_factors_df = als.export_factors(normalize=True, format=\"spark\")\n\n# Save to Delta\nuser_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.user_embeddings\")\nitem_factors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_layer.item_embeddings\")\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#item-to-item-similarity-you-may-also-like","level":2,"title":"Item-to-Item Similarity — \"You May Also Like\"","text":"<pre><code>from rusket import ALS\n\nals = ALS(factors=128).fit(interactions)\nsimilar_skus, similarity_scores = als.similar_items(item_id=\"B22\", n=4)\n</code></pre> <p>Latent-space similarity</p> <p>Discovers implicit relationships — a premium coffee grinder may cluster tightly with an espresso machine even if they're rarely purchased in the same basket, because the same type of customer buys both.</p>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#cross-selling-potential-scoring","level":2,"title":"Cross-Selling Potential Scoring","text":"<pre><code>from rusket import score_potential\n\npotential = score_potential(\n    user_history=purchase_histories,\n    model=als,\n    target_categories=accessory_skus,\n)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#analytics-helpers","level":2,"title":"Analytics Helpers","text":"","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#substitute-cannibalising-products","level":3,"title":"Substitute / Cannibalising Products","text":"<pre><code>from rusket import find_substitutes\n\nsubs = find_substitutes(rules_df, max_lift=0.8)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#customer-saturation","level":3,"title":"Customer Saturation","text":"<pre><code>from rusket import customer_saturation\n\nsaturation = customer_saturation(\n    purchases_df, user_col=\"customer_id\", category_col=\"category_id\",\n)\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#vector-db-export","level":2,"title":"Vector DB Export","text":"<pre><code>df_vectors = als.export_item_factors(include_labels=True)\n\nimport lancedb\ndb    = lancedb.connect(\"./product_vectors\")\ntable = db.create_table(\"skus\", data=df_vectors, mode=\"overwrite\")\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"recommender/#graph-analytics-product-community-detection","level":2,"title":"Graph Analytics — Product Community Detection","text":"<pre><code>from rusket.viz import to_networkx\nimport networkx as nx\n\nG = to_networkx(rules_df, edge_attr=\"lift\")\ncentrality = nx.pagerank(G, weight=\"lift\")\ntop_gateway = sorted(centrality, key=centrality.get, reverse=True)[:5]\n</code></pre>","path":["User Guide","Recommender Workflows"],"tags":[]},{"location":"spark/","level":1,"title":"PySpark Integration","text":"<p>Distributed execution of rusket algorithms across Databricks or on-prem Hadoop clusters via zero-copy Apache Arrow transfers.</p> <p><code>rusket</code> integrates with PySpark clusters via zero-copy Apache Arrow transfers, enabling distributed execution of all its core algorithms without manual serialisation.</p> <p>All distributed functions live in <code>rusket.spark</code> and use <code>applyInArrow</code> (Spark 3.4+) with <code>applyInPandas</code> as a fallback for older versions.</p>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#setup","level":2,"title":"Setup","text":"<pre><code>spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#mine_grouped-distributed-market-basket-analysis-per-store-region","level":2,"title":"<code>mine_grouped</code> — Distributed Market Basket Analysis per Store / Region","text":"<pre><code>import rusket.spark\n\nspark_df = spark.table(\"gold.baskets_ohe\")\n\nfreq_df = rusket.spark.mine_grouped(\n    df=spark_df,\n    group_col=\"store_id\",\n    min_support=0.05,\n    method=\"auto\",\n    use_colnames=True,\n    max_len=3,\n)\n# Output: store_id (string) | support (double) | itemsets (array&lt;string&gt;)\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#rules_grouped-distributed-association-rules-per-segment","level":2,"title":"<code>rules_grouped</code> — Distributed Association Rules per Segment","text":"<pre><code>rules_df = rusket.spark.rules_grouped(\n    df=freq_df,\n    group_col=\"store_id\",\n    num_itemsets={\"store_A\": 45_000, \"store_B\": 12_300},\n    metric=\"confidence\",\n    min_threshold=0.6,\n)\n# Output: store_id | antecedents | consequents | confidence | lift | ...\n</code></pre> <p>Full end-to-end regional pipeline:</p> <pre><code>freq_df  = rusket.spark.mine_grouped(spark_df, group_col=\"store_id\", min_support=0.05)\nrules_df = rusket.spark.rules_grouped(freq_df, group_col=\"store_id\", num_itemsets=20_000)\nrules_df.write.mode(\"overwrite\").saveAsTable(\"gold.per_store_rules\")\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#prefixspan_grouped-distributed-customer-journey-analysis","level":2,"title":"<code>prefixspan_grouped</code> — Distributed Customer Journey Analysis","text":"<pre><code>seq_df = rusket.spark.prefixspan_grouped(\n    df=spark_df,\n    group_col=\"region\",\n    user_col=\"customer_id\",\n    time_col=\"event_ts\",\n    item_col=\"product_id\",\n    min_support=100,\n    max_len=4,\n)\n# Output: region | support (long) | sequence (array&lt;string&gt;)\n# Example:\n# EMEA | 432 | [broadband, mobile, tv_bundle, cancel]\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#hupm_grouped-distributed-high-profit-bundle-discovery","level":2,"title":"<code>hupm_grouped</code> — Distributed High-Profit Bundle Discovery","text":"<pre><code>hupm_df = rusket.spark.hupm_grouped(\n    df=spark_df,\n    group_col=\"region\",\n    transaction_col=\"receipt_id\",\n    item_col=\"product_id\",\n    utility_col=\"margin\",\n    min_utility=500.0,\n    max_len=3,\n)\n# Output: region | utility (double) | itemset (array&lt;long&gt;)\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#recommend_batches-overnight-batch-personalisation-at-scale","level":2,"title":"<code>recommend_batches</code> — Overnight Batch Personalisation at Scale","text":"<pre><code>from rusket import ALS\n\nals = ALS(factors=64, iterations=15).fit(user_item_csr)\n\nrec_df = rusket.spark.recommend_batches(\n    df=spark.table(\"silver.user_sessions\"),\n    model=als,\n    user_col=\"customer_id\",\n    k=10,\n)\n# Output: customer_id (string) | recommended_items (array&lt;int&gt;)\n\nrec_df.write.mode(\"overwrite\").saveAsTable(\"gold.daily_recommendations\")\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#mine_spark-global-mining-via-fpminer-streaming","level":2,"title":"<code>mine_spark</code> — Global Mining via FPMiner Streaming","text":"<pre><code>from rusket.streaming import mine_spark\n\nfreq_df = mine_spark(\n    spark_df=spark.table(\"silver.order_lines\"),\n    n_items=200_000,\n    txn_col=\"order_id\",\n    item_col=\"sku_index\",\n    min_support=0.001,\n    max_len=3,\n)\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"spark/#to_spark","level":2,"title":"<code>to_spark</code>","text":"<p>Convert a Pandas or Polars DataFrame to a PySpark DataFrame:</p> <pre><code>spark_df = rusket.spark.to_spark(spark_session, df)\n</code></pre>","path":["User Guide","PySpark Integration"],"tags":[]},{"location":"streaming/","level":1,"title":"Streaming &amp; Big Data","text":"<p>Handle billions of rows of transaction data without running out of memory using rusket's <code>FPMiner</code> streaming accumulator.</p> <p>When dealing with extremely large transactional datasets — such as billions of rows of e-commerce clickstreams or year-long retail logs — loading the entire one-hot encoded matrix into RAM (even as a sparse matrix) may exhaust your system's memory.</p> <p>To solve this, <code>rusket</code> includes <code>FPMiner</code>, a highly optimized streaming accumulator written completely in Rust.</p>","path":["User Guide","Streaming &amp; Big Data"],"tags":[]},{"location":"streaming/#the-streaming-concept","level":2,"title":"The Streaming Concept","text":"<p>Instead of converting a \"long-format\" event log <code>(user_id, item_id)</code> into a massive <code>N × M</code> sparse matrix, the <code>FPMiner</code> accepts small, memory-safe chunks of raw integers.</p> <p>Rust accumulates these <code>(transaction_id, item_id)</code> pairs internally using a highly efficient <code>HashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>. Because this happens incrementally:</p> <ol> <li>Python memory overhead is strictly limited to the size of a single chunk (e.g., 10 million rows).</li> <li>Matrix pivoting (Group-By operations) is avoided entirely.</li> </ol> <pre><code>graph LR\n    A[\"Parquet File&lt;br/&gt;(1 Billion Rows)\"] --&gt; B[\"Python Chunk&lt;br/&gt;(10M Rows)\"]\n    B --&gt;|Stream| C[\"Rust FPMiner&lt;br/&gt;(Accumulates inside Heap)\"]\n    C --&gt;|Next Chunk| A\n    C --&gt;|mine()| D[\"Frequent Itemsets&lt;br/&gt;pd.DataFrame\"]</code></pre>","path":["User Guide","Streaming &amp; Big Data"],"tags":[]},{"location":"streaming/#reading-from-disk-parquet-csv","level":2,"title":"Reading from Disk (Parquet / CSV)","text":"<pre><code>import pandas as pd\nfrom rusket import FPMiner\n\nminer = FPMiner(n_items=100_000)\n\nfor chunk in pd.read_parquet(\"massive_event_log.parquet\", chunksize=10_000_000):\n    txn_ids = chunk[\"user_session\"].to_numpy(dtype=\"int64\")\n    item_ids = chunk[\"product_id\"].to_numpy(dtype=\"int32\")\n    miner.add_chunk(txn_ids, item_ids)\n\nfreq_itemsets = miner.mine(\n    min_support=0.005,\n    max_len=4,\n    method=\"auto\"\n)\n</code></pre>","path":["User Guide","Streaming &amp; Big Data"],"tags":[]},{"location":"streaming/#arrow-and-duckdb-integrations","level":2,"title":"Arrow and DuckDB Integrations","text":"<p>For even higher performance, bypass Pandas entirely using <code>pyarrow</code> underneath a DuckDB query engine:</p> <pre><code>import duckdb\nfrom rusket.streaming import mine_duckdb\n\ncon = duckdb.connect(\"my_analytics_db.duckdb\")\n\nfreq = mine_duckdb(\n    con=con,\n    query=\"SELECT session_id, product_id FROM sales WHERE region = 'EMEA'\",\n    n_items=50_000,\n    txn_col=\"session_id\",\n    item_col=\"product_id\",\n    min_support=0.01,\n    chunk_size=5_000_000\n)\n</code></pre>","path":["User Guide","Streaming &amp; Big Data"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/","level":1,"title":"Rusket vs MLxtend: Market Basket Analysis at Scale","text":"<p>In this notebook we use a realistic synthetic retail dataset — with genuine co-purchase correlations and a pair of competing substitute brands — to show why <code>rusket</code> is the fastest association-rule library in Python.</p> <p>We then use the discovered rules to perform Assortment Optimization (Cannibalization Detection) and visualize the results with Plotly.</p> <pre><code>import os\nimport pathlib\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\nfrom mlxtend.frequent_patterns import fpgrowth as mlxtend_fpgrowth\n\nfrom rusket import association_rules, mine\nfrom rusket.analytics import find_substitutes\n\n# Crisp dark theme for all charts\npio.templates.default = \"plotly_dark\"\n\n# Nicer float display in DataFrames\npd.options.display.float_format = \"{:.3f}\".format\n\n# Charts saved as self-contained HTML for MkDocs embedding\nCHARTS_DIR = pathlib.Path(\"docs/notebooks/charts\")\nCHARTS_DIR.mkdir(parents=True, exist_ok=True)\n\ndef save_chart(fig, name: str) -&gt; None:\n    path = CHARTS_DIR / f\"{name}.html\"\n    fig.write_html(str(path), include_plotlyjs=\"cdn\", full_html=True)\n    print(f\"Chart saved → {path}\")\n</code></pre>","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/#1-generating-a-realistic-correlated-dataset","level":2,"title":"1. Generating a Realistic Correlated Dataset","text":"<p>A purely random basket matrix (the typical benchmark approach) has no real signal: every item pair will have lift ≈ 1.0, and no rules will pass a meaningful confidence threshold. Instead we generate baskets from three customer segments with strong co-purchase behaviour, plus two competing cola brands that are negatively correlated (lift &lt; 1 — genuine substitutes).</p> <pre><code>def generate_basket_data(n_transactions: int = 20_000, seed: int = 42) -&gt; pd.DataFrame:\n    \"\"\"\n    Segment-based basket generator with realistic co-purchase correlations.\n\n    Three segments create strong *positive* correlations (high lift).\n    Two competing cola brands are *negatively* correlated (lift ≈ 0.76, substitutes).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = n_transactions\n\n    cols = [\n        # Tech accessories cluster\n        \"Mouse\", \"Keyboard\", \"USB_Hub\", \"Webcam\",\n        # Barista / coffee cluster\n        \"Espresso_Beans\", \"Milk_Frother\", \"Travel_Mug\",\n        # Home-office cluster\n        \"Notebook\", \"Gel_Pen\", \"Highlighter\",\n        # Competing brands — negative correlation\n        \"Cola_A\", \"Cola_B\",\n    ]\n    df = pd.DataFrame(False, index=range(n), columns=cols)\n\n    # Tech buyers (40%) cluster\n    seg = rng.random(n) &lt; 0.40\n    for p in [\"Mouse\", \"Keyboard\", \"USB_Hub\", \"Webcam\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.75\n\n    # Coffee buyers (35%) cluster\n    seg = rng.random(n) &lt; 0.35\n    for p in [\"Espresso_Beans\", \"Milk_Frother\", \"Travel_Mug\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.78\n\n    # Home-office buyers (45%) cluster\n    seg = rng.random(n) &lt; 0.45\n    for p in [\"Notebook\", \"Gel_Pen\", \"Highlighter\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.72\n\n    # Substitutes: Cola_A is popular (~38%)\n    # Cola_B can appear with A at only 16% probability → co-occurrence ~6%\n    # Independence would predict ~8% → lift ≈ 0.76\n    a_mask = rng.random(n) &lt; 0.38\n    b_with_a = a_mask &amp; (rng.random(n) &lt; 0.16)\n    b_only = (~a_mask) &amp; (rng.random(n) &lt; 0.24)\n    df[\"Cola_A\"] = a_mask\n    df[\"Cola_B\"] = b_with_a | b_only\n\n    return df\n\n\ndf = generate_basket_data(n_transactions=20_000)\nprint(f\"Dataset: {df.shape[0]:,} baskets × {df.shape[1]} products\")\nprint(f\"Avg basket size: {df.sum(axis=1).mean():.1f} items\")\nprint(f\"Cola_A support: {df['Cola_A'].mean():.3f}\")\nprint(f\"Cola_B support: {df['Cola_B'].mean():.3f}\")\nprint(f\"Cola_A &amp; Cola_B co-occurrence: {(df['Cola_A'] &amp; df['Cola_B']).mean():.3f}\")\ndf.head(5)\n</code></pre> <pre><code>Dataset: 20,000 baskets × 12 products\nAvg basket size: 3.6 items\nCola_A support: 0.380\nCola_B support: 0.212\nCola_A &amp; Cola_B co-occurrence: 0.061\n</code></pre> Mouse Keyboard USB_Hub Webcam Espresso_Beans Milk_Frother Travel_Mug Notebook Gel_Pen Highlighter Cola_A Cola_B 0 False False False False False False False False False False True False 1 False False False False False False False False False False True False 2 False False False False False False False False False False False False 3 False False False False False False False True True True False False 4 True True False False False False False True True False False True","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/#2-the-benchmark-rusket-vs-mlxtend","level":2,"title":"2. The Benchmark: Rusket vs MLxtend","text":"<p>We mine all product combinations appearing in at least 5% of baskets. <code>rusket</code> provides FP-Growth and Eclat — both written entirely in Rust.</p> <pre><code>min_support = 0.05\n\n# --- Rusket FP-Growth ---\nt0 = time.time()\nrusket_res = mine(df, min_support=min_support, method=\"fpgrowth\", use_colnames=True)\nrusket_time = time.time() - t0\nprint(f\"🚀 Rusket FP-Growth: {rusket_time:.4f}s  ({len(rusket_res):,} itemsets)\")\n\n# --- Rusket Eclat ---\nt0 = time.time()\nrusket_eclat_res = mine(df, min_support=min_support, method=\"eclat\", use_colnames=True)\nrusket_eclat_time = time.time() - t0\nprint(f\"🚀 Rusket Eclat:     {rusket_eclat_time:.4f}s\")\n\n# --- MLxtend FP-Growth ---\nt0 = time.time()\nmlxtend_res = mlxtend_fpgrowth(df, min_support=min_support, use_colnames=True)\nmlxtend_time = time.time() - t0\nprint(f\"🐢 MLxtend FP-Growth:{mlxtend_time:.4f}s  ({len(mlxtend_res):,} itemsets)\")\nprint(\"-\" * 50)\nprint(f\"🏆 Rusket is {mlxtend_time / rusket_time:.1f}× faster than MLxtend!\")\n</code></pre> <pre><code>🚀 Rusket FP-Growth: 0.0129s  (226 itemsets)\n🚀 Rusket Eclat:     0.0027s\n🐢 MLxtend FP-Growth:0.0421s  (226 itemsets)\n--------------------------------------------------\n🏆 Rusket is 3.3× faster than MLxtend!\n</code></pre> <pre><code>fig = px.bar(\n    x=[\"MLxtend (Python)\", \"Rusket Eclat (Rust)\", \"Rusket FP-Growth (Rust)\"],\n    y=[mlxtend_time, rusket_eclat_time, rusket_time],\n    title=\"⏱ Execution Time — Lower is Better\",\n    labels={\"x\": \"Implementation\", \"y\": \"Time (seconds)\"},\n    color=[\"baseline\", \"optimized\", \"optimized\"],\n    color_discrete_map={\"baseline\": \"#EF553B\", \"optimized\": \"#00CC96\"},\n    text_auto=\".2f\",\n)\nfig.update_traces(textfont_size=15)\nfig.update_layout(showlegend=False, title_font_size=20)\nsave_chart(fig, \"benchmark\")\nfig.show()\n</code></pre> <pre><code>Chart saved → docs/notebooks/charts/benchmark.html\n</code></pre>","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/#3-generating-cross-sell-rules","level":2,"title":"3. Generating Cross-Sell Rules","text":"<p>From the frequent itemsets we generate association rules — \"If a customer buys A, they will also buy B\" — ranked by lift (how much more likely the co-purchase is versus random chance). Lift &gt; 1 means a genuine affinity; lift &lt; 1 means the products repel each other.</p> <pre><code>t0 = time.time()\nrules = association_rules(rusket_res, num_itemsets=len(df), min_threshold=0.01)\nprint(f\"Generated {len(rules):,} rules in {time.time() - t0:.5f}s\")\n\n# Top cross-sell rules by lift\n(\n    rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]\n    .sort_values(\"lift\", ascending=False)\n    .head(8)\n    .assign(\n        support=lambda d: d[\"support\"].round(3),\n        confidence=lambda d: d[\"confidence\"].round(3),\n        lift=lambda d: d[\"lift\"].round(2),\n    )\n    .reset_index(drop=True)\n)\n</code></pre> <pre><code>Generated 1,412 rules in 0.00574s\n</code></pre> antecedents consequents support confidence lift 0 frozenset({Gel_Pen, Espresso_Beans}) frozenset({Notebook, Travel_Mug}) 0.051 0.588 6.670 1 frozenset({Notebook, Travel_Mug}) frozenset({Gel_Pen, Espresso_Beans}) 0.051 0.583 6.670 2 frozenset({Notebook, Espresso_Beans}) frozenset({Gel_Pen, Travel_Mug}) 0.051 0.586 6.570 3 frozenset({Gel_Pen, Travel_Mug}) frozenset({Notebook, Espresso_Beans}) 0.051 0.576 6.570 4 frozenset({Milk_Frother, Gel_Pen}) frozenset({Highlighter, Travel_Mug}) 0.050 0.571 6.560 5 frozenset({Highlighter, Travel_Mug}) frozenset({Milk_Frother, Gel_Pen}) 0.050 0.580 6.560 6 frozenset({Milk_Frother, Gel_Pen}) frozenset({Notebook, Travel_Mug}) 0.050 0.571 6.480 7 frozenset({Notebook, Travel_Mug}) frozenset({Milk_Frother, Gel_Pen}) 0.050 0.573 6.480","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/#4-assortment-optimization-substitute-detection","level":2,"title":"4. Assortment Optimization — Substitute Detection","text":"<p>Most tutorials stop at finding items bought together. But what about items that prevent each other from being bought?</p> <p>If Product A and Product B are both individually popular but their co-occurrence is lower than random chance (lift &lt; 1), they are substitutes — customers choose one instead of the other. Retailers use this to:</p> <ul> <li>Delist redundant SKUs (reduce warehouse cost)</li> <li>Negotiate better terms with the weaker brand</li> <li>Optimise shelf-space by not displaying competing items side-by-side</li> </ul> <p><code>rusket</code> provides <code>find_substitutes</code> out of the box:</p> <pre><code>substitutes = find_substitutes(rules, max_lift=0.9)\nprint(f\"Found {len(substitutes)} cannibalizing product pair(s).\")\n\n(\n    substitutes[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]\n    .assign(\n        support=lambda d: d[\"support\"].round(3),\n        confidence=lambda d: d[\"confidence\"].round(3),\n        lift=lambda d: d[\"lift\"].round(3),\n    )\n    .reset_index(drop=True)\n)\n</code></pre> <pre><code>Found 2 cannibalizing product pair(s).\n</code></pre> antecedents consequents support confidence lift 0 frozenset({Cola_B}) frozenset({Cola_A}) 0.061 0.290 0.762 1 frozenset({Cola_A}) frozenset({Cola_B}) 0.061 0.162 0.762","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/1_online_retail_basket_analysis/#visualizing-the-product-strategy-quadrant","level":3,"title":"Visualizing the Product Strategy Quadrant","text":"<p>Plot every rule as a point: Confidence (x-axis) vs Lift (y-axis).</p> <ul> <li>Top-right (high confidence, high lift): perfect cross-sell candidates</li> <li>Below the dashed line (lift &lt; 1): substitutes / cannibalizing products</li> </ul> <pre><code># Plot only singleton→singleton rules for readability\nsingleton_rules = rules[\n    (rules[\"antecedents\"].apply(len) == 1)\n    &amp; (rules[\"consequents\"].apply(len) == 1)\n].copy()\n\nsingleton_rules[\"rule_label\"] = (\n    singleton_rules[\"antecedents\"].apply(lambda x: next(iter(x)))\n    + \" → \"\n    + singleton_rules[\"consequents\"].apply(lambda x: next(iter(x)))\n)\n\nfig = px.scatter(\n    singleton_rules,\n    x=\"confidence\",\n    y=\"lift\",\n    size=\"support\",\n    color=\"lift\",\n    hover_name=\"rule_label\",\n    hover_data={\"confidence\": \":.3f\", \"lift\": \":.3f\", \"support\": \":.3f\"},\n    color_continuous_scale=\"RdYlGn\",\n    title=\"📊 Product Strategy: Cross-Sells vs Substitutes\",\n    labels={\"confidence\": \"Confidence\", \"lift\": \"Lift\"},\n)\nfig.add_hline(\n    y=1.0, line_dash=\"dash\", line_color=\"white\",\n    annotation_text=\"Lift = 1.0  (independent)\",\n    annotation_position=\"top left\",\n)\nfig.add_annotation(\n    x=0.85, y=singleton_rules[\"lift\"].max() * 0.92,\n    text=\"✅ Cross-sell\", showarrow=False,\n    font=dict(color=\"#00CC96\", size=14),\n)\nfig.add_annotation(\n    x=0.18, y=0.60,\n    text=\"⚠️ Substitutes\", showarrow=False,\n    font=dict(color=\"#EF553B\", size=14),\n)\nfig.update_layout(title_font_size=20)\nsave_chart(fig, \"product_strategy\")\nfig.show()\n</code></pre> <pre><code>Chart saved → docs/notebooks/charts/product_strategy.html\n</code></pre>","path":["User Guide","Notebooks","Rusket vs MLxtend: Market Basket Analysis at Scale"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/","level":1,"title":"Hybrid Recommendations &amp; User Potential with MovieLens","text":"<p>In this cookbook, we will use the classic MovieLens 100k dataset to showcase <code>rusket</code>'s collaborative filtering models (<code>ALS</code> and <code>BPR</code>) and demonstrate how to deploy the ultimate hybrid engine using <code>NextBestAction</code>.</p> <pre><code>import time\n\nimport numpy as np\nimport pandas as pd\n\nfrom rusket import ALS, BPR, NextBestAction\nfrom rusket.recommend import score_potential\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/#1-load-movielens-data","level":2,"title":"1. Load MovieLens Data","text":"<p>We will download and parse the MovieLens 100k dataset into a Pandas DataFrame representing user ID, movie ID, and rating (1-5).</p> <pre><code>import os\nimport urllib.request\nimport zipfile\n\nif not os.path.exists(\"ml-100k\"):\n    url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n    urllib.request.urlretrieve(url, \"ml-100k.zip\")\n    with zipfile.ZipFile(\"ml-100k.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(\".\")\n\ncolumns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\ndf = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=columns)\n\nprint(f\"Loaded {len(df):,} ratings!\")\ndf.head()\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/#2-fitting-alternating-least-squares-als","level":2,"title":"2. Fitting Alternating Least Squares (ALS)","text":"<p><code>rusket</code> provides <code>ALS</code>, a highly optimized collaborative filtering engine that learns latent representations of users based on their interaction histories.</p> <p>We can fit our model directly off the raw Pandas transaction dataframe using <code>fit_transactions</code>:</p> <pre><code>als_model = ALS(factors=64, iterations=15, alpha=15.0, regularization=0.01, seed=42)\n\nt0 = time.time()\nals_model.fit_transactions(df, user_col=\"user_id\", item_col=\"item_id\", rating_col=\"rating\")\nprint(f\"⚡ ALS training complete in {time.time() - t0:.4f}s!\")\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/#3-bayesian-personalized-ranking-bpr","level":2,"title":"3. Bayesian Personalized Ranking (BPR)","text":"<p>While ALS is great for general score predictions, BPR natively optimizes for Rank. This makes it the superior choice when your goal is purely to rank Top-N recommendations based entirely on implicit (binary) views or clicks, ignoring explicit star ratings.</p> <pre><code>bpr_model = BPR(factors=64, iterations=150, learning_rate=0.05, regularization=0.01)\n\n# Notice BPR trains on implicit behavior (user_col and item_col), completely ignoring ratings\nt0 = time.time()\nbpr_model.fit_transactions(df, user_col=\"user_id\", item_col=\"item_id\")\nprint(f\"⚡ BPR training complete in {time.time() - t0:.4f}s!\")\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/#4-next-best-action-hybrid-api","level":2,"title":"4. Next Best Action (Hybrid API)","text":"<p>The <code>NextBestAction</code> engine wraps these complicated matrices and index boundaries into a dead-simple business API for analysts. </p> <p>If we pass it our <code>als_model</code>, we can instantly ask for the top 5 next best products for a handful of target users.</p> <pre><code>nba = NextBestAction(als_model=als_model)\n\n# Target users from our CRM\ntarget_users = pd.DataFrame({\"customer_id\": [1, 5, 25, 42]})\n\n# Predict the best 3 movies for these users to watch next!\nrecommendations = nba.predict_next_chunk(target_users, user_col=\"customer_id\", k=3)\nrecommendations\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/2_movielens_recommendations/#5-marketing-potential-score","level":2,"title":"5. Marketing Potential Score","text":"<p>Want to launch an email marketing campaign for specific movies (say movies <code>10</code>, <code>50</code>, and <code>100</code>), but only want to email users who are highly primed to buy them? </p> <p>We can use the <code>score_potential</code> API to predict their exact likelihood of interaction across the entire customer base.</p> <pre><code>user_histories = df.groupby(\"user_id\")[\"item_id\"].apply(list).tolist()\n\npotential_scores = score_potential(user_history=user_histories, als_model=als_model, target_categories=[10, 50, 100])\n\nprint(\"Top 5 user scores for Movie ID 10:\")\n# Get users with the highest probability to interact with Movie 10\nmovie_10_scores = potential_scores[:, 0]  # First column corresponds to Item 10\nbest_users_for_campaign = np.argsort(movie_10_scores)[::-1][:5]\n\nfor u in best_users_for_campaign:\n    print(f\"User {u}: {movie_10_scores[u]:.2f}\")\n</code></pre>","path":["User Guide","Notebooks","Hybrid Recommendations &amp; User Potential with MovieLens"],"tags":[]},{"location":"notebooks/3_ecommerce_lifecycle_mining/","level":1,"title":"Sequential Pattern Mining (PrefixSpan)","text":"<p>In standard Market Basket Analysis, we look at the items inside a single checkout. However, if we want to model lifecycle purchasing or churn behavior, we need an algorithm that natively understands time.</p> <p>In this cookbook, we will mine Sequential Patterns using <code>rusket</code>'s blazing fast PrefixSpan implementation over an e-commerce clickstream log.</p> <pre><code>import time\n\nimport pandas as pd\n\nfrom rusket import prefixspan, sequences_from_event_log\n</code></pre>","path":["User Guide","Notebooks","Sequential Pattern Mining (PrefixSpan)"],"tags":[]},{"location":"notebooks/3_ecommerce_lifecycle_mining/#1-the-e-commerce-event-log","level":2,"title":"1. The E-Commerce Event Log","text":"<p>We start with a classic log of distinct user events over time. This could be page views, checkout events, or support tickets.</p> <pre><code>events = pd.DataFrame(\n    {\n        \"user_id\": [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n        \"timestamp\": [\n            \"2024-01-01 10:00\",\n            \"2024-01-05 10:05\",\n            \"2024-01-10 10:10\",\n            \"2024-01-02 11:00\",\n            \"2024-01-07 11:05\",\n            \"2024-01-03 09:00\",\n            \"2024-01-04 09:05\",\n            \"2024-01-09 09:10\",\n            \"2024-01-01 12:00\",\n            \"2024-01-08 12:00\",\n            \"2024-01-15 12:00\",\n        ],\n        \"event_name\": [\n            \"signup\",\n            \"view_product\",\n            \"add_to_cart\",\n            \"signup\",\n            \"view_product\",\n            \"signup\",\n            \"view_product\",\n            \"checkout\",\n            \"view_product\",\n            \"checkout\",\n            \"churn\",\n        ],\n    }\n)\n\n# Ensure correct temporal ordering\nevents[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"])\nevents.sort_values([\"user_id\", \"timestamp\"], inplace=True)\nevents.head()\n</code></pre> user_id timestamp event_name 0 1 2024-01-01 10:00:00 signup 1 1 2024-01-05 10:05:00 view_product 2 1 2024-01-10 10:10:00 add_to_cart 3 2 2024-01-02 11:00:00 signup 4 2 2024-01-07 11:05:00 view_product","path":["User Guide","Notebooks","Sequential Pattern Mining (PrefixSpan)"],"tags":[]},{"location":"notebooks/3_ecommerce_lifecycle_mining/#2-compiling-the-sequential-database","level":2,"title":"2. Compiling the Sequential Database","text":"<p>Rusket requires data grouped into discrete sequential arrays of integers per user. We provide a <code>sequences_from_event_log</code> helper to automatically convert your Pandas DataFrame into this required zero-copy format.</p> <pre><code>sequences, label_mapping = sequences_from_event_log(\n    events, user_col=\"user_id\", time_col=\"timestamp\", item_col=\"event_name\"\n)\n\nprint(f\"Compiled {len(sequences)} distinct user sequences.\")\nprint(f\"Internal Mapping Table: {label_mapping}\")\n</code></pre> <pre><code>```text\nCompiled 4 distinct user sequences.\nInternal Mapping Table: {0: 'signup', 1: 'view_product', 2: 'add_to_cart', 3: 'checkout', 4: 'churn'}\n```\n</code></pre>","path":["User Guide","Notebooks","Sequential Pattern Mining (PrefixSpan)"],"tags":[]},{"location":"notebooks/3_ecommerce_lifecycle_mining/#3-mining-sequential-patterns","level":2,"title":"3. Mining Sequential Patterns","text":"<p>Now we pass our compiled sequences into the <code>prefixspan</code> model. We will ask for patterns that happen to at least 2 independent users.</p> <pre><code># Mine patterns\nt0 = time.time()\npatterns_df = prefixspan(sequences, min_support=2)\nprint(f\"Found {len(patterns_df)} sequential patterns in {time.time() - t0:.4f}s!\")\n\n# Restore the human-readable labels from our internal `label_mapping`\npatterns_df[\"event_path\"] = patterns_df[\"sequence\"].apply(lambda seq: \" → \".join([label_mapping[idx] for idx in seq]))\n\n# Display the most frequent sequences\npatterns_df.sort_values(\"support\", ascending=False)[[\"support\", \"event_path\"]]\n</code></pre> <pre><code>Found 5 sequential patterns in 0.0013s!\n</code></pre> support event_path 0 4 view_product 1 3 signup 2 3 signup → view_product 3 2 view_product → checkout 4 2 checkout <p>Using these sequential outputs, businesses can automatically map out the 'Happy Path' to <code>checkout</code> vs the 'Failure Path' leading to <code>churn</code>.</p>","path":["User Guide","Notebooks","Sequential Pattern Mining (PrefixSpan)"],"tags":[]}]}