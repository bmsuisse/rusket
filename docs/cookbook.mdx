---
title: "Cookbook"
description: "A hands-on guide to every feature in rusket — from market basket analysis to billion-scale collaborative filtering."
---

A hands-on guide to every feature in `rusket` — from market basket analysis to billion-scale collaborative filtering.

---

## Setup

```python
pip install rusket
```

```python
import numpy as np
import pandas as pd
import polars as pl
from rusket import mine, eclat, association_rules, ALS, BPR
from rusket import prefixspan, sequences_from_event_log, hupm, similar_items, Recommender, score_potential
```

---

## 1. Market Basket Analysis — Grocery Retail

### Business context

A supermarket chain wants to identify which product combinations appear most frequently in customer baskets across all checkout terminals. The output drives:

- **"Frequently Bought Together"** widgets on the self-checkout screen
- **Shelf adjacency** decisions (place high-lift pairs closer together)
- **Promotional bundles** (discount pairs with high confidence but low current margin)

### Prepare the basket data

In practice this comes from your POS system as a long-format order table. For demonstration we build a plausible synthetic dataset:

```python
import numpy as np
import pandas as pd
from rusket import from_transactions

np.random.seed(42)

# Realistic grocery catalogue with purchase probabilities (power-law distributed)
categories = {
    "Milk": 0.55, "Bread": 0.52, "Butter": 0.36, "Eggs": 0.41,
    "Cheese": 0.28, "Yogurt": 0.22, "Coffee": 0.31, "Tea": 0.18,
    "Sugar": 0.20, "Apples": 0.25, "Bananas": 0.30, "Oranges": 0.15,
    "Chicken": 0.35, "Pasta": 0.27, "Tomato Sauce": 0.26, "Onions": 0.40,
}

n_receipts = 10_000
df_long = pd.DataFrame(
    [(receipt, product)
     for receipt in range(n_receipts)
     for product, prob in categories.items()
     if np.random.rand() < prob],
    columns=["receipt_id", "product"],
)
print(f"Simulated {n_receipts:,} receipts, {len(df_long):,} line items")

# Convert to one-hot basket matrix
basket = from_transactions(df_long, transaction_col="receipt_id", item_col="product")
```

### Find frequent product combinations

```python
from rusket import mine

# Keep combinations appearing in ≥5% of receipts
freq = mine(basket, min_support=0.05, use_colnames=True)
print(f"Found {len(freq):,} frequent product combinations")

top_combos = freq.sort_values("support", ascending=False)
print(top_combos.head(10))
# e.g. (Milk,) 55%, (Bread,) 52%, (Milk, Bread) 28% ...
```

### Generate cross-sell rules

```python
from rusket import association_rules

rules = association_rules(freq, num_itemsets=n_receipts, min_threshold=0.3)

# Filter for actionable rules: high confidence AND lift (better than random)
actionable = rules[(rules["confidence"] > 0.45) & (rules["lift"] > 1.2)]
print(actionable.sort_values("lift", ascending=False).head(10))
# antecedents     consequents   confidence  lift
# (Butter,)       (Bread,)      0.72        1.38   → 72% of Butter buyers also buy Bread
```

### Limit itemset length for large catalogues

```python
# For a full supermarket with 5000 SKUs: cap at pairs and triples to avoid explosion
freq_pairs = mine(basket, min_support=0.02, max_len=2, use_colnames=True)
```

---

## 2. ECLAT — When to Use vs FPGrowth

ECLAT uses a vertical bitset representation. It is **faster than FPGrowth for sparse datasets** with many unique items and a low support threshold.

```python
freq_ec = eclat(basket, min_support=0.05, use_colnames=True)
```

**Rule of thumb:**

| Condition | Recommended algorithm |
|---|---|
| Dense dataset, few items | `mine(method="auto")` |
| Sparse dataset, many items, low support | `mine(method="auto")` |
| Very large dataset (100M+ rows) | `FPMiner` with streaming |

---

## 3. Transaction Helpers

Convert long-format order data (e.g., from a database) to the one-hot boolean matrix format required by `mine` (which automatically routes to `fpgrowth` or `eclat`).

### From a Pandas DataFrame

```python
from rusket import from_transactions

orders = pd.DataFrame({
    "order_id": [1, 1, 1, 2, 2, 3],
    "item":     ["Milk", "Bread", "Eggs", "Milk", "Butter", "Eggs"],
})

# Converts long-format → wide boolean matrix
basket = from_transactions(orders, user_col="order_id", item_col="item")
fi = mine(basket, min_support=0.3, use_colnames=True)
```

### From a Polars DataFrame

```python
orders_pl = pl.DataFrame({
    "order_id": [1, 1, 1, 2, 2, 3],
    "item":     ["Milk", "Bread", "Eggs", "Milk", "Butter", "Eggs"],
})

basket = from_transactions(orders_pl, user_col="order_id", item_col="item")
```

### From a Spark DataFrame

```python
# Works with PySpark DataFrames via .toPandas() under the hood
basket = from_transactions(spark_df, user_col="order_id", item_col="item")
```

---

## 4. Collaborative Filtering with ALS — "For You" Personalisation

`ALS` (Alternating Least Squares) learns latent user and item embeddings from **implicit feedback** (purchases, clicks, plays) and enables personalised "For You" recommendations.

### Business context

An e-commerce platform wants to show a personalised homepage to each logged-in user. ALS learns from past purchase history which categories of products each user affinity group prefers — without any explicit ratings.

### Fit from purchase history (event log)

```python
from rusket import ALS

# Purchase history from your order management system
purchases = pd.DataFrame({
    "customer_id": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1003],
    "sku":         ["A10", "B22", "C15",  "A10", "D33",  "B22", "C15", "E07"],
    "revenue":     [29.99, 49.00, 9.99,  29.99, 15.00, 49.00, 9.99, 22.00],
})

# revenue used as confidence weight (alpha scaling)
model = ALS(factors=64, iterations=15, alpha=40.0, cg_iters=3, verbose=True)
model = ALS.from_transactions(
    purchases,
    transaction_col="customer_id",
    item_col="sku",
    rating_col="revenue",
)
```

### Get personalised recommendations

```python
# Top-5 SKUs for customer 1002, excluding already-purchased items
skus, scores = model.recommend_items(user_id=1002, n=5, exclude_seen=True)
print(f"Recommended SKUs for customer 1002: {skus}")

# Target: which customers should receive a promo for SKU B22 (high-margin item)?
top_customers, scores = model.recommend_users(item_id="B22", n=100)
print(f"Top customers to target with B22 promo: {top_customers[:5]}")
```

### Fit from transaction data

```python
# If you have pre-built purchase integers from your warehouse:
model2 = ALS(factors=32, iterations=10, verbose=True)
model2 = ALS.from_transactions(purchases, transaction_col="customer_id", item_col="sku")
```

### Access latent factors directly

```python
# NumPy arrays (n_users × factors) and (n_items × factors)
print(model.user_factors.shape)  # (10000, 64)
print(model.item_factors.shape)  # (5000, 64)
```

---

## 5. Out-of-Core ALS for 1B+ Ratings

When the interaction matrix exceeds available RAM, use the out-of-core streaming loader. The CSR matrix is stored on SSD and the OS pages data into RAM on demand.

### Download and prepare the MovieLens 1B dataset

```python
import urllib.request, tarfile, os

# Download (~1.4 GB)
url = "https://files.grouplens.org/datasets/movielens/ml-20mx16x32.tar"
urllib.request.urlretrieve(url, "ml-1b.tar")

with tarfile.open("ml-1b.tar") as t:
    t.extractall("data/ml-1b/")
```

### Build the out-of-core CSR matrix

```python
import numpy as np
from scipy import sparse
from pathlib import Path

data_dir = Path("data/ml-1b/ml-20mx16x32")
npz_files = sorted(data_dir.glob("trainx*.npz"))

# Pass 1 — count ratings per user
max_user, max_item, nnz = 0, 0, 0
counts = np.zeros(100_000_000, dtype=np.int64)  # pre-allocate for max users

for f in npz_files:
    arr = np.load(f)["arr_0"]          # shape (N, 2) — [user_id, item_id]
    uids, iids = arr[:, 0], arr[:, 1]
    max_user = max(max_user, int(uids.max()))
    max_item = max(max_item, int(iids.max()))
    chunk_counts = np.bincount(uids, minlength=max_user + 1)
    counts[:len(chunk_counts)] += chunk_counts
    nnz += len(uids)

n_users, n_items = max_user + 1, max_item + 1
indptr = np.zeros(n_users + 1, dtype=np.int64)
np.cumsum(counts[:n_users], out=indptr[1:])

# Pass 2 — write indices/data to SSD memory maps
mmap_indices = np.memmap("indices.mmap", dtype=np.int32, mode="w+", shape=(nnz,))
mmap_data    = np.memmap("data.mmap",    dtype=np.float32, mode="w+", shape=(nnz,))
pos = indptr[:-1].copy()

for f in npz_files:
    arr = np.load(f)["arr_0"]
    uids, iids = arr[:, 0].astype(np.int64), arr[:, 1].astype(np.int32)
    for u, i in zip(uids, iids):
        p = pos[u]
        mmap_indices[p] = i
        mmap_data[p]    = 1.0
        pos[u] += 1

mmap_indices.flush()
mmap_data.flush()
```

### Fit ALS on the out-of-core matrix

```python
# Bypass scipy's int32 limits by direct property assignment
mat = sparse.csr_matrix((n_users, n_items))
mat.indptr  = indptr
mat.indices = mmap_indices
mat.data    = mmap_data

model = ALS(
    factors=64,
    iterations=5,      # fewer iterations for 1B — each takes hours on SSD
    alpha=40.0,
    verbose=True,
    cg_iters=3,
)
model.fit(mat)
```

<Tip>
**Hardware sizing:** On a machine with ≥ 32 GB RAM the mmap working set stays hot in OS page cache and each iteration completes in ~5 minutes. On 8 GB RAM each iteration is disk-bound and takes hours.
</Tip>

---

## 6. Bayesian Personalized Ranking (BPR)

Unlike ALS which tries to reconstruct the full interaction matrix, BPR explicitly optimizes the model to rank positive observed items higher than unobserved items. This makes BPR excellent for top-N ranking tasks on implicit data (like clicks or views).

```python
from rusket import BPR
from scipy.sparse import csr_matrix
import numpy as np

# Prepare sparse interaction matrix
rows = np.random.randint(0, 1000, size=5000)
cols = np.random.randint(0, 500, size=5000)
mat = csr_matrix((np.ones(5000), (rows, cols)), shape=(1000, 500))

# Initialize and fit BPR with Hogwild! parallel SGD
model = BPR(
    factors=64,
    learning_rate=0.01,
    regularization=0.01,
    iterations=100,
    seed=42,
)
model.fit(mat)

# Recommend items just like ALS
items, scores = model.recommend_items(user_id=10, n=5)
```

---

## 7. Sequential Pattern Mining (PrefixSpan)

PrefixSpan discovers frequent sequences of events over time. Unlike standard market basket analysis (which looks at what is bought *together*), PrefixSpan finds patterns *across ordered events* — ideal for customer journey analysis, funnel optimisation, and churn prediction.

**Business scenario:** A SaaS company wants to understand which product page navigation sequences lead customers to checkout. Which paths are most common? Where do users drop off?

```python
import pandas as pd
from rusket import prefixspan, sequences_from_event_log

# 1. Website clickstream log — each row is one page visit
clickstream = pd.DataFrame({
    "session_id": [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4],
    "timestamp":  [10, 20, 30, 15, 25, 5, 15, 35, 10, 18, 40],
    "page": [
        "Home", "Pricing", "Checkout",
        "Home", "Pricing",
        "Features", "Pricing", "Checkout",
        "Home", "Features", "Checkout",
    ],
})

# 2. Convert to sequence format expected by the Rust miner
seqs, mapping = sequences_from_event_log(
    clickstream, user_col="session_id", time_col="timestamp", item_col="page"
)

# 3. Mine navigation sequences seen in ≥2 sessions (absolute count)
patterns_df = prefixspan(seqs, min_support=2, max_len=4)

# 4. Map integer IDs back to readable page names
patterns_df["path"] = patterns_df["sequence"].apply(
    lambda seq: " → ".join(mapping[s] for s in seq)
)
print(patterns_df[["support", "path"]].sort_values("support", ascending=False).head(10))
```

---

## 8. High-Utility Pattern Mining (HUPM)

Standard Frequent Itemset Mining (FP-Growth/Eclat) treats all items equally. HUPM considers the **profit** or **utility** of items, discovering itemsets that generate high total revenue even if they are bought infrequently.

```python
from rusket import HUPM
import pandas as pd

receipts = pd.DataFrame({
    "receipt_id": [1, 1, 1, 2, 2, 3, 3, 4, 4, 4],
    "product":    ["champagne", "foie_gras", "truffle_oil",
                   "champagne", "truffle_oil",
                   "foie_gras", "truffle_oil",
                   "champagne", "foie_gras", "truffle_oil"],
    "margin":     [18.50, 14.00, 8.00,   # receipt 1 margins
                   18.50, 8.00,            # receipt 2
                   14.00, 8.00,            # receipt 3
                   18.50, 14.00, 8.00],    # receipt 4
})

# Discover all product bundles generating ≥ €30 total margin
high_value = HUPM.from_transactions(
    receipts,
    transaction_col="receipt_id",
    item_col="product",
    utility_col="margin",
    min_utility=30.0,
).mine()

print(high_value.sort_values("utility", ascending=False))
```

---

## 9. Native Polars Integration

`rusket` returns itemsets as zero-copy **PyArrow `ListArray`** structures, making Polars interoperability very efficient.

```python
df_pl = pl.from_pandas(df)
fi_pl = mine(df_pl, min_support=0.05, use_colnames=True)

# LazyFrame works too:
lazy = df_pl.lazy()
fi_pl2 = mine(lazy.collect(), min_support=0.05, use_colnames=True)
```

### Query itemsets with PyArrow compute

```python
import pyarrow.compute as pc

# Find itemsets containing "Milk" as first element
contains_milk = pc.list_element(fi["itemsets"].array, 0) == "Milk"
fi[contains_milk].head()
```

---

## 10. Spark / Databricks Integration

`rusket` provides native integration with PySpark out of the box, leaning heavily on **Apache Arrow** to completely bypass Python-to-JVM serialization and Pandas memory bloat.

### 10.1 Streaming 1B+ Rows from Spark

```python
from rusket import mine_spark

spark_df = spark.table("silver_transactions")

# Streams Arrow RecordBatches directly to the Rust backend
frequent_itemsets = mine_spark(
    spark_df, 
    n_items=500_000, 
    txn_col="transaction_id", 
    item_col="product_id", 
    min_support=0.001
)
```

### 10.2 Distributed Parallel Mining (Grouped)

```python
from rusket.spark import mine_grouped

spark_df = spark.table("retail_transactions")
regional_rules_df = mine_grouped(spark_df, group_col="store_id", min_support=0.05)
```

### 10.3 Collaborative Filtering (ALS) from Spark

```python
from rusket import ALS

ratings_spark = spark.table("implicit_ratings") 
model = ALS.from_transactions(
    ratings_spark, 
    transaction_col="user_id", 
    item_col="item_id", 
    rating_col="clicks",
    factors=64,
    iterations=10,
    verbose=True
)
```

<Note>
**Out-of-Core Models**: For Spark tables spanning >100M rows, use `mine_spark` for Frequent Pattern mining, or export the table to an Out-of-Core disk map (Section 5) for ALS factorisation.
</Note>

---

## 11. Tuning Guide

### FPGrowth / ECLAT

| Parameter | Default | Effect |
|---|---|---|
| `min_support` | required | Lower → more itemsets, slower |
| `max_len` | None | Cap itemset size — huge speedup on large catalogs |
| `use_colnames` | False | Return column names instead of indices |

### ALS

| Parameter | Default | Notes |
|---|---|---|
| `factors` | 64 | Higher → better quality, more RAM, slower |
| `iterations` | 15 | 5–15 is typical |
| `alpha` | 40.0 | Higher → stronger signal |
| `regularization` | 0.01 | Increase if overfitting |
| `cg_iters` | 3 | CG solver steps |

### BPR

| Parameter | Default | Notes |
|---|---|---|
| `factors` | 64 | Higher → better quality, more RAM |
| `iterations` | 100 | Try 100-500 |
| `learning_rate` | 0.01 | SGD learning rate |
| `regularization` | 0.01 | Increase if overfitting |

---

## 12. Item Similarity and Cross-Selling Potential

```python
from rusket import similar_items

# Given an ALS model fitted on purchases
item_ids, match_scores = similar_items(model, item_id=102, n=5)
```

### Calculate Cross-Selling Potential

```python
from rusket import score_potential

user_purchase_history = [
    [0, 1, 5], # User 0 bought items 0, 1, 5
    [1, 3],    # User 1 bought items 1, 3
    [0]        # User 2 bought item 0
]
target_items = [2, 4, 6]

scores = score_potential(
    user_purchase_history, 
    als_model=model, 
    target_categories=target_items
)
```

---

## 13. Hybrid Recommender (ALS + Association Rules)

```python
from rusket import Recommender

rec = Recommender(als_model=model, rules_df=strong_rules)

# 1. "For You" (Personalized cross-selling based on user history)
item_ids, scores = rec.recommend_for_user(user_id=125, n=5)

# 2. "Frequently Bought Together" (Cart-based additions)
active_cart = [10, 15] 
suggested_additions = rec.recommend_for_cart(active_cart, n=3)
```

---

## 14. GenAI / LLM Stack Integration

### Vector Export & Vector Databases (LanceDB)

```python
import lancedb
from rusket import export_item_factors

df_vectors = export_item_factors(als_model)
db = lancedb.connect("./lancedb")
table = db.create_table("item_embeddings", data=df_vectors, mode="overwrite")

query_vector = df_vectors.iloc[0]["vector"]
results = table.search(query_vector).limit(5).to_pandas()
```

### Graph Generation for Community Detection

```python
import networkx as nx
from rusket.viz import to_networkx

G = to_networkx(rules_df, source_col="antecedents", target_col="consequents", edge_attr="lift")
centrality = nx.pagerank(G, weight='weight')
```

---

## 15. Visualizing Latent Spaces (PCA)

We apply **L2 Normalization** followed by **PCA**. Because `rusket` exposes ALS factors directly as NumPy arrays, you can do this **without adding dependencies like `scikit-learn`**:

```python
import numpy as np
import plotly.express as px

item_factors = model.item_factors
item_norms = np.linalg.norm(item_factors, axis=1, keepdims=True)
item_factors_norm = item_factors / np.clip(item_norms, a_min=1e-10, a_max=None)

def compute_pca_3d(data):
    data_centered = data - np.mean(data, axis=0)
    U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)
    return np.dot(data_centered, Vt[:3].T)

item_pca = compute_pca_3d(item_factors_norm)

fig = px.scatter_3d(
    x=item_pca[:, 0], y=item_pca[:, 1], z=item_pca[:, 2],
    title="ALS Latent Space (3D PCA Mapping)"
)
fig.update_traces(marker=dict(size=3, opacity=0.7))
fig.show()
```

---

## 16. Translating Spark MLlib to `rusket`

For users migrating from Databricks or PySpark, `rusket` offers a highly similar API without the distributed computing overhead. 

```python
import pandas as pd
import numpy as np
from rusket import ALS
from rusket.recommend import score_potential

url = "https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt"
ratings = pd.read_csv(url, sep="::", engine="python", names=["userId", "movieId", "rating", "timestamp"])

# Random Split (80/20)
shuffled = ratings.sample(frac=1.0, random_state=42)
split_idx = int(len(shuffled) * 0.8)
training = shuffled.iloc[:split_idx]
test = shuffled.iloc[split_idx:]

model = ALS.from_transactions(training, transaction_col="userId", item_col="movieId", rating_col="rating", factors=10, iterations=5, regularization=0.01, seed=42)

user_histories = training.groupby("userId")["movieId"].apply(list).to_dict()
history_list = [user_histories.get(uid, []) for uid in range(model._n_users)]

all_predictions = score_potential(history_list, model)

test_users = test["userId"].values
test_movies = test["movieId"].values
actual_ratings = test["rating"].values

try:
    internal_user_ids = np.array([model._user_labels.index(u) for u in test_users])
    internal_movie_ids = np.array([model._item_labels.index(str(m)) for m in test_movies])
    
    predicted_ratings = all_predictions[internal_user_ids, internal_movie_ids]
    
    valid_mask = ~np.isinf(predicted_ratings) & ~np.isnan(predicted_ratings)
    rmse = np.sqrt(np.mean((predicted_ratings[valid_mask] - actual_ratings[valid_mask]) ** 2))
    print(f"Root-mean-square error = {rmse:.4f}")
    
except ValueError as e:
    print("Cold start warning: Some users/items in test set were not in training.")
```
