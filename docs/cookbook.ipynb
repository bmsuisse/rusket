{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Market Basket Analysis Cookbook\n",
    "# \n",
    "# Welcome to the `rusket` cookbook! This guide provides comprehensive examples on how to perform market basket analysis and generate recommendation rules efficiently.\n",
    "# We will use Plotly for visualizations to get insights into our frequent itemsets and association rules.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "from rusket import fpgrowth, association_rules\n",
    "\n",
    "\n",
    "# ## 1. Synthetic Dataset Generation\n",
    "# \n",
    "# Let's start by generating a synthetic retail dataset. We'll simulate a supermarket where customers buy various categories of items.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "items = [\n",
    "    \"Milk\", \"Bread\", \"Butter\", \"Eggs\", \"Cheese\", \"Yogurt\", \"Coffee\", \"Tea\", \n",
    "    \"Sugar\", \"Apples\", \"Bananas\", \"Oranges\", \"Chicken\", \"Beef\", \"Fish\",\n",
    "    \"Rice\", \"Pasta\", \"Tomato Sauce\", \"Onions\", \"Garlic\"\n",
    "]\n",
    "\n",
    "n_transactions = 10_000\n",
    "n_items = len(items)\n",
    "\n",
    "# Simulate different purchase frequencies (power-law distribution)\n",
    "probabilities = np.power(np.arange(1, n_items + 1, dtype=float), -0.7)\n",
    "probabilities /= probabilities.max()\n",
    "probabilities = np.clip(probabilities * 0.3, 0.01, 0.8)\n",
    "\n",
    "data = np.random.rand(n_transactions, n_items) < probabilities\n",
    "df = pd.DataFrame(data, columns=items)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ## 2. Frequent Pattern Mining\n",
    "# \n",
    "# We'll extract frequent itemsets using the blazing-fast `fpgrowth` algorithm from `rusket`.\n",
    "# We set `min_support=0.05`, meaning an itemset must appear in at least 5% of all transactions.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract frequent itemsets\n",
    "fi = fpgrowth(df, min_support=0.05, use_colnames=True)\n",
    "\n",
    "print(f\"Found {len(fi)} frequent itemsets.\")\n",
    "fi.sort_values(by=\"support\", ascending=False).head(10)\n",
    "\n",
    "\n",
    "# ### Visualizing Frequent Itemsets\n",
    "# \n",
    "# Let's plot the top 20 most frequent itemsets to understand what items are bought together most often.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get top 20 itemsets\n",
    "top_fi = fi.sort_values(by=\"support\", ascending=False).head(20).copy()\n",
    "# Format itemsets as strings\n",
    "top_fi['itemsets_str'] = top_fi['itemsets'].apply(lambda x: \" + \".join(list(x)))\n",
    "\n",
    "fig = px.bar(\n",
    "    top_fi, \n",
    "    x=\"support\", \n",
    "    y=\"itemsets_str\", \n",
    "    orientation='h',\n",
    "    title=\"Top 20 Frequent Itemsets by Support\",\n",
    "    labels={'support': 'Support', 'itemsets_str': 'Itemset'},\n",
    "    color=\"support\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ## 3. Generating Association Rules\n",
    "# \n",
    "# Now that we have our frequent itemsets, we can generate association rules. \n",
    "# We'll use the fundamental `confidence` metric, setting a threshold to filter out weak rules.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(fi, num_itemsets=len(df), min_threshold=0.3)\n",
    "\n",
    "print(f\"Generated {len(rules)} association rules.\")\n",
    "rules.sort_values(by=\"lift\", ascending=False).head()\n",
    "\n",
    "\n",
    "# ### Filtering Rules\n",
    "# \n",
    "# Often, we'll want to filter rules based on multiple metrics. For example, rules with high confidence *and* high lift. High lift (> 1) indicates that the items are positively correlated.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter for strong rules\n",
    "strong_rules = rules[(rules['confidence'] > 0.4) & (rules['lift'] > 1.2)]\n",
    "strong_rules = strong_rules.sort_values(by=\"lift\", ascending=False)\n",
    "strong_rules.head(10)\n",
    "\n",
    "\n",
    "# ### Visualizing Association Rules\n",
    "# \n",
    "# A scatter plot of Support vs. Confidence is great for identifying the most valuable rules. We'll use color to represent `lift`.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    rules, \n",
    "    x=\"support\", \n",
    "    y=\"confidence\", \n",
    "    color=\"lift\",\n",
    "    hover_data=[\"antecedents\", \"consequents\"],\n",
    "    title=\"Association Rules: Support vs Confidence\",\n",
    "    color_continuous_scale=\"Plasma\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ## 4. Seamless Polars Integration\n",
    "# \n",
    "# `rusket` works natively with `polars` without requiring expensive conversions. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_pl = pl.from_pandas(df)\n",
    "fi_pl = fpgrowth(df_pl, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate rules directly from Polars dataframe\n",
    "rules_pl = association_rules(fi_pl, num_itemsets=df_pl.height, min_threshold=0.3)\n",
    "rules_pl.head(5)\n",
    "\n",
    "\n",
    "# ## 5. Working with PyArrow Outputs\n",
    "# \n",
    "# To achieve blazing-fast performance, `rusket` returns itemsets as zero-copy **PyArrow `ListArray`** structures backed by Pandas.\n",
    "# This eliminates Python object overhead and allows you to process millions of rules with minimal memory.\n",
    "# \n",
    "# ### Querying PyArrow Itemsets\n",
    "# \n",
    "# Because the `itemsets` column uses `pd.ArrowDtype(pa.list_(pa.string()))`, standard Python `set` equality operations won't work perfectly out of the box. \n",
    "# You should use PyArrow compute functions or cast them to Python sets when filtering row-by-row.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3852e16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Extract itemsets using PyArrow compute to find transactions containing a specific item\n",
    "# For example, let's find all itemsets that contain 'Milk'\n",
    "contains_milk = pc.list_element(fi['itemsets'].array, 0) == \"Milk\"\n",
    "\n",
    "# Alternatively, if you need to do complex Python-native filtering, you can convert to sets:\n",
    "# (Note: this materializes Python objects, so only do this on filtered sub-sets!)\n",
    "top_10 = fi.head(10).copy()\n",
    "top_10['python_sets'] = top_10['itemsets'].apply(set)\n",
    "\n",
    "print(\"Zero-Copy PyArrow Array Dtype:\", fi['itemsets'].dtype)\n",
    "top_10.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
