# rusket â€” LLM context file
# Auto-generated by scripts/gen_llm_txt.py â€” do not edit by hand.
# Source: https://github.com/bmsuisse/rusket



================================================================================
## API Reference
================================================================================

# API Reference

> This file is **auto-generated** by `scripts/gen_api_reference.py`.  Do not edit by hand â€” update the Python docstrings instead.

## Functional API

Convenience module-level functions.  For most use-cases these are the only entry points you need.

### `mine`

Mine frequent itemsets using the optimal algorithm.

This module-level function relies on the Object-Oriented APIs.
Automatically selects between FP-Growth and Eclat based on density,
or falls back to FPMiner (streaming) if memory is low.

```python
from rusket.mine import mine

mine(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, method: 'str' = 'auto', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -> 'pd.DataFrame'
```

---

### `fpgrowth`

Find frequent itemsets using the optimal algorithm (Eclat or FP-growth).

This module-level function relies on the Object-Oriented APIs.

```python
from rusket.fpgrowth import fpgrowth

fpgrowth(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, method: 'str' = 'auto', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -> 'pd.DataFrame'
```

---

### `eclat`

Find frequent itemsets using the Eclat algorithm.

This module-level function relies on the Object-Oriented APIs.

```python
from rusket.eclat import eclat

eclat(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, column_names: 'list[str] | None' = None) -> 'pd.DataFrame'
```

---

### `association_rules`

```python
from rusket.association_rules import association_rules

association_rules(df: 'pd.DataFrame | Any', num_itemsets: 'int | None' = None, df_orig: 'pd.DataFrame | None' = None, null_values: 'bool' = False, metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, support_only: 'bool' = False, return_metrics: 'list[str]' = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'representativity', 'leverage', 'conviction', 'zhangs_metric', 'jaccard', 'certainty', 'kulczynski']) -> 'pd.DataFrame'
```

---

### `prefixspan`

Mine sequential patterns using the PrefixSpan algorithm.

This function discovers frequent sequences of items across multiple users/sessions.
Currently, this assumes sequences where each event consists of a single item
(e.g., a sequence of page views or a sequence of individual products bought over time).

```python
from rusket.prefixspan import prefixspan

prefixspan(sequences: 'list[list[int]]', min_support: 'int', max_len: 'int | None' = None) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| sequences | list of list of int | A list of sequences, where each sequence is a list of integers representing items. Example: `[[1, 2, 3], [1, 3], [2, 3]]`. |
| min_support | int | The minimum absolute support (number of sequences a pattern must appear in). |
| max_len | int, optional | The maximum length of the sequential patterns to mine. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame containing 'support' and 'sequence' columns. |

---

### `hupm`

Mine high-utility itemsets.

This function discovers combinations of items that generate a high total utility
(e.g., profit) across all transactions, even if they aren't the most frequent.

```python
from rusket.hupm import hupm

hupm(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| transactions | list of list of int | A list of transactions, where each transaction is a list of item IDs. |
| utilities | list of list of float | A list of identical structure to `transactions`, but containing the numeric utility (e.g., profit) of that item in that specific transaction. |
| min_utility | float | The minimum total utility required to consider a pattern "high-utility". |
| max_len | int, optional | The maximum length of the itemsets to mine. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame containing 'utility' and 'itemset' columns. |

---

### `sequences_from_event_log`

Convert an event log DataFrame into the sequence format required by PrefixSpan.

Accepts Pandas, Polars, or PySpark DataFrames. Data is grouped by `user_col`,
ordered by `time_col`, and `item_col` values are collected into sequences.

```python
from rusket.prefixspan import sequences_from_event_log

sequences_from_event_log(df: 'Any', user_col: 'str', time_col: 'str', item_col: 'str') -> 'tuple[list[list[int]], dict[int, Any]]'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df | pd.DataFrame \| pl.DataFrame \| pyspark.sql.DataFrame | Event log containing users, timestamps, and items. |
| user_col | str | Column name identifying the sequence (e.g., user_id or session_id). |
| time_col | str | Column name for ordering events. |
| item_col | str | Column name for the items. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| tuple of (indptr, indices, item_mapping) |  | - indptr: CSR-style index pointer list. - indices: Flattened item index list. - item_mapping: A dictionary mapping the integer IDs back to the original item labels. |

---

### `mine_hupm`

Mine high-utility itemsets from a long-format DataFrame.

Converts a Pandas or Polars DataFrame into the required list-of-lists format
and runs the High-Utility Pattern Mining (HUPM) algorithm.

```python
from rusket.hupm import mine_hupm

mine_hupm(data: 'Any', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| data | pd.DataFrame or pl.DataFrame | A long-format DataFrame where each row represents an item in a transaction. |
| transaction_col | str | Column name identifying the transaction ID. |
| item_col | str | Column name identifying the item ID (must be numeric integers). |
| utility_col | str | Column name identifying the numeric utility (e.g. price, profit) of the item. |
| min_utility | float | The minimum total utility required to consider a pattern "high-utility". |
| max_len | int, optional | Maximum length of the itemsets to mine. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame containing 'utility' and 'itemset' columns. |

---

### `mine_duckdb`

Stream directly from a DuckDB query via Arrow RecordBatches.

This is extremely memory efficient, bypassing Pandas entirely.

```python
from rusket.streaming import mine_duckdb

mine_duckdb(con: 'Any', query: 'str', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, chunk_size: 'int' = 1000000) -> 'pd.DataFrame'
```

---

### `mine_spark`

Stream natively from a PySpark DataFrame on Databricks via Arrow.

Uses `toLocalIterator()` to fetch Arrow chunks incrementally directly
to the driver node, avoiding massive memory spikes.

```python
from rusket.streaming import mine_spark

mine_spark(spark_df: 'Any', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None) -> 'pd.DataFrame'
```

---

### `from_transactions`

Convert long-format transactional data to a one-hot boolean matrix.

The return type mirrors the input type:

- **Polars** ``DataFrame`` â†’ **Polars** ``DataFrame``
- **Pandas** ``DataFrame`` â†’ **Pandas** ``DataFrame``
- **Spark** ``DataFrame``  â†’ **Spark**  ``DataFrame``
- ``list[list[...]]``      â†’ **Pandas** ``DataFrame``

```python
from rusket.transactions import from_transactions

from_transactions(data: 'DataFrame | Sequence[Sequence[str | int]] | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| data |  | One of:  - **Pandas / Polars / Spark DataFrame** with (at least) two columns: one for the transaction identifier and one for the item. - **List of lists** where each inner list contains the items of a single transaction, e.g. ``[["bread", "milk"], ["bread", "eggs"]]``. |
| transaction_col |  | Name of the column that identifies transactions.  If ``None`` the first column is used.  Ignored for list-of-lists input. |
| item_col |  | Name of the column that contains item values.  If ``None`` the second column is used.  Ignored for list-of-lists input. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| DataFrame |  | A boolean DataFrame (same type as input) ready for :func:`rusket.fpgrowth` or :func:`rusket.eclat`. Column names correspond to the unique items. |

**Examples**

```python
>>> import rusket
>>> import pandas as pd
>>> df = pd.DataFrame({
...     "order_id": [1, 1, 1, 2, 2, 3],
...     "item": [3, 4, 5, 3, 5, 8],
... })
>>> ohe = rusket.from_transactions(df)
>>> freq = rusket.fpgrowth(ohe, min_support=0.5, use_colnames=True)
```

---

### `from_transactions_csr`

Convert long-format transactional data to a CSR matrix + column names.

Unlike :func:`from_transactions`, this returns a raw
``scipy.sparse.csr_matrix`` that can be passed directly to
:func:`rusket.fpgrowth` or :func:`rusket.eclat` â€” **no pandas overhead**.

For billion-row datasets, this processes data in chunks of ``chunk_size``
rows, keeping peak memory to one chunk + the running CSR.

```python
from rusket.transactions import from_transactions_csr

from_transactions_csr(data: 'DataFrame | str | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, chunk_size: 'int' = 10000000) -> 'tuple[Any, list[str]]'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| data |  | One of:  - **Pandas DataFrame** with (at least) two columns. - **Polars DataFrame** or **Spark DataFrame** (converted internally). - **File path** (str / Path) to a Parquet file â€” read in chunks. |
| transaction_col |  | Name of the transaction-id column. Defaults to the first column. |
| item_col |  | Name of the item column. Defaults to the second column. |
| chunk_size |  | Number of rows per chunk. Lower values use less memory. Default: 10 million rows. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| tuple[scipy.sparse.csr_matrix, list[str]] |  | A CSR matrix and the list of column (item) names.  Pass directly::  csr, names = from_transactions_csr(df) freq = fpgrowth(csr, min_support=0.001, use_colnames=True, column_names=names) |

**Examples**

```python
>>> import rusket
>>> csr, names = rusket.from_transactions_csr("orders.parquet")
>>> freq = rusket.fpgrowth(csr, min_support=0.001,
...                        use_colnames=True, column_names=names)
```

---

### `from_pandas`

Shorthand for ``from_transactions(df, transaction_col, item_col)``.

```python
from rusket.transactions import from_pandas

from_pandas(df: 'pd.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -> 'pd.DataFrame'
```

---

### `from_polars`

Shorthand for ``from_transactions(df, transaction_col, item_col)``.

```python
from rusket.transactions import from_polars

from_polars(df: 'pl.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -> 'pl.DataFrame'
```

---

### `from_spark`

Shorthand for ``from_transactions(df, transaction_col, item_col)``.

```python
from rusket.transactions import from_spark

from_spark(df: 'SparkDataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None) -> 'SparkDataFrame'
```

---

## OOP Mining API

All mining classes share a common `Miner.from_transactions()` / `.mine()` interface. `FPGrowth`, `Eclat`, `AutoMiner`, and `HUPM` also inherit `RuleMinerMixin` which adds `.association_rules()` and `.recommend_items()` helpers.

### `FPGrowth`

FP-Growth frequent itemset miner.

This class wraps the fast, core Rust FP-Growth implementation.

```python
from rusket.fpgrowth import FPGrowth

FPGrowth(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')
```

#### `FPGrowth.mine`

Execute the FP-growth algorithm on the stored data.

```python
from rusket.fpgrowth import FPGrowth.mine

FPGrowth.mine(**kwargs: 'Any') -> 'pd.DataFrame'
```

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pandas.DataFrame |  | DataFrame with two columns: - `support`: the support score. - `itemsets`: list of items (indices or column names). |

---

---

### `Eclat`

Eclat frequent itemset miner.

Eclat is typically faster than FP-growth on dense datasets due to
efficient vertical bitset intersection logic.

```python
from rusket.eclat import Eclat

Eclat(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')
```

#### `Eclat.mine`

Execute the Eclat algorithm on the stored data.

```python
from rusket.eclat import Eclat.mine

Eclat.mine(**kwargs: 'Any') -> 'pd.DataFrame'
```

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pandas.DataFrame |  | DataFrame with two columns: - `support`: the support score. - `itemsets`: list of items (indices or column names). |

---

---

### `AutoMiner`

Automatic frequent itemset miner.

Selects the optimal miner (FP-Growth or Eclat) based on matrix density.
Automatically falls back to streaming (FPMiner) if the dataset exceeds
available memory.

```python
from rusket.mine import AutoMiner

AutoMiner(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')
```

#### `AutoMiner.mine`

Execute the optimal algorithm on the stored data.

```python
from rusket.mine import AutoMiner.mine

AutoMiner.mine(**kwargs: 'Any') -> 'pd.DataFrame'
```

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pandas.DataFrame |  | DataFrame with two columns: - `support`: the support score. - `itemsets`: list of items (indices or column names). |

---

---

### `PrefixSpan`

Sequential Pattern Mining (PrefixSpan) model.

This class discovers frequent sequences of items across multiple users/sessions.

```python
from rusket.prefixspan import PrefixSpan

PrefixSpan(data: 'list[list[int]]', min_support: 'int', max_len: 'int | None' = None, item_mapping: 'dict[int, Any] | None' = None)
```

#### `PrefixSpan.mine`

Mine sequential patterns using PrefixSpan.

```python
from rusket.prefixspan import PrefixSpan.mine

PrefixSpan.mine(**kwargs: 'Any') -> 'pd.DataFrame'
```

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame containing 'support' and 'sequence' columns. Sequences are mapped back to original item names if `from_transactions` was used. |

---

---

### `HUPM`

High-Utility Pattern Mining (HUPM) model.

This class discovers combinations of items that generate a high total utility
(e.g., profit) across all transactions, even if they aren't the most frequent.

```python
from rusket.hupm import HUPM

HUPM(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None)
```

#### `HUPM.mine`

Mine high-utility itemsets.

```python
from rusket.hupm import HUPM.mine

HUPM.mine(**kwargs: 'Any') -> 'pd.DataFrame'
```

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame containing 'utility' and 'itemset' columns. |

---

---

### `FPMiner`

Streaming FP-Growth / Eclat accumulator for billion-row datasets.

Feeds (transaction_id, item_id) integer arrays to Rust one chunk at a
time.  Rust accumulates per-transaction item lists in a
``HashMap<i64, Vec<i32>>``.  Peak **Python** memory = one chunk.

```python
from rusket.streaming import FPMiner

FPMiner(n_items: 'int', max_ram_mb: 'int | None' = -1, hint_n_transactions: 'int | None' = None) -> 'None'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| n_items | int | Number of distinct items (column count).  All item IDs fed via :meth:`add_chunk` must be in ``[0, n_items)``. |

**Examples**

```python
Process a Parquet file 10 M rows at a time:

>>> import pandas as pd
>>> import numpy as np
>>> from rusket import FPMiner
>>> miner = FPMiner(n_items=500_000)
>>> for chunk in pd.read_parquet("orders.parquet", chunksize=10_000_000):
...     txn = chunk["txn_id"].to_numpy(dtype="int64")
...     item = chunk["item_idx"].to_numpy(dtype="int32")
...     miner.add_chunk(txn, item)
>>> freq = miner.mine(min_support=0.001, max_len=3, use_colnames=False)
```

#### `FPMiner.add_arrow_batch`

Feed a PyArrow RecordBatch directly into the miner.
Zero-copy extraction is used if types match (Int64/Int32).

```python
from rusket.streaming import FPMiner.add_arrow_batch

FPMiner.add_arrow_batch(batch: 'Any', txn_col: 'str', item_col: 'str') -> 'FPMiner'
```

---

#### `FPMiner.add_chunk`

Feed a chunk of (transaction_id, item_id) pairs.

```python
from rusket.streaming import FPMiner.add_chunk

FPMiner.add_chunk(txn_ids: 'np.ndarray', item_ids: 'np.ndarray') -> 'FPMiner'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| txn_ids | np.ndarray[int64] | 1-D array of transaction identifiers (arbitrary 64-bit integers). |
| item_ids | np.ndarray[int32] | 1-D array of item column indices (0-based). |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| self  (for chaining) |  |  |

---

#### `FPMiner.mine`

Mine frequent itemsets from all accumulated transactions.

```python
from rusket.streaming import FPMiner.mine

FPMiner.mine(min_support: 'float' = 0.5, max_len: 'int | None' = None, use_colnames: 'bool' = False, column_names: 'list[str] | None' = None, method: "typing.Literal['fpgrowth', 'eclat', 'auto']" = 'auto', verbose: 'int' = 0) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| min_support | float | Minimum support threshold in ``(0, 1]``. |
| max_len | int \| None | Maximum itemset length. |
| use_colnames | bool | If ``True``, itemsets contain column names instead of indices. |
| column_names | list[str] \| None | Column names to use when ``use_colnames=True``. |
| method | "fpgrowth" \| "eclat" \| "auto" | Mining algorithm to use.  ``"auto"`` (default) picks the best algorithm automatically based on data density after pre-filtering rare items (Borgelt 2003 heuristic: density < 15% â†’ Eclat, else FPGrowth). |
| verbose | int | Level of verbosity: >0 prints progress logs and times. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | Columns ``support`` and ``itemsets``. |

---

#### `FPMiner.reset`

Free all accumulated data.

```python
from rusket.streaming import FPMiner.reset

FPMiner.reset() -> 'None'
```

---

---

## `RuleMinerMixin` â€” Shared Miner Interface

`FPGrowth`, `Eclat`, `AutoMiner`, and `HUPM` all inherit these methods from `RuleMinerMixin`.  You do not construct `RuleMinerMixin` directly.

### `RuleMinerMixin.association_rules`

Generate association rules from the mined frequent itemsets.

```python
from rusket.model import RuleMinerMixin.association_rules

RuleMinerMixin.association_rules(metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, return_metrics: 'list[str] | None' = None) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| metric | str, default='confidence' | The metric to evaluate if a rule is of interest. |
| min_threshold | float, default=0.8 | The minimum threshold for the evaluation metric. |
| return_metrics | list[str] \| None, default=None | List of metrics to include in the resulting DataFrame. Defaults to all available metrics. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | DataFrame of strong association rules. |

---

### `RuleMinerMixin.recommend_items`

Suggest items to add to an active cart using association rules.

```python
from rusket.model import RuleMinerMixin.recommend_items

RuleMinerMixin.recommend_items(items: 'list[Any]', n: 'int' = 5) -> 'list[Any]'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| items | list[Any] | The items currently in the cart or basket. |
| n | int, default=5 | The maximum number of items to recommend. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| list[Any] |  | List of recommended items, ordered by lift and then confidence. |

> **Notes**
> Rules are computed once and cached with the key ``(metric="lift",
min_threshold=1.0)``.  Call :meth:`_invalidate_rules_cache` to force
a re-computation after re-mining.

---

### `RuleMinerMixin._invalidate_rules_cache`

Clear the cached association rules (call after re-mining).

```python
from rusket.model import RuleMinerMixin._invalidate_rules_cache

RuleMinerMixin._invalidate_rules_cache() -> 'None'
```

---

## Recommenders

### `ALS`

Implicit ALS collaborative filtering model.

```python
from rusket.als import ALS

ALS(factors: 'int' = 64, regularization: 'float' = 0.01, alpha: 'float' = 40.0, iterations: 'int' = 15, seed: 'int' = 42, verbose: 'bool' = False, cg_iters: 'int' = 10, use_cholesky: 'bool' = False, anderson_m: 'int' = 0, **kwargs: 'Any') -> 'None'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| factors | int | Number of latent factors. |
| regularization | float | L2 regularisation weight. |
| alpha | float | Confidence scaling: ``confidence = 1 + alpha * r``. |
| iterations | int | Number of ALS outer iterations. |
| seed | int | Random seed. |
| cg_iters | int | Conjugate Gradient iterations per user/item solve (ignored when ``use_cholesky=True``).  Reduce to 3 for very large datasets. |
| use_cholesky | bool | Use a direct Cholesky solve instead of iterative CG. Exact solution; faster when users have many interactions relative to ``factors``. |
| anderson_m | int | History window for **Anderson Acceleration** of the outer ALS loop (default 0 = disabled).  Recommended value: **5**.  ALS is a fixed-point iteration ``(U,V) â†’ F(U,V)``.  Anderson mixing extrapolates over the last ``m`` residuals to reach the fixed point faster, typically reducing the number of outer iterations by 30â€“50 % at identical recommendation quality::  # Baseline: 15 iterations model = ALS(iterations=15, cg_iters=3)  # Anderson-accelerated: 10 iterations, ~2.5Ã— faster, same quality model = ALS(iterations=10, cg_iters=3, anderson_m=5)  Memory overhead: ``m`` copies of the full ``(U âˆ¥ V)`` matrix (~57 MB per copy at 25M ratings, k=64). |

#### `ALS.fit`

Fit the model to the user-item interaction matrix.

```python
from rusket.als import ALS.fit

ALS.fit(interactions: 'Any') -> 'ALS'
```

**Raises**

| Exception | Condition |
| --- | --- |
| RuntimeError |  | If the model is already fitted. Create a new instance to refit. |
| TypeError |  | If the input matrix is not a recognizable sparse matrix or numpy array. |

---

#### `ALS.recommend_items`

Top-N items for a user. Set exclude_seen=False to include already-seen items.

```python
from rusket.als import ALS.recommend_items

ALS.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -> 'tuple[Any, Any]'
```

---

#### `ALS.recommend_users`

Top-N users for an item.

```python
from rusket.als import ALS.recommend_users

ALS.recommend_users(item_id: 'int', n: 'int' = 10) -> 'tuple[Any, Any]'
```

---

---

### `BPR`

Bayesian Personalized Ranking (BPR) model for implicit feedback.

BPR optimizes for ranking rather than reconstruction error (like ALS).
It works by drawing positive items the user interacted with, and negative items
they haven't, and adjusting latent factors to ensure the positive item scores higher.

```python
from rusket.bpr import BPR

BPR(factors: 'int' = 64, learning_rate: 'float' = 0.05, regularization: 'float' = 0.01, iterations: 'int' = 150, seed: 'int' = 42, verbose: 'bool' = False, **kwargs: 'Any') -> 'None'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| factors | int | Number of latent factors (default: 64). |
| learning_rate | float | SGD learning rate (default: 0.05). |
| regularization | float | L2 regularization weight (default: 0.01). |
| iterations | int | Number of passes over the entire interaction dataset (default: 150). |
| seed | int | Random seed for Hogwild! SGD sampling (default: 42). |

#### `BPR.fit`

Fit the BPR model to the user-item interaction matrix.

```python
from rusket.bpr import BPR.fit

BPR.fit(interactions: 'Any') -> 'BPR'
```

**Raises**

| Exception | Condition |
| --- | --- |
| RuntimeError |  | If the model is already fitted. |
| TypeError |  | If the input matrix is not a recognizable sparse matrix or numpy array. |

---

#### `BPR.recommend_items`

Top-N items for a user.

```python
from rusket.bpr import BPR.recommend_items

BPR.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -> 'tuple[Any, Any]'
```

---

---

### `Recommender`

Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.

```python
from rusket.recommend import Recommender

Recommender(als_model: 'ALS | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)
```

#### `Recommender.predict_next_chunk`

Batch-rank the next best products for every user in *user_history_df*.

```python
from rusket.recommend import Recommender.predict_next_chunk

Recommender.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -> 'pd.DataFrame'
```

---

#### `Recommender.recommend_for_cart`

Suggest items to add to an active cart using association rules.

```python
from rusket.recommend import Recommender.recommend_for_cart

Recommender.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -> 'list[int]'
```

---

#### `Recommender.recommend_for_user`

Top-N recommendations for a user via Hybrid ALS + Semantic.

```python
from rusket.recommend import Recommender.recommend_for_user

Recommender.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -> 'tuple[np.ndarray, np.ndarray]'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| user_id | int | The user ID to generate recommendations for. |
| n | int, default=5 | Number of items to return. |
| alpha | float, default=0.5 | Weight blending CF vs Semantic. ``alpha=1.0`` is pure CF. ``alpha=0.0`` is pure semantic. |
| target_item_for_semantic | int \| None, default=None | If provided, semantic similarity is computed against this item. If None, and alpha < 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF. |

---

---

### `NextBestAction`

Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.

```python
from rusket.recommend import NextBestAction

NextBestAction(als_model: 'ALS | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)
```

#### `NextBestAction.predict_next_chunk`

Batch-rank the next best products for every user in *user_history_df*.

```python
from rusket.recommend import NextBestAction.predict_next_chunk

NextBestAction.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -> 'pd.DataFrame'
```

---

#### `NextBestAction.recommend_for_cart`

Suggest items to add to an active cart using association rules.

```python
from rusket.recommend import NextBestAction.recommend_for_cart

NextBestAction.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -> 'list[int]'
```

---

#### `NextBestAction.recommend_for_user`

Top-N recommendations for a user via Hybrid ALS + Semantic.

```python
from rusket.recommend import NextBestAction.recommend_for_user

NextBestAction.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -> 'tuple[np.ndarray, np.ndarray]'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| user_id | int | The user ID to generate recommendations for. |
| n | int, default=5 | Number of items to return. |
| alpha | float, default=0.5 | Weight blending CF vs Semantic. ``alpha=1.0`` is pure CF. ``alpha=0.0`` is pure semantic. |
| target_item_for_semantic | int \| None, default=None | If provided, semantic similarity is computed against this item. If None, and alpha < 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF. |

---

---

## Analytics & Utilities

### `score_potential`

Cross-selling potential scores â€” shape ``(n_users, n_items)`` or ``(n_users, len(target_categories))``.

Items the user has already interacted with are masked to ``-inf``.

```python
from rusket.recommend import score_potential

score_potential(user_history: 'list[list[int]]', als_model: 'ALS', target_categories: 'list[int] | None' = None) -> 'np.ndarray'
```

---

### `similar_items`

Find the most similar items to a given item ID based on latent factors.

Computes cosine similarity between the specified item's latent vector
and all other item vectors in the ``item_factors`` matrix.

```python
from rusket.similarity import similar_items

similar_items(model: rusket.typing.SupportsItemFactors, item_id: int, n: int = 5) -> tuple[numpy.ndarray, numpy.ndarray]
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| model | SupportsItemFactors | A fitted model instance with an ``item_factors`` property. |
| item_id | int | The internal integer index of the target item. |
| n | int | Number of most similar items to return. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| tuple[np.ndarray, np.ndarray] |  | ``(item_ids, cosine_similarities)`` sorted in descending order. |

---

### `find_substitutes`

Substitute/cannibalizing products via negative association rules.

Items with high individual support but low co-occurrence (lift < 1.0)
likely cannibalize each other.

```python
from rusket.analytics import find_substitutes

find_substitutes(rules_df: 'pd.DataFrame', max_lift: 'float' = 0.8) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| rules_df |  | DataFrame output from ``rusket.association_rules``. |
| max_lift |  | Upper bound for lift; lift < 1.0 implies negative correlation. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame sorted ascending by lift (most severe cannibalization first). |  |  |

---

### `customer_saturation`

Customer saturation by unique items/categories bought, split into deciles.

```python
from rusket.analytics import customer_saturation

customer_saturation(df: 'pd.DataFrame', user_col: 'str', category_col: 'str | None' = None, item_col: 'str | None' = None) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | Interaction DataFrame. |
| user_col |  | Column identifying the user. |
| category_col |  | Category column (optional; at least one of category/item required). |
| item_col |  | Item column (optional). |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame with ``unique_count``, ``saturation_pct``, and ``decile`` columns. |  |  |

---

### `export_item_factors`

Exports latent item factors as a Pandas DataFrame for Vector DBs.

This format is ideal for ingesting into FAISS, Pinecone, or Qdrant for
Retrieval-Augmented Generation (RAG) and semantic search.

```python
from rusket.export import export_item_factors

export_item_factors(model: 'SupportsItemFactors', include_labels: 'bool' = True) -> 'pd.DataFrame'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| model | SupportsItemFactors | A fitted model instance with an ``item_factors`` property. |
| include_labels | bool, default=True | Whether to include the string item labels (if available from the model's fitting method). |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pd.DataFrame |  | A DataFrame where each row is an item with columns ``item_id``, optionally ``item_label``, and ``vector`` (a dense 1-D numpy array of the item's latent factors). |

**Examples**

```python
>>> model = rusket.ALS(factors=32).fit(interactions)
>>> df = rusket.export_item_factors(model)
>>> # Ingest into FAISS / Pinecone / Qdrant
>>> vectors = np.stack(df["vector"].values)
```

---

## Visualization (`rusket.viz`)

Graph and visualization utilities.  Requires `networkx` (`pip install networkx`).

### `rusket.viz.to_networkx`

Convert a Rusket association rules DataFrame into a NetworkX Directed Graph.

Nodes represent individual items. Directed edges represent rules
(antecedent â†’ consequent). Edge weights are set by the ``edge_attr``
parameter (typically lift or confidence).

This is extremely useful for running community detection algorithms
(e.g., Louvain, Girvan-Newman) to automatically discover **product clusters**,
or for visualising cross-selling patterns as a force-directed graph.

```python
from rusket.viz import rusket.viz.to_networkx

rusket.viz.to_networkx(rules_df: 'pd.DataFrame', source_col: 'str' = 'antecedents', target_col: 'str' = 'consequents', edge_attr: 'str' = 'lift') -> 'networkx.DiGraph'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| rules_df | pd.DataFrame | A Pandas DataFrame generated by ``rusket.association_rules()``. |
| source_col | str, default='antecedents' | Column name containing antecedents (graph edge sources). |
| target_col | str, default='consequents' | Column name containing consequents (graph edge targets). |
| edge_attr | str, default='lift' | The metric to use as edge weight/thickness. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| networkx.DiGraph |  | A directed graph of the association rules. If ``rules_df`` is empty, returns an empty ``DiGraph``. |

> **Notes**
> Requires the ``networkx`` package (``pip install networkx``).
When multiple rules produce the same directed edge, only the highest-weight
rule is retained.

**Examples**

```python
>>> import rusket
>>> G = rusket.viz.to_networkx(rules_df, edge_attr="lift")
>>> # Community detection with networkx
>>> import networkx.algorithms.community as nx_comm
>>> communities = nx_comm.greedy_modularity_communities(G.to_undirected())
```

---

## Distributed Spark API (`rusket.spark`)

All functions in `rusket.spark` distribute computation across PySpark partitions using Apache Arrow (zero-copy) for maximum throughput.

### `rusket.spark.mine_grouped`

Distribute Market Basket Analysis across PySpark partitions.

This function groups a PySpark DataFrame by `group_col` and applies
`rusket.mine` to each group concurrently across the cluster.

It assumes the input PySpark DataFrame is formatted like a dense
boolean matrix (One-Hot Encoded) per group, where rows are transactions.

```python
from rusket.spark import rusket.spark.mine_grouped

rusket.spark.mine_grouped(df: 'Any', group_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, method: 'str' = 'auto', use_colnames: 'bool' = True) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | The input `pyspark.sql.DataFrame`. |
| group_col |  | The column to group by (e.g. `store_id`). |
| min_support |  | Minimum support threshold. |
| max_len |  | Maximum itemset length. |
| method |  | Algorithm to use: 'auto', 'fpgrowth', or 'eclat'. |
| use_colnames |  | If True, returns item names instead of column indices. Must be True for PySpark `applyInArrow` schema consistency. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | A PySpark DataFrame containing: - `group_col` - `support` (float) - `itemsets` (array of strings) |

---

### `rusket.spark.rules_grouped`

Distribute Association Rule Mining across PySpark partitions.

This takes the frequent itemsets DataFrame (output of `mine_grouped`)
and applies `association_rules` uniformly across the groups.

```python
from rusket.spark import rusket.spark.rules_grouped

rusket.spark.rules_grouped(df: 'Any', group_col: 'str', num_itemsets: 'dict[Any, int] | int', metric: 'str' = 'confidence', min_threshold: 'float' = 0.8) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | The PySpark `DataFrame` containing frequent itemsets. |
| group_col |  | The column to group by. |
| num_itemsets |  | A dictionary mapping group IDs to their total transaction count, or a single integer if all groups have the same number of transactions. |
| metric |  | The metric to filter by (e.g. "confidence", "lift"). |
| min_threshold |  | The minimal threshold for the evaluation metric. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | A DataFrame containing antecedents, consequents, and all rule metrics, prepended with the `group_col`. |

---

### `rusket.spark.prefixspan_grouped`

Distribute Sequential Pattern Mining (PrefixSpan) across PySpark partitions.

This function groups a PySpark DataFrame by `group_col` and applies
`PrefixSpan.from_transactions` to each group concurrently across the cluster.

```python
from rusket.spark import rusket.spark.prefixspan_grouped

rusket.spark.prefixspan_grouped(df: 'Any', group_col: 'str', user_col: 'str', time_col: 'str', item_col: 'str', min_support: 'int' = 1, max_len: 'int | None' = None) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | The input `pyspark.sql.DataFrame`. |
| group_col |  | The column to group by (e.g. `store_id`). |
| user_col |  | The column identifying the sequence within each group (e.g., `user_id` or `session_id`). |
| time_col |  | The column used for ordering events within a sequence. |
| item_col |  | The column containing the items. |
| min_support |  | The minimum absolute support (number of sequences a pattern must appear in). |
| max_len |  | Maximum length of the sequential patterns to mine. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | A PySpark DataFrame containing: - `group_col` - `support` (long/int64) - `sequence` (array of strings) |

---

### `rusket.spark.hupm_grouped`

Distribute High-Utility Pattern Mining (HUPM) across PySpark partitions.

This function groups a PySpark DataFrame by `group_col` and applies
`HUPM.from_transactions` to each group concurrently across the cluster.

```python
from rusket.spark import rusket.spark.hupm_grouped

rusket.spark.hupm_grouped(df: 'Any', group_col: 'str', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | The input `pyspark.sql.DataFrame`. |
| group_col |  | The column to group by (e.g. `store_id`). |
| transaction_col |  | The column identifying the transaction within each group. |
| item_col |  | The column containing the numeric item IDs. |
| utility_col |  | The column containing the numeric utility (e.g., profit) of the item in the transaction. |
| min_utility |  | The minimum total utility required to consider a pattern "high-utility". |
| max_len |  | Maximum length of the itemsets to mine. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | A PySpark DataFrame containing: - `group_col` - `utility` (double/float64) - `itemset` (array of longs/int64) |

---

### `rusket.spark.recommend_batches`

Distribute Batch Recommendations across PySpark partitions.

This function uses `mapInArrow` to process partitions of users concurrently,
applying a pre-fitted `Recommender` (or `ALS`) to each chunk.

```python
from rusket.spark import rusket.spark.recommend_batches

rusket.spark.recommend_batches(df: 'Any', model: 'Any', user_col: 'str' = 'user_id', k: 'int' = 5) -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| df |  | The PySpark `DataFrame` containing user histories (must contain `user_col`). |
| model |  | The pre-trained `Recommender` or `ALS` model instance to use for scoring. |
| user_col |  | The column identifying the user. |
| k |  | The number of top recommendations to return per user. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | A DataFrame with two columns: - `user_col` - `recommended_items` (array of longs/int64) |

---

### `rusket.spark.to_spark`

Convert a Pandas or Polars DataFrame into a PySpark DataFrame.

```python
from rusket.spark import rusket.spark.to_spark

rusket.spark.to_spark(spark_session: 'Any', df: 'Any') -> 'Any'
```

**Parameters**

| Parameter | Type | Description |
| --- | --- | --- |
| spark_session |  | The active PySpark `SparkSession`. |
| df |  | The `pd.DataFrame` or `pl.DataFrame` to convert. |

**Returns**

| Name | Type | Description |
| --- | --- | --- |
| pyspark.sql.DataFrame |  | The resulting PySpark DataFrame. |

---



================================================================================
## Changelog
================================================================================

# Changelog

All notable changes are documented here.
This project follows [Semantic Versioning](https://semver.org/).

---

### ğŸ“¦ Miscellaneous

- auto-format code with Ruff [skip ci]
- resolve u.vlock conflict

### ğŸ”„ CI/CD

- fix detached head for auto-commit and bump to v0.1.37

### ğŸ”„ CI/CD

- add git-auto-commit for ruff format and bump to v0.1.36

### ğŸ“¦ Miscellaneous

- fix ruff format issues from type ignores and bump to v0.1.35

### ğŸ“¦ Miscellaneous

- bypass pandas-stubs typing issues for older Python versions and bump to v0.1.34

### ğŸ“¦ Miscellaneous

- fix Ruff trailing whitespace formatting error in test_fpbase.py, bump to v0.1.33

### ğŸ“¦ Miscellaneous

- fix PrefixSpan KeyError in test_spark_prefixspan and bump to v0.1.32

### ğŸ“¦ Miscellaneous

- fix pandas FutureWarning in tests and bump to v0.1.31

### ğŸ“¦ Miscellaneous

- fix PySpark assertion and Pytest deprecation warnings, bump v0.1.30

### Benchmarks

- add comprehensive benchmark scripts and final report against Python libraries
- fix missing imports and numpy compatibility, fix ruff lints

### Mining

- optimize prefixspan removing hashmaps and pyo3 object lists for 1.15x speedup
- optimize prefixspan with zero-copy numpy ffi over pyo3 for 2.05x speedup

### ğŸ“¦ Miscellaneous

- untrack benchmarking, profile, and recbole test artifacts
- fix pytest warnings/pyright errors and bump version to v0.1.29

### ğŸ“¦ Miscellaneous

- bump to v0.1.28, fix typing issues in tests

### Style

- run ruff format and fix lints
- Auto-format with Ruff

### ğŸ› Bug Fixes

- ensure Spark input is handled before Polars coercion in from_transactions

### ğŸ“– Documentation

- update logo asset path to `logo_single.svg` in documentation and configuration.

### ğŸ“¦ Miscellaneous

- update uv.lock for 0.1.26
- exclude non-essential files from sdist
- exclude dev/docs from sdist and wheels

### ğŸš€ Features

- Add strict UI typings (SupportsItemFactors), classes API filtering, and generated Schema
- add natively rust-backed evaluation metrics and model selection splitters

### ğŸ“– Documentation

- sync changelog and api reference for 0.1.26

### ğŸ“¦ Miscellaneous

- bump version to 0.1.26

### ğŸš€ Features

- `from_transactions` now preserves input DataFrame type for Pandas, Polars, and Spark with updated type hints and tests.

### Benchmark

- add script comparing eALS vs iALS

### Debug

- re-raise exception in als_grouped worker to reveal root cause

### Merge

- feature/fin-lcm-miner into main (FIN/LCM algorithms, FM/FPMC)

### âš¡ Performance

- SIMD unrolling for dot and axpy hot-loops in ALS solver

### ğŸ› Bug Fixes

- auto-coerce 0/1 pandas DataFrames to bool in dispatch, silence non-bool DeprecationWarning
- add criterion dev-dependency for bench targets
- validate DataFrame before coercing to bool so invalid values (e.g. 2) raise ValueError
- add fitted property to ItemKNN
- suppress DeprecationWarning in als_grouped Spark worker
- use internal model indices in als_grouped worker to correctly map user_labels
- resolve all pyright errors and ruff format/lint failures for CI
- resolve all ruff format/lint and pyright CI failures

### ğŸ“– Documentation

- fix MDX parsing errors for Mintlify
- add business-oriented LightGCN and SASRec example notebooks
- migrate to Zensical for GitHub Pages deployment

### ğŸ“¦ Miscellaneous

- Remove Python profiling, benchmarking, and RecBole testing scripts, and update Cargo.toml and .gitignore.
- untrack generated artifacts (tensorboard logs, dSYM, recbole_data, saved)
- untrack ai slop benches/ directory
- bump version to 0.1.25

### ğŸš€ Features

- implement FIN and LCM algorithms with fast bitset operations
- wip RecBole benchmarking and FM/FPMC algorithms
- add LightGCN and SASRec recommendation models

### Bench

- fix unfair benchmark timing and optimize EASE with Cholesky

### Benchmark

- add script comparing dEclat vs ECLAT
- add script comparing eALS vs iALS

### Style

- run ruff format

### âš¡ Performance

- SIMD unrolling for dot and axpy hot-loops in ALS solver

### ğŸ“– Documentation

- expose llm.txt in docs root and fix test_real_world.py sampling
- migrate to Mintlify
- auto-update API reference, changelog, and llm.txt
- fix MDX parsing errors for Mintlify
- auto-update API reference, changelog, and llm.txt
- add als 25m benchmark sweep chart
- update changelog for YOLO release

### ğŸ“¦ Miscellaneous

- include Mintlify config and generated MDX docs

### ğŸš€ Features

- Add ultra-fast Sparse ItemKNN algorithm using BM25 and Rust Rayon
- implement FIN and LCM algorithms with fast bitset operations
- wip RecBole benchmarking and FM/FPMC algorithms
- Add grouped PySpark support for ALS

### Style

- apply ruff formatting and fixes
- Update logo colors from purple to orange.
- refine logos with orange theme, update mkdocs palette and extra.css

### ğŸ› Bug Fixes

- resolve PySpark ChunkedArray fallback warning and implement BPR fit_transactions
- fix pyright errors reported on ci

### ğŸ“– Documentation

- add Polars/PySpark PrefixSpan tests and cookbook examples
- improve API documentation, update marketing copy, and setup PySpark skips
- enhance PrefixSpan and HUPM cookbook sections with clearer descriptions, business scenarios, and updated Python code examples.

### ğŸ“¦ Miscellaneous

- commit remaining unstaged files from previous sessions
- bump version to 0.1.21
- bump version to 0.1.22
- bump version to 0.1.23

### ğŸ”§ Refactoring

- simplify BaseModel and remove implicit recommender duplication
- update logo SVG basket elements to use curved paths and refined wire details.

### ğŸš€ Features

- core algorithms via Faer, HUPM, Arrow Streams, and Hybrid Recommender
- complete PySpark and Polars integration for PrefixSpan via native PyArrow sequences
- implement recommend_items for association rule models
- Introduce new documentation notebooks, update PySpark integration documentation, and add a notebook conversion workflow.
- automated doc sync scripts (changelog, API ref, llm.txt)
- enhance recommender system documentation and examples, update core logic, and refresh logos.
- merge feature/fpgrowth-mlxtend-api

### âš¡ Performance

- Boost FPGrowth performance with a new architecture, update benchmarks and documentation, add new logos, and remove temporary test files."

### ğŸ› Bug Fixes

- skip mlxtend comparison at >1M rows to prevent CI timeout

### ğŸ“– Documentation

- add genai and lancedb integration examples to cookbook
- add cookbook examples for ALS PCA visualization and Spark MLlib translation
- conquer 1 billion row challenge architecture and bump v0.1.20

### ğŸ”„ CI/CD

- trigger Deploy Docs on benchmarks/** changes too

### ğŸ”§ Refactoring

- clean Python layer â€” remove stale timing vars, dead code, AI-slop comments

### ğŸ› Bug Fixes

- Loosen numerical tolerance for parallel Hogwild! BPR test to fix CI

### ğŸ“– Documentation

- use relative path for logo in README

### ğŸ“– Documentation

- Comprehensive Interactive Cookbook with Real-World Datasets

### Bench

- add Cholesky to ALS benchmark script and fix pyright

### ğŸ“– Documentation

- feature rusket.mine as the primary public api endpoint across mkdocs and readme
- append comprehensive cookbook examples for prefixspan, hupm, bpr, similarity, and recommender modules

### ğŸ“¦ Miscellaneous

- safe checkpoint

### ğŸš€ Features

- add method='auto' routing to dynamically select eclat or fpgrowth based on dataset density

### ğŸš€ Features

- YOLO release v0.1.16

### âš¡ Performance

- implement rayon multi-threading for FPMiner chunk ingestion
- revert SmallVec regression, clean HashMap FPMiner + scale to 1B benchmark
- item pre-filter + with_capacity hint in FPMiner
- fix freq-sort to ascending (Eclat-optimal: least-frequent items first)

### ğŸ› Bug Fixes

- pyright unbound variables correctly initialized
- pyright complaints about unbound variables and missing als_fit_implicit argument
- benchmark now uses 8GB in-memory limit instead of disk-spilling at scale
- streaming.py cleanup + als_fit_implicit cg_iters stub + psutil available RAM strategy
- batched mining at 250M rows per batch to avoid OOM at 800M+
- SCALE_TARGETS scoping + launch 1B Eclat scale-up
- restore SEP in benchmark f-strings

### ğŸ“– Documentation

- add FPMiner out-of-core streaming section and 300M benchmark
- add ALS feature and market basket analysis to README

### ğŸš€ Features

- add verbose mode to fpgrowth, eclat, and FPMiner for large-scale feedback
- implement hybrid memory/disk out-of-core FPMiner with dynamic RAM limit
- add verbose iteration timing + out-of-core 1B support
- comprehensive cookbook + ALS speed improvements
- HashMap FPMiner + creative benchmark (method Ã— chunk-size Ã— scale)
- frequency-sorted remap + mine_auto + hint_n_transactions (Borgelt 2003)
- Anderson Acceleration for ALS outer loop (anderson_m param)

### ğŸš€ Features

- FPMiner streaming accumulator v0.1.14

### ğŸš€ Features

- direct scipy CSR support in fpgrowth/eclat + pd.factorize + scale benchmarks

### ğŸš€ Features

- automated scale benchmark with Plotly chart (1M-500M rows)

### ğŸš€ Features

- sparse CSR from_transactions + million-scale benchmarks (66Ã— faster)

### Bench

- add real-world dataset benchmark (auto-downloads, with timeouts)

### ğŸ“– Documentation

- add Eclat API, real-world benchmarks, and usage examples

### ğŸš€ Features

- add from_transactions, from_pandas, from_polars, from_spark helpers

### Test

- add dedicated test_eclat.py for standalone eclat() function

### âš¡ Performance

- arena-based FPNode with flat children storage (7.8x speedup)

### ğŸ› Bug Fixes

- add readme and license to pyproject.toml for PyPI, bump to 0.1.9

### ğŸš€ Features

- add Eclat algorithm (method='eclat') with 2.4-2.8x speedup on sparse data
- make eclat the default method (faster in all benchmarks)
- expose eclat() as standalone public function

### ğŸ› Bug Fixes

- remove orphaned FPGrowth import after FP-TDA removal

### ğŸ“¦ Miscellaneous

- remove FP-TDA implementation
- add MIT license
- add dependabot.yml to match httprx structure

### ğŸš€ Features

- implement zero-copy slice algorithm for FP-TDA

### ğŸ“¦ Miscellaneous

- remove tracked __pycache__ / .pyc files

### ğŸ› Bug Fixes

- remove target-cpu=native from .cargo/config.toml to fix CI SIGILL crashes
- exclude test_benchmark.py from regular pytest run to prevent mlxtend timeouts
- increase CI timeout to 45min for slow free-threaded Python builds
- benchmark CI - conditional baseline compare + PyPI trusted publishing (OIDC)
- fptda iterative mining to avoid stack overflow on sparse data

### ğŸ“– Documentation

- compact logo, remove fast pattern mining subtitle

### ğŸ“¦ Miscellaneous

- merge feat/regression-benchmarks into main
- bump version to 0.1.5

### ğŸ”§ Refactoring

- extract FPBase, add FPTda class, FP-TDA in benchmarks

### ğŸš€ Features

- regression benchmark tests + fix warnings
- add FP-TDA algorithm (IJISRT25NOV1256)\n\nImplements the Frequent-Pattern Two-Dimensional Array algorithm as a\ndrop-in alternative to FP-Growth. Uses right-to-left column projection\non sorted transaction lists instead of conditional subtree construction.\n\n- src/fptda.rs: Rust core (fptda_from_dense / fptda_from_csr)\n- rusket/fptda.py: Python wrapper, identical API to fpgrowth()\n- rusket/__init__.py: export rusket.fptda\n- tests/test_fptda.py: 22 tests (mix-ins + cross-check vs fpgrowth)\n- src/fpgrowth.rs: made process_item_counts/flatten_results pub(crate)\n- src/lib.rs: register new pyfunction bindings

### Style

- apply ruff format and fix lint errors

### ğŸ› Bug Fixes

- remove tracked site/ dir, rename fpgrowth-pyo3â†’rusket, fix docs workflow

### ğŸ“– Documentation

- add CI/CD workflow guidance to AGENTS.md
- publish real benchmark numbers with Plotly interactive chart
- add GitHub Pages enable step to AGENTS.md
- replace cookbook notebook with clean markdown, simplify docs workflow
- add YOLO section to AGENTS.md; merge feat/regression-benchmarks

### ğŸš€ Features

- add benchmark against efficient-apriori
- Bump version to 0.1.3, refine FPGrowth Arrow data type handling, update dependencies, and refactor test and project files.

### ğŸ› Bug Fixes

- add mkdocs-jupyter dependency for github pages

### ğŸ“¦ Miscellaneous

- fix docs deployment and format readme

### âš¡ Performance

- zero-copy pyarrow backend implementation

### ğŸ› Bug Fixes

- resolve SIGABRT panic in fpgrowth.rs and restore missing validation checks in python port

### ğŸ“– Documentation

- add comprehensive Jupyter cookbook with Plotly graphs and benchmark results
- add pyarrow zero-copy dataframe slicing examples

### ğŸ“¦ Miscellaneous

- add pytest-timeout to dev dependencies
- bump version to 0.1.1

### ğŸ“– Documentation

- emphasize ultimate blazing speed in README

### ğŸ“¦ Miscellaneous

- add maturin and pyright to dev dependencies for CI

### ğŸ”„ CI/CD

- configure automated pypi release and github tags workflow

### ğŸš€ Features

- optimised FP-Growth (mimalloc + SmallVec + PAR_ITEMS_CUTOFF=4 + parallel freq count + dedup)



================================================================================
## Architecture
================================================================================

# Architecture

How rusket relies on Rust, PyO3, and Rayon for zero-copy, zero-allocation Python bindings.

rusket is structured as a thin Python layer over a Rust core, compiled as a native extension module via [PyO3](https://pyo3.rs) and [maturin](https://github.com/PyO3/maturin).

## Repository layout

```
rusket/
â”œâ”€â”€ src/                          # Rust (PyO3)
â”‚   â”œâ”€â”€ lib.rs                    # Module root â€” exports to Python
â”‚   â”œâ”€â”€ fpgrowth.rs               # FP-Tree + FP-Growth algorithm
â”‚   â”œâ”€â”€ association_rules.rs      # Rule generation + 12 metrics
â”‚   â””â”€â”€ common.rs                 # Shared helpers
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ rusket/                  # Primary Python package (pyproject.toml name)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fpgrowth.py           # Dispatch + numpy conversion
â”‚   â”‚   â”œâ”€â”€ association_rules.py  # Label mapping + Rust call
â”‚   â”‚   â””â”€â”€ _validation.py        # Input validation helpers
â”‚   â””â”€â”€ fpgrowth_pyo3/            # Legacy compat package
â”‚       â””â”€â”€ ...
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_fpbase.py            # Shared base test classes
    â”œâ”€â”€ test_fpgrowth.py          # FP-Growth tests
    â”œâ”€â”€ test_association_rules.py # Association rules tests
    â””â”€â”€ test_benchmark.py         # Performance benchmarks
```

---

## Data flow

```mermaid
graph TD
    classDef python fill:#4B8BBE,stroke:#306998,stroke-width:2px,color:white;
    classDef rust fill:#DEA584,stroke:#000,stroke-width:2px,color:black;
    classDef data fill:#FFD43B,stroke:#306998,stroke-width:2px,color:black;

    A["Python Caller<br/>rusket.mine(df)"]:::python --> B{"Input Data Type"}:::python

    B -->|Dense Pandas| C["C-Contiguous Array<br/>(uint8)"]:::data
    B -->|Sparse Pandas| D["CSR Matrix<br/>(indptr, indices)"]:::data
    B -->|Polars| E["Numpy View<br/>(Zero-copy)"]:::data

    C --> F["Rust FFI<br/>fpgrowth_from_dense"]:::rust
    D --> G["Rust FFI<br/>fpgrowth_from_csr"]:::rust
    E --> F

    F --> H["Tree Construction<br/>(Single Pass)"]:::rust
    G --> H

    H --> I["Recursive Mining<br/>(Rayon Parallel)"]:::rust
    I --> J["Raw Vectors<br/>Vec<(count, Vec<usize>)>"]:::data

    J --> K["Python Transformation<br/>Build pd.DataFrame"]:::python
```

---

## FP-Growth algorithm

The Rust implementation follows the classic Han et al. (2000) FP-Growth algorithm:

1. **Header table scan** â€” count item frequencies; prune items below `min_count`.
2. **FP-Tree construction** â€” single-pass over transactions; compress into a prefix-tree structure.
3. **Recursive mining** â€” for each frequent item, extract the conditional pattern base, build a conditional FP-Tree, and mine it recursively.
4. **Output** â€” each leaf path materialises as one frequent itemset `(count, items)`.

### Dispatch paths

| Path | Rust function | Input shape | Notes |
|---|---|---|---|
| Dense pandas | `fpgrowth_from_dense` | `[n_rows Ã— n_cols]` uint8 | Contiguous C array |
| Sparse pandas | `fpgrowth_from_csr` | CSR `indptr + indices` | Zero-copy scipy CSR |
| Polars | `fpgrowth_from_dense` | same as dense | Arrow â†’ NumPy view |

---

## Association rules

Rule generation is vectorised in Rust:

1. For each frequent itemset of length â‰¥ 2, enumerate all non-empty antecedent / consequent splits.
2. Look up antecedent and consequent supports from a pre-built hash map.
3. Compute all 12 metrics in a single pass; filter by `(metric, min_threshold)`.
4. Return raw integer index lists to Python; Python maps back to column names / frozensets.

---

## Building from source

```bash
# Prerequisites: Rust 1.83+, Python 3.10+, uv
rustup update
uv sync

# Debug build (fast compile, slower runtime)
uv run maturin develop

# Release build (optimised)
uv run maturin develop --release

# Type checking
uv run basedpyright

# Tests
uv run pytest tests/ -x -q

# Cargo lint
cargo check
cargo clippy
```


================================================================================
## Quickstart
================================================================================

# Quick Start

Install rusket and run your first Market Basket Analysis in minutes.

## Installation

=== "pip"
    ```bash
    pip install rusket
    ```

=== "uv"
    ```bash
    uv add rusket
    ```

=== "conda"
    ```bash
    pip install rusket  # rusket is not on conda-forge yet
    ```

To also enable **Polars** support:

=== "pip"
    ```bash
    pip install "rusket[polars]"
    ```

=== "uv"
    ```bash
    uv add "rusket[polars]"
    ```

!!! tip "Coming from mlxtend?"
    rusket is a **drop-in replacement**. In most cases you only need to change your import:
    ```python
    # Before
    from mlxtend.frequent_patterns import fpgrowth, association_rules
    # After
    from rusket import mine, association_rules
    ```
    See the full [Migration Guide](migration.md) for details.

---

## Business Scenario â€” Supermarket Cross-Selling

## Step 1 â€” Prepare your data

`mine` expects a **one-hot encoded** DataFrame where rows are transactions and columns are products.

```python
import pandas as pd
from rusket import from_transactions

orders = pd.DataFrame({
    "receipt_id": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1004],
    "product":    ["milk", "bread", "butter",
                   "milk", "eggs",
                   "bread", "butter",
                   "milk", "bread", "eggs", "coffee"],
})

basket = from_transactions(orders, transaction_col="receipt_id", item_col="product")
```

---

## Step 2 â€” Mine frequent product combinations

```python
from rusket import mine

freq = mine(basket, min_support=0.4, use_colnames=True)
print(freq.sort_values("support", ascending=False))
```

!!! tip
    `mine(method="auto")` picks `eclat` for sparse data (density < 0.15) and `fpgrowth` for dense data.

---

## Step 3 â€” Generate "Frequently Bought Together" rules

```python
from rusket import association_rules

rules = association_rules(
    freq,
    num_itemsets=len(basket),
    metric="confidence",
    min_threshold=0.6,
)
print(rules[["antecedents", "consequents", "support", "confidence", "lift"]])
```

!!! note
    Pass the **total transaction count** (`len(basket)`) as `num_itemsets` so that support-based metrics are computed correctly.

---

## OOP API â€” Fluent Pipeline

```python
from rusket import AutoMiner

model = AutoMiner.from_transactions(orders, transaction_col="receipt_id", item_col="product", min_support=0.4)
freq  = model.mine(use_colnames=True)
rules = model.association_rules(metric="lift", min_threshold=1.0)

basket_contents = ["milk", "bread"]
suggestions = model.recommend_items(basket_contents, n=3)
```

---

## Billion-Scale Streaming

```python
from rusket import FPMiner

miner = FPMiner(n_items=500_000)

for chunk in pd.read_parquet("sales_fact.parquet", chunksize=10_000_000):
    txn  = chunk["receipt_id"].to_numpy(dtype="int64")
    item = chunk["product_idx"].to_numpy(dtype="int32")
    miner.add_chunk(txn, item)

freq  = miner.mine(min_support=0.001, max_len=3)
rules = association_rules(freq, num_itemsets=miner.n_transactions)
```

!!! tip
    Peak Python memory = one chunk. Rust holds the per-transaction item lists. The final mining step passes CSR arrays directly â€” zero copies.

### Direct CSR path

```python
from scipy import sparse as sp
from rusket import mine

csr = sp.csr_matrix(
    (np.ones(len(receipt_ids), dtype=np.int8), (receipt_ids, sku_indices)),
    shape=(n_receipts, n_skus),
)
freq = mine(csr, min_support=0.001, column_names=sku_names)
```

---

## What's Next?

- [Migration from mlxtend](migration.md)
- [API Reference](api-reference.md)
- [Polars Support](polars.md)
- [Recommender Workflows](recommender.md)
