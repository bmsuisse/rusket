{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#rusket","title":"rusket","text":"<p>Blazing-fast FP-Growth and Association Rules for Python \u2014 pure Rust via PyO3.</p> <p> </p>"},{"location":"#what-is-rusket","title":"What is rusket?","text":"<p><code>rusket</code> is a drop-in replacement for <code>mlxtend.frequent_patterns.fpgrowth</code> and <code>mlxtend.frequent_patterns.association_rules</code> \u2014 identical API, significantly faster, dramatically lower memory footprint.</p> <p>The core algorithm is implemented in Rust via PyO3 and maturin, with three optimised dispatch paths exposed to Python:</p> Input Rust path Notes Dense pandas DataFrame <code>fpgrowth_from_dense</code> Flat <code>uint8</code> buffer \u2014 zero-copy Sparse pandas DataFrame <code>fpgrowth_from_csr</code> Raw CSR arrays \u2014 zero-copy Polars DataFrame <code>fpgrowth_from_dense</code> Arrow-backed <code>numpy</code> buffer"},{"location":"#why-rusket","title":"Why rusket?","text":"Feature rusket mlxtend Speed (medium dataset) ~0.4 s ~4 s Memory (large dataset) ~3 s OOM Polars support \u2705 \u274c Sparse DataFrame support \u2705 \u26a0\ufe0f limited Zero Python dependencies \u2705 (<code>numpy</code>, <code>pandas</code>) \u274c (many) 12 association metrics \u2705 \u2705"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom rusket import fpgrowth, association_rules\n\ndf = pd.DataFrame({\n    \"milk\":  [1, 1, 0, 1],\n    \"bread\": [1, 0, 1, 1],\n    \"eggs\":  [0, 1, 1, 1],\n})\n\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nrules = association_rules(freq, num_itemsets=len(df), metric=\"confidence\", min_threshold=0.6)\nprint(rules[[\"antecedents\", \"consequents\", \"confidence\"]])\n</code></pre> <p>Get Started API Reference View on GitHub</p>"},{"location":"1b-challenge/","title":"The Road to 1 Billion Rows","text":"<p>A story of memory management, algorithmic trade-offs, and the satisfying click of watching numbers improve.</p> <p>At the core of <code>rusket</code> is the belief that frequent itemset mining should scale on a single machine. No Spark cluster, no distributed coordination \u2014 just one process, tuned to be as efficient as possible.</p> <p>The 1 Billion Row Challenge is how we hold ourselves accountable.</p>"},{"location":"1b-challenge/#where-we-started-sorted-chunks-k-way-merge","title":"Where We Started: Sorted Chunks + K-Way Merge","text":"<p>The first streaming design took inspiration from external-sort databases:</p> <ol> <li>Each call to <code>add_chunk()</code> receives <code>(txn_id, item_id)</code> arrays from Python.</li> <li>Rust sorts each chunk in-place using <code>rayon::par_sort_unstable()</code>.</li> <li>Chunks exceeding the RAM budget are spilled to anonymous <code>tempfile</code> files on disk.</li> <li>When <code>.mine()</code> is called, a k-way heap merge streams all sorted chunks in order, building the CSR matrix on the fly.</li> </ol> <p>The appeal was clear: never hold more than one chunk in RAM at once. </p> <pre><code>Memory: O(chunk_size)\nmine() memory: O(k cursors + CSR output)\n</code></pre>"},{"location":"1b-challenge/#the-first-problem-disk-exhaustion","title":"The first problem: disk exhaustion","text":"<p>Running the 500M \u2192 1B targets, our machine hit 99% disk usage. The culprit: 500k transactions/chunk \u00d7 23 items/txn \u00d7 12 bytes/pair = ~138 MB per chunk. At 1B rows that's ~2,000 chunks = ~276 GB of tempfiles. Oops.</p> <p>We tried raising <code>max_ram_mb</code> to 8,000 \u2014 but the process was OOM-killed at 800M rows (exit code 137).</p>"},{"location":"1b-challenge/#the-second-problem-mine_t-grows-super-linearly","title":"The second problem: <code>mine_t</code> grows super-linearly","text":"<p>Even with disk spilling working, the k-way merge over thousands of files was slow:</p> target_rows add_t mine_t total 300M 36.1s 516.8s 552.9s 500M 65.2s 1543.5s 1608.7s <p>The heap with 1,000+ cursors causes cache thrashing. <code>mine_t</code> was growing super-linearly \u2014 a fundamental problem with the architecture.</p>"},{"location":"1b-challenge/#the-insight-hashmap-aggregation","title":"The Insight: HashMap Aggregation","text":"<p>The key observation was that the sorted-chunk approach stores every pair from every chunk, even if the same transaction appears in 100 different chunks. The real data that matters is:</p> <pre><code>unique_transactions \u00d7 avg_items_per_transaction\n</code></pre> <p>For 1B rows with ~43M unique transactions \u00d7 23 items: that's only ~5GB \u2014 vs the sorted approach's ~12GB.</p> <p>We replaced the entire chunk + merge system with a single <code>AHashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>:</p> <pre><code>pub fn add_chunk(&amp;mut self, txn_ids: ..., item_ids: ...) {\n    for (&amp;t, &amp;i) in txns.iter().zip(items.iter()) {\n        self.txns.entry(t).or_default().push(i);\n    }\n}\n</code></pre> <p><code>mine()</code> now just: 1. Collects <code>(txn_id, &amp;items)</code> from the HashMap 2. <code>par_sort</code> by txn_id 3. Sort+dedup each transaction's item list 4. Build CSR \u2192 feed to algorithm</p> <p>No k-way merge. No disk spill. No tempfiles.</p>"},{"location":"1b-challenge/#initial-numbers-100m-200m-fp-growth","title":"Initial numbers (100M &amp; 200M, FP-Growth)","text":"target_rows add_t mine_t total 100M 5.7s 26.6s 32.3s 200M 10.4s 84.6s 95.0s <p>Compared to the old approach's 300M taking 299.5s \u2014 we were doing 200M in 95s. A 3\u00d7 speedup just from the architecture change.</p>"},{"location":"1b-challenge/#what-we-tried-the-iteration-phase","title":"What We Tried: The Iteration Phase","text":"<p>We then instrumented the benchmark to systematically compare every knob we could turn.</p>"},{"location":"1b-challenge/#approach-smallvec-u16-items-regression","title":"Approach: SmallVec + u16 items (\u274c Regression)","text":"<p>The idea: use <code>SmallVec&lt;[u16; 32]&gt;</code> to store items inline on the stack (avoiding heap allocations for short transactions), and <code>u16</code> instead of <code>i32</code> to halve memory consumption.</p> <p>In theory: 43M \u00d7 (8 + 2\u00d732) = ~3GB vs ~5GB.</p> <p>In practice: a 2\u00d7 slowdown. The 32-element inline size is too large for the CPU stack, causes cache pressure on every HashMap lookup, and the <code>u16\u2192i32</code> conversion at mine time adds hidden overhead.</p> <p>Lesson: measure first, optimize second.</p>"},{"location":"1b-challenge/#knob-chunk-size-100k-vs-500k-vs-2m","title":"Knob: Chunk size (100k vs 500k vs 2M)","text":"chunk add_t mine_t total 100k 10.0s 3.8s 13.8s 500k 12.1s 5.3s 17.4s 2M 12.5s 5.3s 17.8s <p>Chunk size barely matters \u2014 <code>add_t</code> and <code>mine_t</code> are dominated by HashMap operations and algorithm complexity, not chunk boundary overhead.</p>"},{"location":"1b-challenge/#knob-algorithm-fp-growth-vs-eclat","title":"Knob: Algorithm (FP-Growth vs Eclat)","text":"<p>This was the biggest discovery. At 100M rows:</p> method add_t mine_t total M rows/s fpgrowth 10.0s ~55s ~65s 1.5 eclat 10.0s 3.8s 13.8s 7.24 <p>Eclat is ~14\u00d7 faster at mining for dense retail data (23 items/txn). The reason: Eclat works with vertical tidlists \u2014 for dense datasets with many frequent 2-itemsets, the intersection operations are extremely cache-friendly, while FP-Growth's conditional pattern base construction has much higher memory pressure.</p>"},{"location":"1b-challenge/#final-results-the-road-to-1b","title":"Final Results: The Road to 1B","text":"<p>Running with Eclat + 500k chunks on real bootstrapped retail data:</p>"},{"location":"1b-challenge/#dense-retail-data-andi_data-8416-txns-119-items-avg-23-itemstxn","title":"Dense retail data (andi_data: 8,416 txns \u00d7 119 items, avg 23 items/txn)","text":"target_rows add_t mine_t total M rows/s itemsets 100M 12.5s 6.2s 18.7s 5.35 15,218 200M 22.7s 13.4s 36.1s 5.53 15,226 500M 61.1s 38.7s 99.8s 5.01 15,234 1B 173.5s 208.5s 382.1s 2.62 15,233 <p>\u2705 1 Billion rows. 382 seconds. 15,233 frequent itemsets. No OOM. No disk spill.</p> <p>The itemset count is consistent across all scales (15,218\u201315,234), confirming the synthetic sampling faithfully preserves the original item frequency distribution.</p>"},{"location":"1b-challenge/#sparse-catalogue-data-andi_data2-540k-txns-2603-items-avg-44-itemstxn","title":"Sparse catalogue data (andi_data2: 540k txns \u00d7 2,603 items, avg 4.4 items/txn)","text":"target_rows add_t mine_t total M rows/s itemsets 100M 20.2s 4.9s 25.1s 4.00 37 200M 26.2s 9.2s 35.4s 5.67 37 500M \u274c \u274c OOM - - 1B \u274c \u274c OOM - - <p>While sparse data (4.4 items/txn) is faster end-to-end for smaller datasets due to fewer items per transaction, the memory overhead of the HashMap scaling up to 1 billion rows on this specific dataset shape still exceeds our machine's constraints.</p> <p>The 1B challenge for dataset 2 remains open.</p>"},{"location":"1b-challenge/#what-we-learned","title":"What We Learned","text":""},{"location":"1b-challenge/#architecture-beats-micro-optimisation","title":"Architecture beats micro-optimisation","text":"<p>The jump from k-way merge \u2192 HashMap changed 5-minute runs into 30-second runs. No amount of SIMD or loop unrolling would have bridged that gap.</p>"},{"location":"1b-challenge/#eclat-vs-fp-growth-depends-on-density","title":"Eclat vs FP-Growth depends on density","text":"data density winner why Dense (avg &gt;10 items/txn) Eclat Tidlist intersections are O(n), very cache-friendly Sparse (avg &lt;5 items/txn) FP-Growth or similar Fewer candidates, conditional bases are small <p>The default <code>method=\"eclat\"</code> is now recommended for most real-world transaction data.</p>"},{"location":"1b-challenge/#add_t-scales-linearly-mine_t-grows-with-complexity","title":"<code>add_t</code> scales linearly; <code>mine_t</code> grows with complexity","text":"<p><code>add_t</code> is essentially O(total_pairs) \u2014 just HashMap insertions. <code>mine_t</code> grows with the number of candidate itemsets, which is roughly stable once you have a representative sample. This explains why going from 500M \u2192 1B doubles <code>add_t</code> but only doubles <code>mine_t</code>.</p>"},{"location":"1b-challenge/#running-it-yourself","title":"Running It Yourself","text":"<pre><code>from rusket import FPMiner\nimport numpy as np\n\n# Stream your data in chunks\nminer = FPMiner(n_items=your_n_items)\n\nfor txn_ids_chunk, item_ids_chunk in your_data_stream():\n    miner.add_chunk(\n        txn_ids_chunk.astype(np.int64),\n        item_ids_chunk.astype(np.int32),\n    )\n\n# Mine \u2014 Eclat is the winner for dense retail data\nfreq = miner.mine(min_support=0.02, max_len=3, method=\"eclat\")\nprint(f\"Found {len(freq):,} frequent itemsets\")\n</code></pre> <p>The full benchmark script is in <code>benchmarks/bench_fpminer_realistic.py</code>.</p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#mine-recommended","title":"<code>mine</code> (Recommended)","text":"<pre><code>from rusket import mine\n\nmine(\n    df,\n    min_support=0.5,\n    null_values=False,\n    use_colnames=False,\n    max_len=None,\n    method=\"auto\",\n    verbose=0,\n) -&gt; pd.DataFrame\n</code></pre> <p>Dynamically selects the optimal mining algorithm (<code>fpgrowth</code> or <code>eclat</code>) based on the dataset density heuristically. It's highly recommended to use this entry point instead of calling the algorithms directly.</p>"},{"location":"api-reference/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>df</code> <code>pd.DataFrame \\| pl.DataFrame</code> \u2014 One-hot encoded DataFrame. Rows are transactions, columns are items. Accepts bool or 0/1 integer values. Sparse pandas DataFrames are supported via the CSR path. <code>min_support</code> <code>float</code> <code>0.5</code> Minimum support threshold in <code>(0, 1]</code>. Items occurring in fewer than <code>ceil(min_support \u00d7 n_rows)</code> transactions are excluded. <code>null_values</code> <code>bool</code> <code>False</code> Allow NaN values in <code>df</code> (pandas only). When <code>True</code>, NaNs are treated as zeros. <code>use_colnames</code> <code>bool</code> <code>False</code> If <code>True</code>, itemsets contain column names instead of integer column indices. <code>max_len</code> <code>int \\| None</code> <code>None</code> Maximum itemset length. <code>None</code> means unlimited. <code>method</code> <code>\"auto\" \\| \"fpgrowth\" \\| \"eclat\"</code> <code>\"auto\"</code> Algorithm to use. \"auto\" selects Eclat for sparse datasets and FP-Growth for dense ones. <code>verbose</code> <code>int</code> <code>0</code> Verbosity level."},{"location":"api-reference/#returns","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns:</p> Column Type Description <code>support</code> <code>float</code> Support of the itemset (fraction of transactions). <code>itemsets</code> <code>frozenset</code> Set of column indices (or names when <code>use_colnames=True</code>)."},{"location":"api-reference/#raises","title":"Raises","text":"Exception Condition <code>ValueError</code> <code>min_support</code> \u2264 0 <code>TypeError</code> <code>df</code> is not a pandas or Polars DataFrame"},{"location":"api-reference/#examples","title":"Examples","text":"Dense pandasSparse pandasPolars <pre><code>import pandas as pd\nfrom rusket import mine\n\ndf = pd.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = mine(df, min_support=0.5, use_colnames=True)\n</code></pre> <pre><code>import pandas as pd\nfrom pandas.arrays import SparseArray\nfrom rusket import mine\n\ndf = pd.DataFrame.sparse.from_spmatrix(my_csr_matrix, columns=items)\nfreq = mine(df, min_support=0.1, use_colnames=True)\n</code></pre> <pre><code>import polars as pl\nfrom rusket import mine\n\ndf = pl.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = mine(df, min_support=0.5, use_colnames=True)\n</code></pre>"},{"location":"api-reference/#fpgrowth","title":"<code>fpgrowth</code>","text":"<pre><code>from rusket import fpgrowth\n\nfpgrowth(\n    df,\n    min_support=0.5,\n    null_values=False,\n    use_colnames=False,\n    max_len=None,\n    verbose=0,\n) -&gt; pd.DataFrame\n</code></pre> <p>Find frequent itemsets in a one-hot encoded transaction DataFrame using the FP-Growth algorithm, implemented in Rust for maximum performance.</p>"},{"location":"api-reference/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>df</code> <code>pd.DataFrame \\| pl.DataFrame</code> \u2014 One-hot encoded DataFrame. Rows are transactions, columns are items. Accepts bool or 0/1 integer values. Sparse pandas DataFrames are supported via the CSR path. <code>min_support</code> <code>float</code> <code>0.5</code> Minimum support threshold in <code>(0, 1]</code>. Items occurring in fewer than <code>ceil(min_support \u00d7 n_rows)</code> transactions are excluded. <code>null_values</code> <code>bool</code> <code>False</code> Allow NaN values in <code>df</code> (pandas only). When <code>True</code>, NaNs are treated as zeros. <code>use_colnames</code> <code>bool</code> <code>False</code> If <code>True</code>, itemsets contain column names instead of integer column indices. <code>max_len</code> <code>int \\| None</code> <code>None</code> Maximum itemset length. <code>None</code> means unlimited. <code>verbose</code> <code>int</code> <code>0</code> Verbosity level. Currently unused; kept for API compatibility with mlxtend."},{"location":"api-reference/#returns_1","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns:</p> Column Type Description <code>support</code> <code>float</code> Support of the itemset (fraction of transactions). <code>itemsets</code> <code>frozenset</code> Set of column indices (or names when <code>use_colnames=True</code>)."},{"location":"api-reference/#raises_1","title":"Raises","text":"Exception Condition <code>ValueError</code> <code>min_support</code> \u2264 0 <code>TypeError</code> <code>df</code> is not a pandas or Polars DataFrame"},{"location":"api-reference/#examples_1","title":"Examples","text":"Dense pandasSparse pandasPolars <pre><code>import pandas as pd\nfrom rusket import fpgrowth\n\ndf = pd.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\n</code></pre> <pre><code>import pandas as pd\nfrom pandas.arrays import SparseArray\nfrom rusket import fpgrowth\n\ndf = pd.DataFrame.sparse.from_spmatrix(my_csr_matrix, columns=items)\nfreq = fpgrowth(df, min_support=0.1, use_colnames=True)\n</code></pre> <pre><code>import polars as pl\nfrom rusket import fpgrowth\n\ndf = pl.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\n</code></pre>"},{"location":"api-reference/#eclat","title":"<code>eclat</code>","text":"<pre><code>from rusket import eclat\n\neclat(\n    df,\n    min_support=0.5,\n    null_values=False,\n    use_colnames=False,\n    max_len=None,\n    verbose=0,\n) -&gt; pd.DataFrame\n</code></pre> <p>Find frequent itemsets using the Eclat algorithm (vertical bitset representation with hardware <code>popcnt</code> for support counting). Same parameters and return value as <code>fpgrowth</code>.</p>"},{"location":"api-reference/#parameters_2","title":"Parameters","text":"<p>Same as <code>fpgrowth</code> \u2014 see above.</p>"},{"location":"api-reference/#returns_2","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns <code>['support', 'itemsets']</code> \u2014 identical format to <code>fpgrowth</code>.</p>"},{"location":"api-reference/#examples_2","title":"Examples","text":"<pre><code>import pandas as pd\nfrom rusket import eclat\n\ndf = pd.DataFrame({\"a\": [True,True,False], \"b\": [True,False,True], \"c\": [False,True,True]})\nfreq = eclat(df, min_support=0.5, use_colnames=True)\n</code></pre>"},{"location":"api-reference/#association_rules","title":"<code>association_rules</code>","text":"<pre><code>from rusket import association_rules\n\nassociation_rules(\n    df,\n    num_itemsets,\n    df_orig=None,\n    null_values=False,\n    metric=\"confidence\",\n    min_threshold=0.8,\n    support_only=False,\n    return_metrics=ALL_METRICS,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate association rules from a DataFrame of frequent itemsets. The rule-generation and metric computation is performed in Rust.</p>"},{"location":"api-reference/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> \u2014 Output of <code>mine()</code> or <code>fpgrowth()</code> with columns <code>['support', 'itemsets']</code>. <code>num_itemsets</code> <code>int</code> \u2014 Total number of transactions in the original dataset (= <code>len(original_df)</code>). <code>df_orig</code> <code>pd.DataFrame \\| None</code> <code>None</code> Original (non-binarised) DataFrame. Only needed when <code>null_values=True</code>. <code>null_values</code> <code>bool</code> <code>False</code> Apply null-value correction (not yet on the Rust path; falls back gracefully). <code>metric</code> <code>str</code> <code>\"confidence\"</code> Primary filter metric. See Metrics below. <code>min_threshold</code> <code>float</code> <code>0.8</code> Minimum value of <code>metric</code> for a rule to be included in the result. <code>support_only</code> <code>bool</code> <code>False</code> If <code>True</code>, only compute support; fill all other metrics with <code>NaN</code>. <code>return_metrics</code> <code>list[str]</code> all 12 Metric columns to include in the result DataFrame."},{"location":"api-reference/#returns_3","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns:</p> Column Type Description <code>antecedents</code> <code>frozenset</code> Left-hand side (LHS) of the rule. <code>consequents</code> <code>frozenset</code> Right-hand side (RHS) of the rule. <code>antecedent support</code> <code>float</code> Support of the antecedent alone. <code>consequent support</code> <code>float</code> Support of the consequent alone. <code>support</code> <code>float</code> Support of the full rule (LHS \u222a RHS). <code>confidence</code> <code>float</code> P(RHS | LHS). <code>lift</code> <code>float</code> Confidence / consequent support. <code>representativity</code> <code>float</code> Fraction of transactions covered by the rule. <code>leverage</code> <code>float</code> Support \u2212 antecedent_support \u00d7 consequent_support. <code>conviction</code> <code>float</code> (1 \u2212 consequent_support) / (1 \u2212 confidence). <code>zhangs_metric</code> <code>float</code> Zhang's correlation metric. <code>jaccard</code> <code>float</code> Jaccard similarity of antecedent and consequent. <code>certainty</code> <code>float</code> Certainty factor. <code>kulczynski</code> <code>float</code> Kulczynski measure."},{"location":"api-reference/#metrics","title":"Metrics","text":"<p>The <code>metric</code> parameter accepts any of:</p> <p><code>confidence</code> \u00b7 <code>lift</code> \u00b7 <code>support</code> \u00b7 <code>leverage</code> \u00b7 <code>conviction</code> \u00b7 <code>zhangs_metric</code> \u00b7 <code>jaccard</code> \u00b7 <code>certainty</code> \u00b7 <code>kulczynski</code> \u00b7 <code>representativity</code> \u00b7 <code>antecedent support</code> \u00b7 <code>consequent support</code></p>"},{"location":"api-reference/#raises_2","title":"Raises","text":"Exception Condition <code>ValueError</code> <code>df</code> is missing <code>'support'</code> or <code>'itemsets'</code> columns <code>ValueError</code> <code>df</code> is empty <code>ValueError</code> Unknown <code>metric</code> value and <code>support_only=False</code>"},{"location":"api-reference/#fpminer","title":"<code>FPMiner</code>","text":"<pre><code>from rusket import FPMiner\n\nminer = FPMiner(n_items=500_000)\n</code></pre> <p>Streaming accumulator for billion-scale datasets.  Accepts chunks of <code>(transaction_id, item_id)</code> integer arrays one at a time \u2014 Rust accumulates them in a <code>HashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>.  Peak Python memory = one chunk.</p>"},{"location":"api-reference/#constructor","title":"Constructor","text":"Parameter Type Description <code>n_items</code> <code>int</code> Number of distinct items (column count). Item IDs must be in <code>[0, n_items)</code>."},{"location":"api-reference/#methods","title":"Methods","text":""},{"location":"api-reference/#add_chunktxn_ids-item_ids-self","title":"<code>add_chunk(txn_ids, item_ids) \u2192 self</code>","text":"<p>Feed a chunk of integer pairs into the accumulator.</p> Parameter Type Description <code>txn_ids</code> <code>np.ndarray[int64]</code> Transaction IDs (arbitrary integers). <code>item_ids</code> <code>np.ndarray[int32]</code> Item column indices <code>[0, n_items)</code>."},{"location":"api-reference/#minemin_support-max_len-use_colnames-column_names-method-pddataframe","title":"<code>mine(min_support, max_len, use_colnames, column_names, method) \u2192 pd.DataFrame</code>","text":"<p>Mine frequent itemsets from all accumulated data.</p> Parameter Type Default Description <code>min_support</code> <code>float</code> <code>0.5</code> Minimum support in <code>(0, 1]</code>. <code>max_len</code> <code>int \\| None</code> <code>None</code> Maximum itemset length. <code>use_colnames</code> <code>bool</code> <code>False</code> Return column names instead of indices. <code>column_names</code> <code>list[str] \\| None</code> <code>None</code> Names for columns when <code>use_colnames=True</code>. <code>method</code> <code>\"fpgrowth\" \\| \"eclat\"</code> <code>\"fpgrowth\"</code> Mining algorithm."},{"location":"api-reference/#reset","title":"<code>reset()</code>","text":"<p>Free all accumulated data.</p>"},{"location":"api-reference/#properties","title":"Properties","text":"Property Type Description <code>n_transactions</code> <code>int</code> Distinct transactions accumulated so far. <code>n_items</code> <code>int</code> Column count (set at construction)."},{"location":"api-reference/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom rusket import FPMiner\n\nminer = FPMiner(n_items=500_000)\n\n# Process a Parquet file in 10M-row chunks\nfor chunk in pd.read_parquet(\"orders.parquet\", chunksize=10_000_000):\n    txn = chunk[\"txn_id\"].to_numpy(dtype=\"int64\")\n    item = chunk[\"item_idx\"].to_numpy(dtype=\"int32\")\n    miner.add_chunk(txn, item)\n\nfreq = miner.mine(min_support=0.001, max_len=3, use_colnames=False)\n</code></pre>"},{"location":"api-reference/#from_transactions_csr","title":"<code>from_transactions_csr</code>","text":"<pre><code>from rusket import from_transactions_csr\n\ncsr, column_names = from_transactions_csr(\n    data,\n    transaction_col=None,\n    item_col=None,\n    chunk_size=10_000_000,\n)\n</code></pre> <p>Converts long-format transactional data to a raw <code>scipy.sparse.csr_matrix</code> and a list of column names, for direct input into <code>fpgrowth()</code> or <code>eclat()</code>.</p> <p>Accepts the same input types as <code>from_transactions</code>, plus a file path to a Parquet file, which is read in chunks to avoid loading all data into memory at once.</p>"},{"location":"api-reference/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>data</code> <code>pd.DataFrame \\| str \\| Path</code> \u2014 A pandas/Polars/Spark DataFrame, or a file path to a Parquet file. <code>transaction_col</code> <code>str \\| None</code> <code>None</code> Transaction ID column name (defaults to first column). <code>item_col</code> <code>str \\| None</code> <code>None</code> Item column name (defaults to second column). <code>chunk_size</code> <code>int</code> <code>10_000_000</code> Rows per chunk for large files."},{"location":"api-reference/#returns_4","title":"Returns","text":"<p><code>tuple[scipy.sparse.csr_matrix, list[str]]</code> \u2014 CSR matrix + column names.</p>"},{"location":"api-reference/#example_1","title":"Example","text":"<pre><code>from rusket import from_transactions_csr, mine\n\n# From Parquet \u2014 never loads entire file\ncsr, names = from_transactions_csr(\"orders.parquet\", chunk_size=10_000_000)\nfreq = mine(csr, min_support=0.001, use_colnames=True, column_names=names)\n</code></pre>"},{"location":"api-reference/#recommendation-analytics","title":"Recommendation &amp; Analytics","text":""},{"location":"api-reference/#rusketrecommendrecommender","title":"<code>rusket.recommend.Recommender</code>","text":"<p>High-level Hybrid Recommender that combines ALS and Association Rules.</p> <pre><code>from rusket import Recommender\n\nrec = Recommender(als_model=als, rules_df=rules_df)\nrec.recommend_for_user(user_id=42, n=5)\nrec.recommend_for_cart(cart_items=[14, 7], n=3)\n</code></pre>"},{"location":"api-reference/#rusketscore_potential","title":"<code>rusket.score_potential</code>","text":"<p>Calculates cross-selling potential scores to identify \"missed opportunities\" for users who should have bought an item by now but haven't.</p> <pre><code>from rusket import score_potential\n\nscores = score_potential(user_history, als_model, target_categories=[101, 102])\n</code></pre>"},{"location":"api-reference/#rusketsimilar_items","title":"<code>rusket.similar_items</code>","text":"<p>Find the most similar items to a given item ID based on ALS/BPR latent factors using fast Cosine Similarity.</p> <pre><code>from rusket import similar_items\n\nsimilar_ids, scores = similar_items(als_model, item_id=99, n=5)\n</code></pre>"},{"location":"api-reference/#rusketexport_item_factors","title":"<code>rusket.export_item_factors</code>","text":"<p>Exports ALS/BPR latent item factors as a Pandas DataFrame for Vector DBs (FAISS, Qdrant, Pinecone) for Retrieval-Augmented Generation (RAG).</p> <pre><code>from rusket import export_item_factors\n\ndf_vectors = export_item_factors(als_model, include_labels=True)\n</code></pre>"},{"location":"api-reference/#rusketvizto_networkx","title":"<code>rusket.viz.to_networkx</code>","text":"<p>Converts a <code>rusket</code> association rules DataFrame into a NetworkX Directed Graph. Useful for product clustering and visualization.</p> <pre><code>from rusket.viz import to_networkx\n\nG = to_networkx(rules_df, edge_attr=\"lift\")\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>rusket is structured as a thin Python layer over a Rust core, compiled as a native extension module via PyO3 and maturin.</p>"},{"location":"architecture/#repository-layout","title":"Repository layout","text":"<pre><code>rusket/\n\u251c\u2500\u2500 src/                          # Rust (PyO3)\n\u2502   \u251c\u2500\u2500 lib.rs                    # Module root \u2014 exports to Python\n\u2502   \u251c\u2500\u2500 fpgrowth.rs               # FP-Tree + FP-Growth algorithm\n\u2502   \u251c\u2500\u2500 association_rules.rs      # Rule generation + 12 metrics\n\u2502   \u2514\u2500\u2500 common.rs                 # Shared helpers\n\u251c\u2500\u2500 python/\n\u2502   \u251c\u2500\u2500 rusket/                  # Primary Python package (pyproject.toml name)\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 fpgrowth.py           # Dispatch + numpy conversion\n\u2502   \u2502   \u251c\u2500\u2500 association_rules.py  # Label mapping + Rust call\n\u2502   \u2502   \u2514\u2500\u2500 _validation.py        # Input validation helpers\n\u2502   \u2514\u2500\u2500 fpgrowth_pyo3/            # Legacy compat package\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 test_fpbase.py            # Shared base test classes\n    \u251c\u2500\u2500 test_fpgrowth.py          # FP-Growth tests\n    \u251c\u2500\u2500 test_association_rules.py # Association rules tests\n    \u2514\u2500\u2500 test_benchmark.py         # Performance benchmarks\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<pre><code>flowchart TD\n    A[\"Python caller\\nfpgrowth(df, ...)\"] --&gt; B{Input type?}\n\n    B --&gt;|Dense pandas| C[\"np.ascontiguousarray\\n(uint8, C-order)\"]\n    B --&gt;|Sparse pandas| D[\"df.sparse.to_coo().tocsr()\\nindptr + indices as int32\"]\n    B --&gt;|Polars| E[\"df.to_numpy()\\n(Arrow zero-copy)\"]\n\n    C --&gt; F[\"Rust: fpgrowth_from_dense\\nPyReadonlyArray2&lt;u8&gt;\"]\n    D --&gt; G[\"Rust: fpgrowth_from_csr\\nPyReadonlyArray1&lt;i32&gt;\"]\n    E --&gt; F\n\n    F --&gt; H[\"FP-Tree construction\\n(Rust, single-pass)\"]\n    G --&gt; H\n\n    H --&gt; I[\"Recursive mining\\n(Rayon parallel)\"]\n    I --&gt; J[\"Vec&lt;(count, Vec&lt;usize&gt;)&gt;\"]\n    J --&gt; K[\"Python: build DataFrame\\n(frozensets, support)\"]</code></pre>"},{"location":"architecture/#fp-growth-algorithm","title":"FP-Growth algorithm","text":"<p>The Rust implementation follows the classic Han et al. (2000) FP-Growth algorithm:</p> <ol> <li>Header table scan \u2014 count item frequencies; prune items below <code>min_count</code>.</li> <li>FP-Tree construction \u2014 single-pass over transactions; compress into a prefix-tree structure.</li> <li>Recursive mining \u2014 for each frequent item, extract the conditional pattern base, build a conditional FP-Tree, and mine it recursively.</li> <li>Output \u2014 each leaf path materialises as one frequent itemset <code>(count, items)</code>.</li> </ol>"},{"location":"architecture/#dispatch-paths","title":"Dispatch paths","text":"Path Rust function Input shape Notes Dense pandas <code>fpgrowth_from_dense</code> <code>[n_rows \u00d7 n_cols]</code> uint8 Contiguous C array Sparse pandas <code>fpgrowth_from_csr</code> CSR <code>indptr + indices</code> Zero-copy scipy CSR Polars <code>fpgrowth_from_dense</code> same as dense Arrow \u2192 NumPy view"},{"location":"architecture/#association-rules","title":"Association rules","text":"<p>Rule generation is vectorised in Rust:</p> <ol> <li>For each frequent itemset of length \u2265 2, enumerate all non-empty antecedent / consequent splits.</li> <li>Look up antecedent and consequent supports from a pre-built hash map.</li> <li>Compute all 12 metrics in a single pass; filter by <code>(metric, min_threshold)</code>.</li> <li>Return raw integer index lists to Python; Python maps back to column names / frozensets.</li> </ol>"},{"location":"architecture/#building-from-source","title":"Building from source","text":"<pre><code># Prerequisites: Rust 1.83+, Python 3.10+, uv\nrustup update\nuv sync\n\n# Debug build (fast compile, slower runtime)\nuv run maturin develop\n\n# Release build (optimised)\nuv run maturin develop --release\n\n# Type checking\nuv run pyright python/\n\n# Tests\nuv run pytest tests/ -x -q\n\n# Cargo lint\ncargo check\ncargo clippy\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>rusket includes two mining algorithms (FP-Growth and Eclat), both implemented in Rust. These benchmarks compare rusket against mlxtend (pure Python) on synthetic and real-world datasets.</p> <p>Measured on Apple M-series (arm64).</p>"},{"location":"benchmarks/#scale-benchmarks-1m-1b-rows","title":"Scale Benchmarks: 1M \u2192 1B Rows \ud83d\ude80","text":""},{"location":"benchmarks/#interactive-chart","title":"Interactive Chart","text":""},{"location":"benchmarks/#three-input-paths","title":"Three Input Paths","text":"<p>rusket supports three ways to ingest data at scale:</p> <ol> <li><code>from_transactions</code> \u2192 sparse DataFrame \u2014 returns a pandas DataFrame, easy API</li> <li>Direct CSR \u2192 Rust \u2014 pass <code>scipy.sparse.csr_matrix</code> directly to <code>fpgrowth()</code>, skips pandas entirely</li> <li><code>FPMiner</code> Streaming \u2014 memory-safe accumulator for 100M+ rows that don't fit in RAM</li> </ol>"},{"location":"benchmarks/#in-memory-scale-fpgrowth","title":"In-Memory Scale (fpgrowth)","text":"Scale <code>from_transactions</code> \u2192 fpgrowth Direct CSR \u2192 fpgrowth Speedup 1M rows (200k txns \u00d7 10k items) 5.0s 0.1s 50\u00d7 10M rows (2M txns \u00d7 50k items) 24.4s 1.2s 20\u00d7 50M rows (10M txns \u00d7 100k items) 63.1s 4.0s 15\u00d7 100M rows (20M txns \u00d7 200k items) 134.2s 10.1s 13\u00d7 200M rows (40M txns \u00d7 200k items) 246.8s 17.6s 14\u00d7 <p>Direct CSR is the power-user path</p> <p>At 100M rows, direct CSR mining takes 1.2 seconds \u2014 the bottleneck is entirely the CSR build (24.5s). Compare to the pandas sparse path where mining alone takes 9.0s due to <code>sparse.to_coo().tocsr()</code> overhead.</p>"},{"location":"benchmarks/#out-of-core-scale-fpminer-streaming","title":"Out-of-Core Scale (FPMiner Streaming)","text":"<p>For real-world retail datasets scaling to 1 Billion rows, <code>FPMiner</code> uses a memory-safe chunks approach (per-chunk sort + k-way merge).</p> <p>Benchmark: 2,603 retail items, avg 4.4 items/basket, min_support = 0.1%</p> Scale add_chunk() mine() Total Time Itemsets Found 50M rows 4.8s 5.6s 10.4s 1,260 100M rows 10.6s 13.9s 24.6s 1,254 200M rows 22.7s 33.2s 55.9s 1,261 300M rows 30.0s 55.4s 85.4s 1,259"},{"location":"benchmarks/#vs-mlxtend","title":"vs mlxtend","text":"<p>Comparing the <code>FPMiner</code> streaming accumulator against <code>mlxtend</code>'s standard pandas One-Hot Encoded pipeline:</p> Dataset <code>rusket</code> (prep + mine) <code>mlxtend</code> (prep + mine) Speedup 50k rows (10k txns, 100 items) 0.0 s 0.1 s ~5\u00d7 500k rows (50k txns, 500 items) 0.2 s 1.8 s ~9\u00d7 2M rows (500k txns, 2k items) 0.2 s 16.0 s 80\u00d7 <p>Note: <code>mlxtend</code> starts to struggle heavily with the One-Hot Encoding pandas <code>groupby/unstack</code> memory overhead at scale. <code>rusket</code>'s streaming <code>add_chunk</code> combined with <code>eclat</code> processes 2M rows in 0.2s flat.</p>"},{"location":"benchmarks/#the-auto-routine-algorithm","title":"The \"Auto\" Routine Algorithm","text":"<p><code>rusket.mine(method=\"auto\")</code> dynamically selects the algorithm that performs best based on the dataset density (Borgelt 2003 heuristic).</p> <ul> <li>Density &gt; 0.15 (Dense): Automatically routes to FP-Growth. Tree-traversal performs exceptionally well when most transactions contain a massive overlap of identical items.</li> <li>Density &lt; 0.15 (Sparse): Automatically routes to Eclat. On sparse data (like retail baskets), traversing an enormous tree is memory-intensive. Eclat directly uses hardware SIMD array-intersections (<code>popcnt</code>) on the TID-lists, resulting in massive speedups (often 5\u00d7 to 15\u00d7 faster on sparse arrays).</li> </ul>"},{"location":"benchmarks/#real-world-datasets","title":"Real-World Datasets","text":"<p>Datasets from andi611/Apriori-and-Eclat-Frequent-Itemset-Mining.</p> Dataset Transactions Items <code>rusket</code> <code>mlxtend</code> Speedup andi_data.txt 8,416 119 9.7 s (22.8M itemsets) TIMEOUT \ud83d\udca5 \u221e andi_data2.txt 540,455 2,603 7.9 s 16.2 s 2\u00d7 <p>Dense data</p> <p>On <code>andi_data.txt</code> (~23 items/basket), <code>mlxtend</code> can't finish in 60s. <code>rusket</code> mines 22.8M itemsets in under 10s.</p>"},{"location":"benchmarks/#the-power-user-pipeline","title":"The Power-User Pipeline","text":"<p>For maximum performance at 100M+ scale, skip pandas entirely:</p> <pre><code>import numpy as np\nfrom scipy import sparse as sp\nfrom rusket import fpgrowth\n\n# Your data: integer arrays of (txn_id, item_id)\ntxn_ids = np.array([...], dtype=np.int64)  # transaction IDs\nitem_ids = np.array([...], dtype=np.int32)  # item IDs\n\n# Build CSR directly (fast!)\ncsr = sp.csr_matrix(\n    (np.ones(len(txn_ids), dtype=np.int8), (txn_ids, item_ids)),\n    shape=(n_transactions, n_items),\n)\n\n# Mine directly from CSR \u2014 no pandas overhead\nfreq = fpgrowth(csr, min_support=0.001, use_colnames=True,\n                max_len=3, column_names=item_names)\n</code></pre> <p>At 100M rows, the mining step takes 1.2 seconds (not a typo).</p>"},{"location":"benchmarks/#the-1-billion-row-architecture","title":"\ud83c\udfd7 The 1 Billion Row Architecture","text":"<p>To pass the \"1 Billion Row\" threshold without OOM crashes, <code>rusket</code> employs a zero-allocation mining loop:</p> <ol> <li>Eclat Scratch Buffers: <code>intersect_count_into</code> writes intersections directly into thread-local pre-allocated memory bytes and computes <code>popcnt</code> in a single pass. It implements early-exit loop termination the moment it proves a combination cannot reach <code>min_support</code>.</li> <li>FPGrowth Parallel Tree Build: Conditional FP-trees are collected concurrently inside the rayon parallel mining step, replacing the standard sequential loop and eliminating memory contention bottlenecks.</li> <li><code>AHashMap</code> Deduplication: Extremely fast O(N) duplicate basket counting replaces standard O(N log N) unstable sorts in the core pipeline.</li> </ol>"},{"location":"benchmarks/#running-the-benchmarks","title":"Running the benchmarks","text":"<pre><code>uv run maturin develop --release\nuv run python benchmarks/bench_scale.py    # Scale benchmark + Plotly chart\nuv run python benchmarks/bench_realworld.py  # Real-world datasets\nuv run python benchmarks/bench_vs_mlxtend.py # FPMiner streaming vs mlxtend\nuv run pytest tests/test_benchmark.py -v -s  # pytest-benchmark\n</code></pre>"},{"location":"benchmarks/#why-is-rusket-faster","title":"Why is rusket faster?","text":"Technique Description Zero-copy CSR <code>indptr</code>/<code>indices</code> passed to Rust as pointer hand-offs Arena FP-Tree Flat children arena, incremental <code>is_path()</code> tracking Rayon Parallel conditional mining across CPU cores Eclat popcount <code>Vec&lt;u64&gt;</code> bitsets + hardware <code>popcnt</code> for support No Python loops FP-Tree, mining, and metrics all in Rust <code>pd.factorize</code> O(n) integer encoding, faster than <code>pd.Categorical</code> at scale"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes are documented here. This project follows Semantic Versioning.</p>"},{"location":"changelog/#010-2026-02-19","title":"[0.1.0] \u2014 2026-02-19","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>fpgrowth()</code> \u2014 FP-Growth frequent itemset mining backed by Rust + PyO3<ul> <li>Dense pandas path: flat <code>uint8</code> buffer via <code>fpgrowth_from_dense</code> (zero-copy)</li> <li>Sparse pandas path: CSR arrays via <code>fpgrowth_from_csr</code> (zero-copy)</li> <li>Polars path: Arrow-backed NumPy buffer via <code>fpgrowth_from_dense</code></li> <li>Parallel mining via Rayon</li> </ul> </li> <li><code>association_rules()</code> \u2014 Association rule generation with 12 metrics:     <code>confidence</code>, <code>lift</code>, <code>support</code>, <code>leverage</code>, <code>conviction</code>,     <code>zhangs_metric</code>, <code>jaccard</code>, <code>certainty</code>, <code>kulczynski</code>,     <code>representativity</code>, <code>antecedent support</code>, <code>consequent support</code></li> <li>Drop-in API compatibility with <code>mlxtend.frequent_patterns</code></li> <li><code>max_len</code> parameter to cap itemset size</li> <li><code>support_only</code> flag for fast support-only mode</li> <li><code>return_metrics</code> selector to include only desired metric columns</li> <li>Full test suite mirroring mlxtend behaviour</li> </ul>"},{"location":"changelog/#performance-vs-mlxtend","title":"Performance (vs mlxtend)","text":"Dataset Speedup Memory Small (5 \u00d7 11) ~10\u00d7 \u2013 Medium (10k \u00d7 400) ~5\u20138\u00d7 ~8\u00d7 less Large (100k \u00d7 1000) N/A (OOM) handles it"},{"location":"cookbook/","title":"Rusket Cookbook","text":"<p>A hands-on guide to every feature in <code>rusket</code> \u2014 from market basket analysis to billion-scale collaborative filtering.</p>"},{"location":"cookbook/#setup","title":"Setup","text":"<pre><code>pip install rusket\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport polars as pl\nfrom rusket import mine, eclat, association_rules, ALS, BPR\nfrom rusket import prefixspan, sequences_from_event_log, hupm, similar_items, Recommender, score_potential\n</code></pre>"},{"location":"cookbook/#1-market-basket-analysis-with-fpgrowth","title":"1. Market Basket Analysis with FPGrowth","text":""},{"location":"cookbook/#generate-a-synthetic-retail-dataset","title":"Generate a synthetic retail dataset","text":"<pre><code>np.random.seed(42)\n\nitems = [\n    \"Milk\", \"Bread\", \"Butter\", \"Eggs\", \"Cheese\", \"Yogurt\",\n    \"Coffee\", \"Tea\", \"Sugar\", \"Apples\", \"Bananas\", \"Oranges\",\n    \"Chicken\", \"Beef\", \"Fish\", \"Rice\", \"Pasta\", \"Tomato Sauce\",\n    \"Onions\", \"Garlic\",\n]\n\nn_transactions = 10_000\nprobs = np.power(np.arange(1, len(items) + 1, dtype=float), -0.7)\nprobs = np.clip(probs / probs.max() * 0.3, 0.01, 0.8)\n\ndf = pd.DataFrame(\n    np.random.rand(n_transactions, len(items)) &lt; probs,\n    columns=items,\n)\n</code></pre>"},{"location":"cookbook/#find-frequent-itemsets","title":"Find frequent itemsets","text":"<pre><code>fi = mine(df, min_support=0.05, use_colnames=True)\n# Returns a Pandas DataFrame with columns: support, itemsets\nprint(f\"Found {len(fi)} frequent itemsets\")\nfi.sort_values(\"support\", ascending=False).head(10)\n</code></pre>"},{"location":"cookbook/#generate-association-rules","title":"Generate association rules","text":"<pre><code>rules = association_rules(fi, num_itemsets=len(df), min_threshold=0.3)\n# Returns: antecedents, consequents, support, confidence, lift, ...\nstrong = rules[(rules[\"confidence\"] &gt; 0.4) &amp; (rules[\"lift\"] &gt; 1.2)]\nstrong.sort_values(\"lift\", ascending=False).head(10)\n</code></pre>"},{"location":"cookbook/#limit-itemset-length","title":"Limit itemset length","text":"<pre><code># Only find pairs and triples \u2014 much faster for large catalogs\nfi_pairs = mine(df, min_support=0.02, max_len=2, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#2-eclat-when-to-use-vs-fpgrowth","title":"2. ECLAT \u2014 When to Use vs FPGrowth","text":"<p>ECLAT uses a vertical bitset representation. It is faster than FPGrowth for sparse datasets with many unique items and a low support threshold.</p> <pre><code>fi_ec = eclat(df, min_support=0.05, use_colnames=True)\n</code></pre> <p>Rule of thumb:</p> Condition Recommended algorithm Dense dataset, few items <code>mine(method=\"auto\")</code> Sparse dataset, many items, low support <code>mine(method=\"auto\")</code> Very large dataset (100M+ rows) <code>FPMiner</code> with streaming"},{"location":"cookbook/#3-transaction-helpers","title":"3. Transaction Helpers","text":"<p>Convert long-format order data (e.g., from a database) to the one-hot boolean matrix format required by <code>mine</code> (which automatically routes to <code>fpgrowth</code> or <code>eclat</code>).</p>"},{"location":"cookbook/#from-a-pandas-dataframe","title":"From a Pandas DataFrame","text":"<pre><code>from rusket import from_transactions\n\norders = pd.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\n# Converts long-format \u2192 wide boolean matrix\nbasket = from_transactions(orders, user_col=\"order_id\", item_col=\"item\")\nfi = mine(basket, min_support=0.3, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#from-a-polars-dataframe","title":"From a Polars DataFrame","text":"<pre><code>orders_pl = pl.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\nbasket = from_transactions(orders_pl, user_col=\"order_id\", item_col=\"item\")\n</code></pre>"},{"location":"cookbook/#from-a-spark-dataframe","title":"From a Spark DataFrame","text":"<pre><code># Works with PySpark DataFrames via .toPandas() under the hood\nbasket = from_transactions(spark_df, user_col=\"order_id\", item_col=\"item\")\n</code></pre>"},{"location":"cookbook/#4-collaborative-filtering-with-als","title":"4. Collaborative Filtering with ALS","text":"<p><code>ALS</code> (Alternating Least Squares) learns user and item embeddings from implicit feedback (clicks, plays, purchases) and enables personalised recommendations.</p>"},{"location":"cookbook/#prepare-the-interaction-matrix","title":"Prepare the interaction matrix","text":"<pre><code>from scipy.sparse import csr_matrix\n\n# Build a user \u00d7 item interaction matrix (1 = interacted)\nn_users, n_items = 10_000, 5_000\nrows = np.random.randint(0, n_users, size=200_000)\ncols = np.random.randint(0, n_items, size=200_000)\n\nmat = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32), (rows, cols)),\n    shape=(n_users, n_items),\n)\n</code></pre>"},{"location":"cookbook/#fit-the-model","title":"Fit the model","text":"<pre><code>model = ALS(\n    factors=64,        # latent dimension\n    iterations=15,     # ALS alternating steps\n    alpha=40.0,        # confidence scaling\n    regularization=0.01,\n    verbose=True,      # print per-iteration timing\n    cg_iters=3,        # CG solver steps (3 is usually optimal)\n)\nmodel.fit(mat)\n</code></pre>"},{"location":"cookbook/#get-recommendations","title":"Get recommendations","text":"<pre><code># Top-10 items for user 0, excluding already-seen items\nitem_ids, scores = model.recommend_items(user_id=0, n=10, exclude_seen=True)\n\n# Top-10 users likely to enjoy item 5\nuser_ids, scores = model.recommend_users(item_id=5, n=10)\n</code></pre>"},{"location":"cookbook/#fit-from-transaction-data","title":"Fit from transaction data","text":"<pre><code>purchases = pd.DataFrame({\n    \"user_id\": [1, 1, 2, 3, 3, 3],\n    \"item_id\": [101, 102, 101, 103, 104, 101],\n})\n\nmodel = ALS(factors=32, iterations=10, verbose=True)\nmodel.fit_transactions(purchases, user_col=\"user_id\", item_col=\"item_id\")\n</code></pre>"},{"location":"cookbook/#access-latent-factors-directly","title":"Access latent factors directly","text":"<pre><code># NumPy arrays (n_users \u00d7 factors) and (n_items \u00d7 factors)\nprint(model.user_factors.shape)  # (10000, 64)\nprint(model.item_factors.shape)  # (5000, 64)\n</code></pre>"},{"location":"cookbook/#5-out-of-core-als-for-1b-ratings","title":"5. Out-of-Core ALS for 1B+ Ratings","text":"<p>When the interaction matrix exceeds available RAM, use the out-of-core streaming loader. The CSR matrix is stored on SSD and the OS pages data into RAM on demand.</p>"},{"location":"cookbook/#download-and-prepare-the-movielens-1b-dataset","title":"Download and prepare the MovieLens 1B dataset","text":"<pre><code>import urllib.request, tarfile, os\n\n# Download (~1.4 GB)\nurl = \"https://files.grouplens.org/datasets/movielens/ml-20mx16x32.tar\"\nurllib.request.urlretrieve(url, \"ml-1b.tar\")\n\nwith tarfile.open(\"ml-1b.tar\") as t:\n    t.extractall(\"data/ml-1b/\")\n</code></pre>"},{"location":"cookbook/#build-the-out-of-core-csr-matrix","title":"Build the out-of-core CSR matrix","text":"<pre><code>import numpy as np\nfrom scipy import sparse\nfrom pathlib import Path\n\ndata_dir = Path(\"data/ml-1b/ml-20mx16x32\")\nnpz_files = sorted(data_dir.glob(\"trainx*.npz\"))\n\n# Pass 1 \u2014 count ratings per user\nmax_user, max_item, nnz = 0, 0, 0\ncounts = np.zeros(100_000_000, dtype=np.int64)  # pre-allocate for max users\n\nfor f in npz_files:\n    arr = np.load(f)[\"arr_0\"]          # shape (N, 2) \u2014 [user_id, item_id]\n    uids, iids = arr[:, 0], arr[:, 1]\n    max_user = max(max_user, int(uids.max()))\n    max_item = max(max_item, int(iids.max()))\n    chunk_counts = np.bincount(uids, minlength=max_user + 1)\n    counts[:len(chunk_counts)] += chunk_counts\n    nnz += len(uids)\n\nn_users, n_items = max_user + 1, max_item + 1\nindptr = np.zeros(n_users + 1, dtype=np.int64)\nnp.cumsum(counts[:n_users], out=indptr[1:])\n\n# Pass 2 \u2014 write indices/data to SSD memory maps\nmmap_indices = np.memmap(\"indices.mmap\", dtype=np.int32, mode=\"w+\", shape=(nnz,))\nmmap_data    = np.memmap(\"data.mmap\",    dtype=np.float32, mode=\"w+\", shape=(nnz,))\npos = indptr[:-1].copy()\n\nfor f in npz_files:\n    arr = np.load(f)[\"arr_0\"]\n    uids, iids = arr[:, 0].astype(np.int64), arr[:, 1].astype(np.int32)\n    for u, i in zip(uids, iids):\n        p = pos[u]\n        mmap_indices[p] = i\n        mmap_data[p]    = 1.0\n        pos[u] += 1\n\nmmap_indices.flush()\nmmap_data.flush()\n</code></pre>"},{"location":"cookbook/#fit-als-on-the-out-of-core-matrix","title":"Fit ALS on the out-of-core matrix","text":"<pre><code># Bypass scipy's int32 limits by direct property assignment\nmat = sparse.csr_matrix((n_users, n_items))\nmat.indptr  = indptr\nmat.indices = mmap_indices\nmat.data    = mmap_data\n\nmodel = ALS(\n    factors=64,\n    iterations=5,      # fewer iterations for 1B \u2014 each takes hours on SSD\n    alpha=40.0,\n    verbose=True,\n    cg_iters=3,\n)\nmodel.fit(mat)\n</code></pre> <p>Hardware sizing</p> <p>On a machine with \u2265 32 GB RAM the mmap working set stays hot in OS page cache and each iteration completes in ~5 minutes. On 8 GB RAM each iteration is disk-bound and takes hours.</p>"},{"location":"cookbook/#6-bayesian-personalized-ranking-bpr","title":"6. Bayesian Personalized Ranking (BPR)","text":"<p>Unlike ALS which tries to reconstruct the full interaction matrix, BPR explicitly optimizes the model to rank positive observed items higher than unobserved items. This makes BPR excellent for top-N ranking tasks on implicit data (like clicks or views).</p> <pre><code>from rusket import BPR\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\n# Prepare sparse interaction matrix\nrows = np.random.randint(0, 1000, size=5000)\ncols = np.random.randint(0, 500, size=5000)\nmat = csr_matrix((np.ones(5000), (rows, cols)), shape=(1000, 500))\n\n# Initialize and fit BPR with Hogwild! parallel SGD\nmodel = BPR(\n    factors=64,\n    learning_rate=0.01,\n    regularization=0.01,\n    iterations=100,\n    seed=42,\n)\nmodel.fit(mat)\n\n# Recommend items just like ALS\nitems, scores = model.recommend_items(user_id=10, n=5)\n</code></pre>"},{"location":"cookbook/#7-sequential-pattern-mining-prefixspan","title":"7. Sequential Pattern Mining (PrefixSpan)","text":"<p>PrefixSpan discovers frequent sequences of events over time. Unlike standard market basket analysis where subsets within a single transaction are mined, PrefixSpan finds patterns across ordered transactions.</p> <pre><code>import pandas as pd\nfrom rusket import prefixspan, sequences_from_event_log\n\n# 1. Start with an event log (e.g. clickstream)\nevents = pd.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 2, 3, 3, 3],\n    \"timestamp\": [10, 20, 30, 15, 25, 5, 15, 35],\n    \"page\": [\"Home\", \"Product\", \"Cart\", \"Home\", \"Cart\", \"Product\", \"Cart\", \"Checkout\"],\n})\n\n# 2. Convert to the nested list format expected by the Rust miner\nseqs, mapping = sequences_from_event_log(\n    events, user_col=\"user_id\", time_col=\"timestamp\", item_col=\"page\"\n)\n\n# 3. Mine sequential patterns (min_support = number of sequences)\npatterns_df = prefixspan(seqs, min_support=2, max_len=3)\n\n# 4. Map the integer item IDs back to human-readable labels\npatterns_df[\"sequence_labels\"] = patterns_df[\"sequence\"].apply(\n    lambda seq: [mapping[item] for item in seq]\n)\nprint(patterns_df.head())\n</code></pre>"},{"location":"cookbook/#8-high-utility-pattern-mining-hupm","title":"8. High-Utility Pattern Mining (HUPM)","text":"<p>Frequent itemsets aren't always the most profitable. High-Utility Pattern Mining (HUPM) accounts for the utility (e.g., profit margin or revenue) of items to find sets that generate the highest total value across all transactions, regardless of frequency.</p> <pre><code>from rusket import hupm\n\n# Transactions (lists of item IDs) and their corresponding utilities (profit)\ntransactions = [\n    [1, 2, 3],  # Transaction 1: Items 1, 2, 3\n    [1, 3],     # Transaction 2: Items 1, 3\n    [2, 3],     # Transaction 3: Items 2, 3\n]\n\n# The profit of each item inside that specific transaction\nutilities = [\n    [5.0, 2.0, 1.0], # Profits for items 1, 2, 3 in T1\n    [5.0, 1.0],      # Profits for items 1, 3 in T2\n    [2.0, 1.0],      # Profits for items 2, 3 in T3\n]\n\n# Find itemsets with a total global utility &gt;= 7.0\nhigh_value_patterns = hupm(transactions, utilities, min_utility=7.0)\n\n# Output contains the 'utility' and 'itemset'\nprint(high_value_patterns)\n</code></pre>"},{"location":"cookbook/#9-native-polars-integration","title":"9. Native Polars Integration","text":"<p><code>rusket</code> returns itemsets as zero-copy PyArrow <code>ListArray</code> structures, making Polars interoperability very efficient.</p> <pre><code>df_pl = pl.from_pandas(df)\nfi_pl = mine(df_pl, min_support=0.05, use_colnames=True)\n\n# LazyFrame works too:\nlazy = df_pl.lazy()\n# (convert to eager first before passing to fpgrowth)\nfi_pl2 = mine(lazy.collect(), min_support=0.05, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#query-itemsets-with-pyarrow-compute","title":"Query itemsets with PyArrow compute","text":"<pre><code>import pyarrow.compute as pc\n\n# Find itemsets containing \"Milk\" as first element\ncontains_milk = pc.list_element(fi[\"itemsets\"].array, 0) == \"Milk\"\nfi[contains_milk].head()\n</code></pre>"},{"location":"cookbook/#convert-to-python-sets-only-for-small-subsets","title":"Convert to Python sets (only for small subsets)","text":"<pre><code>top_10 = fi.head(10).copy()\ntop_10[\"sets\"] = top_10[\"itemsets\"].apply(set)\n</code></pre>"},{"location":"cookbook/#10-spark-databricks-integration","title":"10. Spark / Databricks Integration","text":"<pre><code>from rusket import from_transactions\nfrom rusket import ALS\n\n# FPGrowth from Spark\nfi = mine(spark_df.toPandas(), min_support=0.05, use_colnames=True)\n\n# ALS from Spark ratings table\nratings_spark = spark.table(\"ratings\")  # user_id, item_id, rating\nmodel = ALS(factors=64, iterations=10, verbose=True)\nmodel.fit_transactions(ratings_spark, user_col=\"user_id\", item_col=\"item_id\", rating_col=\"rating\")\n</code></pre> <p>Large Spark tables</p> <p>For tables with &gt;100M rows, collect a representative sample or use the out-of-core loader from Section 5.</p>"},{"location":"cookbook/#11-tuning-guide","title":"11. Tuning Guide","text":""},{"location":"cookbook/#fpgrowth-eclat","title":"FPGrowth / ECLAT","text":"Parameter Default Effect <code>min_support</code> required Lower \u2192 more itemsets, slower <code>max_len</code> None Cap itemset size \u2014 huge speedup on large catalogs <code>use_colnames</code> False Return column names instead of indices"},{"location":"cookbook/#als","title":"ALS","text":"Parameter Default Notes <code>factors</code> 64 Higher \u2192 better quality, more RAM, slower <code>iterations</code> 15 5\u201315 is typical; diminishing returns after 20 <code>alpha</code> 40.0 Higher \u2192 stronger signal from implicit feedback <code>regularization</code> 0.01 Increase if overfitting; decrease for denser data <code>cg_iters</code> 3 CG solver steps per ALS step \u2014 3 is almost always optimal <code>verbose</code> False Set <code>True</code> to print per-iteration timing"},{"location":"cookbook/#bpr","title":"BPR","text":"Parameter Default Notes <code>factors</code> 64 Higher \u2192 better quality, more RAM. BPR requires more factors than ALS typically <code>iterations</code> 100 BPR uses SGD and requires more iterations than ALS. Try 100-500. <code>learning_rate</code> 0.01 SGD learning rate. Decrease if unstable, increase if slow convergence. <code>regularization</code> 0.01 Increase if overfitting"},{"location":"cookbook/#prefixspan-hupm","title":"PrefixSpan &amp; HUPM","text":"Parameter Default Notes <code>min_support</code> required Defines frequency for PrefixSpan or total value for HUPM <code>max_len</code> None Cap itemset/sequence size to avoid combinatorial explosions"},{"location":"cookbook/#recommendation-quality-tips","title":"Recommendation quality tips","text":"<ul> <li>Use <code>regularization=0.1</code> for very sparse matrices (&lt; 5 interactions/user)</li> <li><code>alpha=10</code> works better for rating-weighted data vs binary implicit feedback for ALS</li> <li>For top-N ranking optimization directly, use BPR instead of ALS. ALS is better for score prediction and serendipity.</li> <li>For the best cold-start handling, combine ALS/BPR with popularity-based fallback</li> <li>Lower <code>cg_iters</code> (e.g., 1\u20132) for faster but noisier convergence on huge ALS datasets</li> </ul>"},{"location":"cookbook/#9-sequential-pattern-mining-prefixspan","title":"9. Sequential Pattern Mining (PrefixSpan)","text":"<p>When the order of events matters (e.g., website navigation paths, sequential purchases over time), use PrefixSpan instead of FP-Growth.</p>"},{"location":"cookbook/#prepare-the-event-log","title":"Prepare the Event Log","text":"<pre><code>from rusket import prefixspan, sequences_from_event_log\n\nevents = pd.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 2, 3, 3, 3],\n    \"timestamp\": [\n        \"2024-01-01 10:00\", \"2024-01-01 10:05\", \"2024-01-01 10:10\",\n        \"2024-01-02 11:00\", \"2024-01-02 11:05\",\n        \"2024-01-03 09:00\", \"2024-01-03 09:05\", \"2024-01-03 09:10\"\n    ],\n    \"page_id\": [\"home\", \"products\", \"checkout\", \"home\", \"products\", \"home\", \"products\", \"checkout\"]\n})\n\n# Convert timestamp to datetime for correct sorting\nevents[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"])\n\n# Convert to sequence format required by prefixspan\nsequences, idx_to_item = sequences_from_event_log(\n    events, \n    user_col=\"user_id\", \n    time_col=\"timestamp\", \n    item_col=\"page_id\"\n)\n</code></pre>"},{"location":"cookbook/#mine-sequential-patterns","title":"Mine Sequential Patterns","text":"<pre><code># min_support is an absolute number of sequences\npatterns = prefixspan(sequences, min_support=2, max_len=3)\n\n# Map integer IDs back to original page names\npatterns[\"sequence_names\"] = patterns[\"sequence\"].apply(\n    lambda seq: [idx_to_item[idx] for idx in seq]\n)\n\nprint(patterns[[\"support\", \"sequence_names\"]])\n#    support               sequence_names\n# 0        3                       [home]\n# 1        3                   [products]\n# 2        3             [home, products]\n# 3        2                   [checkout]\n# 4        2             [home, checkout]\n# 5        2         [products, checkout]\n# 6        2   [home, products, checkout]\n</code></pre>"},{"location":"cookbook/#10-high-utility-pattern-mining-hupm","title":"10. High-Utility Pattern Mining (HUPM)","text":"<p>Standard Frequent Itemset Mining (FP-Growth/Eclat) treats all items equally. HUPM considers the profit or utility of items, discovering itemsets that generate high total revenue even if they are bought infrequently.</p> <pre><code>from rusket import hupm\n\n# Item IDs bought\ntransactions = [\n    [1, 2, 3], \n    [1, 3],    \n    [2, 3]     \n]\n\n# Profit (or quantity * price) of each item in the respective transaction\nutilities = [\n    [5.0, 10.0, 2.0], # Transaction 1 profits\n    [5.0, 2.0],       # Transaction 2 profits\n    [10.0, 2.0]       # Transaction 3 profits\n]\n\n# Mine itemsets with at least 15.0 total utility\nhigh_utility_itemsets = hupm(transactions, utilities, min_utility=15.0, max_len=3)\nprint(high_utility_itemsets)\n#    utility    itemset\n# 0     20.0       [2]\n# 1     24.0    [2, 3]\n# 2     17.0 [1, 2, 3]\n</code></pre>"},{"location":"cookbook/#11-bayesian-personalized-ranking-bpr","title":"11. Bayesian Personalized Ranking (BPR)","text":"<p>BPR is a matrix factorization model that optimizes for ranking metrics rather than reconstruction error (like ALS). It works by sampling positive (interacted) and negative (unseen) items and ensuring the positive items are ranked higher. Use it when interaction data is purely binary implicit feedback.</p> <pre><code>from rusket import BPR\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\n# Create an implicit feedback matrix (users x items)\nmat = csr_matrix((np.ones(10), ([0,0,0,1,1,2,2,3,3,4], [1,3,4,1,2,2,3,1,4,4])), shape=(5, 5))\n\nmodel = BPR(\n    factors=32,\n    learning_rate=0.05,\n    iterations=200,\n    regularization=0.01,\n    seed=42\n)\n\n# Fit the BPR model\nmodel.fit(mat)\n\n# Recommend 3 items for user 0, excluding items they already interacted with\nitem_ids, scores = model.recommend_items(user_id=0, n=3, exclude_seen=True)\nprint(item_ids, scores)\n</code></pre>"},{"location":"cookbook/#12-item-similarity-and-cross-selling-potential","title":"12. Item Similarity and Cross-Selling Potential","text":"<p>Once you have fitted an ALS or BPR model, the learned latent factors are incredibly useful for measuring item similarity and predicting missed cross-sell opportunities.</p>"},{"location":"cookbook/#find-similar-products-item-to-item","title":"Find Similar Products (Item-to-Item)","text":"<pre><code>from rusket import similar_items\n\n# Given an ALS model fitted on purchases\nitem_ids, match_scores = similar_items(model, item_id=102, n=5)\n\nprint(f\"Items similar to {102}: {item_ids}\")\n# =&gt; Items similar to 102: [105, 99, 110, 87, 10]\n</code></pre>"},{"location":"cookbook/#calculate-cross-selling-potential","title":"Calculate Cross-Selling Potential","text":"<p>Identify the probability that a user should have bought an item by now, but hasn't (<code>score_potential</code>).</p> <pre><code>from rusket import score_potential\n\nuser_purchase_history = [\n    [0, 1, 5], # User 0 bought items 0, 1, 5\n    [1, 3],    # User 1 bought items 1, 3\n    [0]        # User 2 bought item 0\n]\n\n# Provide specific categories/items you want to cross-sell\ntarget_items = [2, 4, 6]\n\n# Matrix of shape (n_users, len(target_items))\nscores = score_potential(\n    user_purchase_history, \n    als_model=model, \n    target_categories=target_items\n)\n\n# The highest scores correspond to the users most primed to buy those specific targets\nprint(\"Cross-sell potential scores:\")\nprint(scores)\n</code></pre>"},{"location":"cookbook/#13-hybrid-recommender-als-association-rules","title":"13. Hybrid Recommender (ALS + Association Rules)","text":"<p>The <code>Recommender</code> workflow class wraps both your collaborative filtering models (ALS) and Frequent Pattern Mining rules into a single API. This easily enables the two most common placement strategies in e-commerce: \"For You\" (ALS) and \"Frequently Bought Together\" (Association Rules).</p> <pre><code>from rusket import Recommender\n\n# Initialize with both your fitted ALS model and Rules DataFrame\nrec = Recommender(als_model=model, rules_df=strong_rules)\n\n# 1. \"For You\" (Personalized cross-selling based on user history)\nitem_ids, scores = rec.recommend_for_user(user_id=125, n=5)\n\n# 2. \"Frequently Bought Together\" (Cart-based additions)\nactive_cart = [10, 15] # User just added items 10 and 15\nsuggested_additions = rec.recommend_for_cart(active_cart, n=3)\n</code></pre>"},{"location":"cookbook/#14-genai-llm-stack-integration","title":"14. GenAI / LLM Stack Integration","text":"<p><code>rusket</code> provides native utilities to export its learned representations and rules into the modern Generative AI and graph analytics stack.</p>"},{"location":"cookbook/#vector-export-vector-databases-lancedb","title":"Vector Export &amp; Vector Databases (LanceDB)","text":"<p>You can easily export ALS latent user or item factors as vector embeddings to power RAG (Retrieval-Augmented Generation) or fast semantic similarity search in vector databases like LanceDB, FAISS, or Qdrant.</p> <pre><code>import lancedb\nfrom rusket import export_item_factors\n\n# Export ALS item factors to a Pandas DataFrame\n# Returns columns: ['item_id', 'vector'] (and 'item_label' if available)\ndf_vectors = export_item_factors(als_model)\n\n# Connect to a local LanceDB instance\ndb = lancedb.connect(\"./lancedb\")\n\n# Ingest the embeddings into a table\ntable = db.create_table(\"item_embeddings\", data=df_vectors, mode=\"overwrite\")\n\n# Perform a vector similarity search (e.g., finding items similar to a given query embedding)\nquery_vector = df_vectors.iloc[0][\"vector\"]\nresults = table.search(query_vector).limit(5).to_pandas()\nprint(results)\n</code></pre>"},{"location":"cookbook/#fast-item-to-item-similarity","title":"Fast Item-to-Item Similarity","text":"<p>If you don't need a full vector database and just want fast, in-memory cosine similarity between items based on their ALS embeddings:</p> <pre><code>from rusket import similar_items\n\n# Find the top 5 most similar items to item_id=42 using Cosine Similarity\nsimilar_ids, similarity_scores = similar_items(als_model, item_id=42, n=5)\n\nprint(f\"Similar items: {similar_ids}\")\nprint(f\"Cosine similarities: {similarity_scores}\")\n</code></pre>"},{"location":"cookbook/#graph-generation-for-community-detection","title":"Graph Generation for Community Detection","text":"<p>Frequent Pattern Mining rules can be naturally represented as a directed graph. You can automatically convert them into a <code>networkx</code> graph to run community detection (like Louvain) and discover \"Product Clusters\" or \"Categories\".</p> <pre><code>import networkx as nx\nfrom rusket.viz import to_networkx\n\n# rules is a Pandas DataFrame from rusket.association_rules()\n# We use 'lift' as the edge weight connecting antecedents to consequents\nG = to_networkx(rules_df, source_col=\"antecedents\", target_col=\"consequents\", edge_attr=\"lift\")\n\n# Run basic graph analytics\nprint(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n\n# E.g., calculate PageRank to find the most influential products\ncentrality = nx.pagerank(G, weight='weight')\ntop_items = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]\nprint(\"Top central products:\", top_items)\n</code></pre>"},{"location":"cookbook/#15-visualizing-latent-spaces-pca","title":"15. Visualizing Latent Spaces (PCA)","text":"<p>When using <code>ALS</code>, raw embeddings capture both magnitude (how frequently an item is bought) and direction (the \"taste\" or behavioral profile of who buys it).</p> <p>To map these multidimensional factors down to a 3D Plotly visualization for dashboarding, we apply L2 Normalization (to focus solely on Cosine Similarity / direction) followed by PCA (Principal Component Analysis).</p> <p>Because <code>rusket</code> exposes ALS factors directly as NumPy arrays, you can do this without adding dependencies like <code>scikit-learn</code> or PySpark:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom rusket import ALS\n\n# 1. Load the Online Retail dataset\nurl = \"https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/refs/heads/master/data/retail-data/all/online-retail-dataset.csv\"\ndf_purchases = pd.read_csv(url)\ndf_purchases = df_purchases.dropna(subset=[\"CustomerID\", \"Description\"])\n\n# 2. Fit an ALS model\nmodel = ALS(factors=64, iterations=15, alpha=40.0, seed=42)\nmodel.fit_transactions(df_purchases, user_col=\"CustomerID\", item_col=\"StockCode\")\n\n# 3. L2 Normalization (Unit Sphere Projection) using pure NumPy\n# Divide each latent factor row by its L2 norm (magnitude)\nitem_factors = model.item_factors\nitem_norms = np.linalg.norm(item_factors, axis=1, keepdims=True)\nitem_factors_norm = item_factors / np.clip(item_norms, a_min=1e-10, a_max=None)\n\n# 4. PCA Reduction (e.g. 64D -&gt; 3D) using Singular Value Decomposition\ndef compute_pca_3d(data):\n    # Mean centering\n    data_centered = data - np.mean(data, axis=0)\n\n    # SVD\n    U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)\n\n    # Extract the top 3 principal components map\n    components = Vt[:3]\n    return np.dot(data_centered, components.T)\n\nitem_pca = compute_pca_3d(item_factors_norm)\n\n# 5. Bind arrays back to a Pandas DataFrame for Plotly\ndf_viz = pd.DataFrame({\n    \"StockCode\": model._item_labels, # The original dataset IDs mapped back\n    \"pca_1\": item_pca[:, 0],\n    \"pca_2\": item_pca[:, 1],\n    \"pca_3\": item_pca[:, 2]\n})\n\n# Merge descriptions back in for hover labels\ndf_items = df_purchases[[\"StockCode\", \"Description\"]].drop_duplicates(\"StockCode\")\ndf_viz = df_viz.merge(df_items, on=\"StockCode\", how=\"inner\")\n\nfig = px.scatter_3d(\n    df_viz, x=\"pca_1\", y=\"pca_2\", z=\"pca_3\",\n    hover_name=\"Description\",\n    title=\"ALS Latent Space (3D PCA Mapping)\"\n)\nfig.update_traces(marker=dict(size=3, opacity=0.7))\nfig.show()\n</code></pre> <p>This workflow matches Spark MLlib's dimensionality reduction pipelines seamlessly while executing locally in microseconds.</p>"},{"location":"cookbook/#16-translating-spark-mllib-to-rusket","title":"16. Translating Spark MLlib to <code>rusket</code>","text":"<p>For users migrating from Databricks or PySpark, <code>rusket</code> offers a highly similar API without the distributed computing overhead. </p> <p>This example translates the famous Recommendation example from Chapter 28 of Spark: The Definitive Guide directly into <code>rusket</code> using pure Python and Pandas.</p>"},{"location":"cookbook/#spark-version-original","title":"Spark Version (Original)","text":"<pre><code>from pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nratings = spark.read.text(\"/data/sample_movielens_ratings.txt\") \\\n  .selectExpr(\"split(value, '::') as col\") \\\n  .selectExpr(\n      \"cast(col[0] as int) as userId\",\n      \"cast(col[1] as int) as movieId\",\n      \"cast(col[2] as float) as rating\"\n  )\n\ntraining, test = ratings.randomSplit([0.8, 0.2])\n\nals = ALS().setMaxIter(5).setRegParam(0.01) \\\n  .setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\n\nalsModel = als.fit(training)\npredictions = alsModel.transform(test)\n\nevaluator = RegressionEvaluator().setMetricName(\"rmse\") \\\n  .setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n\nprint(\"RMSE =\", evaluator.evaluate(predictions))\n</code></pre>"},{"location":"cookbook/#rusket-version-equivalent","title":"<code>rusket</code> Version (Equivalent)","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom rusket import ALS\n\n# 1. Load the data using Pandas\nurl = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt\"\nratings = pd.read_csv(url, sep=\"::\", engine=\"python\", \n                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n\n# 2. Random Split (80/20)\nshuffled = ratings.sample(frac=1.0, random_state=42)\nsplit_idx = int(len(shuffled) * 0.8)\ntraining = shuffled.iloc[:split_idx]\ntest = shuffled.iloc[split_idx:]\n\n# 3. Initialize and Fit the ALS Model\n# Note: rusket uses `factors` instead of `rank`, and `iterations` instead of `maxIter`.\nmodel = ALS(factors=10, iterations=5, regularization=0.01, seed=42)\nmodel.fit_transactions(training, user_col=\"userId\", item_col=\"movieId\", rating_col=\"rating\")\n\n# 4. Generate Predictions for the test set\n# rusket has a built-in vectorized score_potential helper for evaluating target vectors\nfrom rusket.recommend import score_potential\n\n# We reconstruct the user's history from the training set to mask known interactions\nuser_histories = training.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n# Ensure all users in the test set exist in our history mapping, even if empty\nhistory_list = [user_histories.get(uid, []) for uid in range(model._n_users)]\n\n# Calculate raw prediction scores across all users and all items\nall_predictions = score_potential(history_list, model)\n\n# 5. Evaluate RMSE\n# Extract only the actual ratings we care about from the test set\ntest_users = test[\"userId\"].values\ntest_movies = test[\"movieId\"].values\nactual_ratings = test[\"rating\"].values\n\n# Map the raw pandas IDs to rusket's internal 0-indexed matrix IDs\ntry:\n    internal_user_ids = np.array([model._user_labels.index(u) for u in test_users])\n    internal_movie_ids = np.array([model._item_labels.index(str(m)) for m in test_movies])\n\n    # Extract predicted ratings\n    predicted_ratings = all_predictions[internal_user_ids, internal_movie_ids]\n\n    # Calculate RMSE\n    valid_mask = ~np.isinf(predicted_ratings) &amp; ~np.isnan(predicted_ratings)\n    rmse = np.sqrt(np.mean((predicted_ratings[valid_mask] - actual_ratings[valid_mask]) ** 2))\n    print(f\"Root-mean-square error = {rmse:.4f}\")\n\nexcept ValueError as e:\n    # Handle cold-start users/items in the test set not seen in training\n    print(\"Cold start warning: Some users/items in test set were not in training.\")\n</code></pre>"},{"location":"cookbook/#explanation-of-key-differences","title":"Explanation of Key Differences","text":"<ol> <li>No Distributed Execution: PySpark builds physical query plans (<code>.transform()</code>, <code>.show()</code>). <code>rusket</code> executes completely eagerly in memory, heavily relying on Rust arrays and <code>numpy</code> for C-level vector operations.</li> <li>Cold Starts: <code>rusket</code> is designed for implicit feedback recommendations, and its <code>transform</code>/prediction step expects <code>user_id</code> and <code>item_id</code> values to have been seen during <code>.fit()</code>. Proper production code should handle cold starts with popularity backups instead of omitting them.</li> <li>Implicit vs Explicit Feedback: The Databricks exact example uses ALS for explicit ratings out of 5 stars to calculate Regression Error (RMSE). <code>rusket</code> focuses entirely on implicit feedback (clicks, purchases), so it's optimized for calculating Ranking Metrics (like Precision@K) rather than Regression Error. The math works identically, but it scales differently.</li> </ol>"},{"location":"migration/","title":"Migration from mlxtend","text":"<p>rusket is designed as a drop-in replacement for <code>mlxtend.frequent_patterns</code>. In the vast majority of cases the only change you need is the import line.</p>"},{"location":"migration/#import-change","title":"Import change","text":"Before (mlxtend)After (rusket) <pre><code>from mlxtend.frequent_patterns import fpgrowth, association_rules\n</code></pre> <pre><code>from rusket import fpgrowth, association_rules\n</code></pre>"},{"location":"migration/#api-comparison","title":"API comparison","text":""},{"location":"migration/#fpgrowth","title":"<code>fpgrowth</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame \\| pl.DataFrame</code> rusket also accepts Polars <code>min_support</code> <code>float</code> <code>float</code> identical <code>use_colnames</code> <code>bool</code> <code>bool</code> identical <code>max_len</code> <code>int\\|None</code> <code>int\\|None</code> identical <code>verbose</code> <code>int</code> <code>int</code> accepted but unused <code>null_values</code> <code>bool</code> <code>bool</code> pandas only"},{"location":"migration/#association_rules","title":"<code>association_rules</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame</code> output of <code>fpgrowth</code> <code>num_itemsets</code> <code>int</code> <code>int</code> identical <code>metric</code> <code>str</code> <code>str</code> identical (12 metrics) <code>min_threshold</code> <code>float</code> <code>float</code> identical <code>support_only</code> <code>bool</code> <code>bool</code> identical <code>return_metrics</code> <code>list[str]</code> <code>list[str]</code> identical"},{"location":"migration/#return-value","title":"Return value","text":"<p>Both functions return identical DataFrame structures:</p> <ul> <li><code>fpgrowth</code> \u2192 <code>pd.DataFrame</code> with <code>['support', 'itemsets']</code></li> <li><code>association_rules</code> \u2192 <code>pd.DataFrame</code> with <code>['antecedents', 'consequents', ...metrics]</code></li> </ul> <p>Itemsets are <code>frozenset</code> objects, exactly as in mlxtend.</p>"},{"location":"migration/#whats-different","title":"What's different?","text":"<p>Behaviour differences</p> <ul> <li>Performance: rusket is significantly faster on medium/large datasets and uses far less memory.</li> <li>Polars input: rusket accepts <code>polars.DataFrame</code> natively; mlxtend does not.</li> <li>Sparse DataFrames: rusket uses the CSR path, which is more memory-efficient than mlxtend for sparse data.</li> <li><code>null_values</code> / Rust path: When <code>null_values=True</code>, rusket currently falls back gracefully (no error), but the Rust path is not yet used for null-containing DataFrames.</li> </ul>"},{"location":"migration/#uninstalling-mlxtend","title":"Uninstalling mlxtend","text":"<p>Once you have validated that rusket produces the same results:</p> <pre><code>pip uninstall mlxtend\n</code></pre> <p>rusket has no runtime dependency on mlxtend.</p>"},{"location":"polars/","title":"Polars Support","text":"<p>rusket accepts <code>polars.DataFrame</code> natively alongside pandas, via the Arrow-backed zero-copy path.</p>"},{"location":"polars/#installation","title":"Installation","text":"<p>Install rusket with the Polars extra:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>This pins <code>polars&gt;=0.20</code>. If you already have Polars installed, you can also just <code>pip install rusket</code>.</p>"},{"location":"polars/#usage","title":"Usage","text":"<p>The <code>fpgrowth</code> function detects Polars DataFrames automatically \u2014 no extra parameters needed:</p> <pre><code>import polars as pl\nfrom rusket import fpgrowth, association_rules\n\n# Build a Polars one-hot DataFrame\ndf = pl.DataFrame({\n    \"milk\":  [True, True,  False, True],\n    \"bread\": [True, False, True,  True],\n    \"eggs\":  [False, True, True,  True],\n})\n\n# Mine frequent itemsets \u2014 identical call as with pandas\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nprint(freq)\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# ...\n\n# association_rules always returns a pandas DataFrame\nrules = association_rules(freq, num_itemsets=len(df), metric=\"lift\", min_threshold=1.0)\nprint(rules)\n</code></pre> <p>Return type</p> <p><code>fpgrowth</code> always returns a pandas DataFrame, regardless of input type. <code>association_rules</code> also returns a pandas DataFrame.</p>"},{"location":"polars/#how-it-works","title":"How it works","text":"<p>The Polars path uses <code>polars.DataFrame.to_numpy()</code> which returns an Arrow-backed NumPy buffer \u2014 zero-copy for numeric dtypes.</p> <pre><code>Polars DataFrame\n    \u2502\n    \u25bc  df.to_numpy()  (zero-copy for bool/int dtypes)\nnumpy uint8 array\n    \u2502\n    \u25bc  fpgrowth_from_dense()  (Rust, PyO3 ReadonlyArray2&lt;u8&gt;)\nRust FP-Tree mining\n    \u2502\n    \u25bc\npandas DataFrame  [support, itemsets]\n</code></pre> <p>No intermediate Python object creation occurs between the Polars input and the Rust mining step.</p>"},{"location":"polars/#supported-dtypes","title":"Supported dtypes","text":"Polars dtype Supported <code>Boolean</code> \u2705 <code>Int8 / Int16 / Int32 / Int64</code> \u2705 (0/1 values) <code>UInt8 / UInt16 / UInt32 / UInt64</code> \u2705 (0/1 values) <code>Float32 / Float64</code> \u26a0\ufe0f (0.0/1.0 values, cast to uint8) Categorical / String \u274c (pre-encode with <code>get_dummies</code>) <p>Lazy frames</p> <p>Pass <code>.collect()</code> before calling <code>fpgrowth</code> if you have a <code>LazyFrame</code>: <pre><code>freq = fpgrowth(lazy_df.collect(), min_support=0.3, use_colnames=True)\n</code></pre></p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#installation","title":"Installation","text":"pipuvconda <pre><code>pip install rusket\n</code></pre> <pre><code>uv add rusket\n</code></pre> <pre><code>pip install rusket  # rusket is not on conda-forge yet\n</code></pre> <p>To also enable Polars support:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>Coming from mlxtend?</p> <p>rusket is a drop-in replacement. In most cases you only need to change your import: <pre><code># Before\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n# After\nfrom rusket import mine, association_rules\n</code></pre> See the full Migration Guide for details.</p>"},{"location":"quickstart/#step-1-prepare-your-data","title":"Step 1 \u2014 Prepare your data","text":"<p><code>mine</code> expects a one-hot encoded DataFrame of boolean or 0/1 integer values where rows are transactions and columns are items.</p> <pre><code>import pandas as pd\n\ndataset = [\n    [\"milk\", \"bread\"],\n    [\"milk\", \"eggs\"],\n    [\"bread\", \"eggs\"],\n    [\"milk\", \"bread\", \"eggs\"],\n]\n\n# Build a one-hot DataFrame\nitems = [\"milk\", \"bread\", \"eggs\"]\ndf = pd.DataFrame(\n    [[item in tx for item in items] for tx in dataset],\n    columns=items,\n    dtype=bool,\n)\nprint(df)\n#    milk  bread   eggs\n# 0  True   True  False\n# 1  True  False   True\n# 2 False   True   True\n# 3  True   True   True\n</code></pre>"},{"location":"quickstart/#step-2-mine-frequent-itemsets","title":"Step 2 \u2014 Mine frequent itemsets","text":"<pre><code>from rusket import mine\n\n# method=\"auto\" automatically selects FP-Growth or Eclat based on dataset density\nfreq = mine(df, min_support=0.5, use_colnames=True)\nprint(freq)\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# 2     0.75          (eggs,)\n# 3     0.50   (milk, bread,)\n# 4     0.50    (milk, eggs,)\n# 5     0.50   (bread, eggs,)\n# 6     0.25  (milk, bread, eggs,)\n</code></pre>"},{"location":"quickstart/#step-2b-or-use-eclat","title":"Step 2b \u2014 Or use Eclat","text":"<p><code>eclat</code> uses vertical bitset mining \u2014 same API, same results. It can be faster on certain data shapes.</p> <pre><code>from rusket import eclat\n\nfreq = eclat(df, min_support=0.5, use_colnames=True)\nprint(freq)  # identical output to fpgrowth\n</code></pre> <p>When to use which?</p> <p>The <code>mine(method=\"auto\")</code> parameter handles this automatically for you.  It evaluates the density of the dataset <code>nnz / (rows * cols)</code> and picks <code>eclat</code> for sparse datasets <code>&lt; 0.15</code> and <code>fpgrowth</code> for dense datasets.</p>"},{"location":"quickstart/#step-3-generate-association-rules","title":"Step 3 \u2014 Generate association rules","text":"<pre><code>from rusket import association_rules\n\nrules = association_rules(\n    freq,\n    num_itemsets=len(df),\n    metric=\"confidence\",\n    min_threshold=0.6,\n)\nprint(rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n</code></pre> <p>num_itemsets</p> <p>Pass the total transaction count (<code>len(df)</code>) so that support-based metrics are computed correctly.</p>"},{"location":"quickstart/#billion-scale-streaming","title":"Billion-Scale Streaming","text":"<p>For datasets with hundreds of millions or billions of rows, use <code>FPMiner</code> to stream data into Rust one chunk at a time \u2014 without ever holding the full dataset in memory.</p> <pre><code>from rusket import FPMiner\n\nminer = FPMiner(n_items=500_000)  # total distinct items\n\n# Feed chunks \u2014 e.g. from a Parquet file, Spark, or a database cursor\nfor chunk in pd.read_parquet(\"orders.parquet\", chunksize=10_000_000):\n    txn = chunk[\"txn_id\"].to_numpy(dtype=\"int64\")   # arbitrary int IDs\n    item = chunk[\"item_idx\"].to_numpy(dtype=\"int32\") # 0-based item index\n    miner.add_chunk(txn, item)\n\n# Mine \u2014 all data in Rust, output is a normal pandas DataFrame\nfreq = miner.mine(min_support=0.001, max_len=3, use_colnames=False)\nrules = association_rules(freq, num_itemsets=miner.n_transactions)\n</code></pre> <p>Peak memory</p> <p>Peak Python memory = one chunk (typically 1\u20132 GB). Rust holds the per-transaction item lists (~5 GB for 200M transactions). The final mining step passes CSR arrays directly \u2014 zero copies.</p>"},{"location":"quickstart/#direct-csr-path","title":"Direct CSR path","text":"<p>If you already have integer arrays (e.g. from a database query), skip <code>from_transactions</code> entirely:</p> <pre><code>from scipy import sparse as sp\nfrom rusket import mine\n\ncsr = sp.csr_matrix(\n    (np.ones(len(txn_ids), dtype=np.int8), (txn_ids, item_ids)),\n    shape=(n_transactions, n_items),\n)\nfreq = mine(csr, min_support=0.001, column_names=item_names)\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Migration from mlxtend \u2014 side-by-side comparison</li> <li>API Reference \u2014 all parameters and metrics explained</li> <li>Polars Support \u2014 zero-copy Arrow path</li> <li>Benchmarks \u2014 performance vs mlxtend</li> </ul>"},{"location":"recommender/","title":"Recommender Workflows","text":"<p>While <code>rusket</code> provides blazing-fast core algorithms like Alternating Least Squares (ALS) and FP-Growth, raw algorithms often require heavy lifting to translate into business outcomes. </p> <p>To bridge this gap, <code>rusket</code> includes a high-level Business Recommender API designed for e-commerce, content platforms, and marketing analytics.</p>"},{"location":"recommender/#the-hybrid-recommender","title":"The Hybrid Recommender","text":"<p>The <code>Recommender</code> class blends the serendipity of Collaborative Filtering (ALS) with the strict deterministic logic of Frequent Pattern Mining (Association Rules) to provide the \"Next Best Action\" for any context.</p> <pre><code>import pandas as pd\nfrom rusket import ALS, Recommender, mine, association_rules\n\n# 1. Train your Collaborative Filtering model\nals = ALS(factors=64, iterations=15).fit(user_item_sparse_matrix)\n\n# 2. Mine your Association Rules\nfreq = mine(user_item_sparse_matrix, min_support=0.01)\nrules = association_rules(freq, num_itemsets=user_item_sparse_matrix.shape[0])\n\n# 3. Create the Hybrid Engine\nrec = Recommender(als_model=als, rules_df=rules)\n</code></pre>"},{"location":"recommender/#1-personalized-recommendations","title":"1. Personalized Recommendations","text":"<p>Use <code>recommend_for_user</code> to generate a customized list of products for a returning customer based on their latent profile (ALS).</p> <pre><code># Get top 5 product recommendations for user 42\nitems, scores = rec.recommend_for_user(user_id=42, n=5)\n\nprint(f\"Recommended Items: {items}\")\n# Recommended Items: [34, 12, 89, 7, 102]\n</code></pre>"},{"location":"recommender/#2-cart-based-cross-selling","title":"2. Cart-based Cross-Selling","text":"<p>When a user adds items to their shopping cart, you want to show a \"Frequently bought together\" carousel. The <code>recommend_for_cart</code> method uses deterministic association rules to find the highest lift recommendations for the contents of the cart.</p> <pre><code># User has items 14 and 7 in their cart\nsuggested = rec.recommend_for_cart([14, 7], n=3)\n\nprint(f\"Others also bought: {suggested}\")\n# Others also bought: [8, 45, 99] \n</code></pre>"},{"location":"recommender/#item-to-item-similarity-i2i","title":"Item-to-Item Similarity (i2i)","text":"<p>Often you don't have a user context (e.g., an anonymous visitor on a product description page). In these cases, you can use the latent item factors learned by the ALS model to find conceptually similar products.</p> <p>The <code>similar_items</code> function performs ultra-fast Cosine Similarity over the ALS item embeddings.</p> <pre><code>from rusket import similar_items\nfrom rusket import ALS\n\n# Fit ALS on your interaction data\nals = ALS(factors=128).fit(interactions)\n\n# Find 4 products most similar to product 99\nsimilar, similarities = similar_items(als, item_id=99, n=4)\n\nprint(similar) \n# [100, 95, 102, 88]\nprint(similarities)\n# [0.98, 0.94, 0.89, 0.85]\n</code></pre> <p>Note: Because this operates on latent factors, it discovers implicit relationships. For example, it might identify that a high-end coffee grinder is similar to an expensive espresso machine, even if they aren't directly bought in the exact same cart.</p>"},{"location":"recommender/#cross-selling-potential-scoring","title":"Cross-Selling Potential Scoring","text":"<p>For marketing teams, sending generic email blasts is inefficient. The <code>score_potential</code> function quantifies the \"missed opportunity\" by calculating the probability a user should have bought an item by now, but hasn't.</p> <p>This is perfect for building highly targeted audiences for email campaigns or retargeting pixels.</p> <pre><code>from rusket import score_potential\n\n# user_history: list of item IDs each user has already interacted with\n# target_categories: optional list of specific item/category IDs to score\n\npotential_matrix = score_potential(\n    user_history=[[14, 7], [99], [5, 6, 7]], \n    als_model=als,\n    target_categories=[101, 102, 103] # e.g., the \"Electronics\" category\n)\n\n# The result is a dense numpy array of shape (n_users, len(target_categories))\n# Values are raw ALS interaction scores. Items the user has already bought \n# are masked with -infinity.\n</code></pre> <p>By sorting users by their maximum score in <code>potential_matrix</code>, you can instantly generate a ranked list of the absolute best customers to target for an upcoming product launch in the <code>Electronics</code> category.</p>"}]}