{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#rusket","title":"rusket","text":"<p>Blazing-fast FP-Growth and Association Rules for Python \u2014 pure Rust via PyO3.</p> <p> </p>"},{"location":"#what-is-rusket","title":"What is rusket?","text":"<p><code>rusket</code> is a drop-in replacement for <code>mlxtend.frequent_patterns.fpgrowth</code> and <code>mlxtend.frequent_patterns.association_rules</code> \u2014 identical API, significantly faster, dramatically lower memory footprint.</p> <p>The core algorithm is implemented in Rust via PyO3 and maturin, with three optimised dispatch paths exposed to Python:</p> Input Rust path Notes Dense pandas DataFrame <code>fpgrowth_from_dense</code> Flat <code>uint8</code> buffer \u2014 zero-copy Sparse pandas DataFrame <code>fpgrowth_from_csr</code> Raw CSR arrays \u2014 zero-copy Polars DataFrame <code>fpgrowth_from_dense</code> Arrow-backed <code>numpy</code> buffer"},{"location":"#why-rusket","title":"Why rusket?","text":"Feature rusket mlxtend Speed (medium dataset) ~0.4 s ~4 s Memory (large dataset) ~3 s OOM Polars support \u2705 \u274c Sparse DataFrame support \u2705 \u26a0\ufe0f limited Zero Python dependencies \u2705 (<code>numpy</code>, <code>pandas</code>) \u274c (many) 12 association metrics \u2705 \u2705"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom rusket import fpgrowth, association_rules\n\ndf = pd.DataFrame({\n    \"milk\":  [1, 1, 0, 1],\n    \"bread\": [1, 0, 1, 1],\n    \"eggs\":  [0, 1, 1, 1],\n})\n\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nrules = association_rules(freq, num_itemsets=len(df), metric=\"confidence\", min_threshold=0.6)\nprint(rules[[\"antecedents\", \"consequents\", \"confidence\"]])\n</code></pre> <p>Get Started API Reference View on GitHub</p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#fpgrowth","title":"<code>fpgrowth</code>","text":"<pre><code>from rusket import fpgrowth\n\nfpgrowth(\n    df,\n    min_support=0.5,\n    null_values=False,\n    use_colnames=False,\n    max_len=None,\n    verbose=0,\n) -&gt; pd.DataFrame\n</code></pre> <p>Find frequent itemsets in a one-hot encoded transaction DataFrame using the FP-Growth algorithm, implemented in Rust for maximum performance.</p>"},{"location":"api-reference/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>df</code> <code>pd.DataFrame \\| pl.DataFrame</code> \u2014 One-hot encoded DataFrame. Rows are transactions, columns are items. Accepts bool or 0/1 integer values. Sparse pandas DataFrames are supported via the CSR path. <code>min_support</code> <code>float</code> <code>0.5</code> Minimum support threshold in <code>(0, 1]</code>. Items occurring in fewer than <code>ceil(min_support \u00d7 n_rows)</code> transactions are excluded. <code>null_values</code> <code>bool</code> <code>False</code> Allow NaN values in <code>df</code> (pandas only). When <code>True</code>, NaNs are treated as zeros. <code>use_colnames</code> <code>bool</code> <code>False</code> If <code>True</code>, itemsets contain column names instead of integer column indices. <code>max_len</code> <code>int \\| None</code> <code>None</code> Maximum itemset length. <code>None</code> means unlimited. <code>verbose</code> <code>int</code> <code>0</code> Verbosity level. Currently unused; kept for API compatibility with mlxtend."},{"location":"api-reference/#returns","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns:</p> Column Type Description <code>support</code> <code>float</code> Support of the itemset (fraction of transactions). <code>itemsets</code> <code>frozenset</code> Set of column indices (or names when <code>use_colnames=True</code>)."},{"location":"api-reference/#raises","title":"Raises","text":"Exception Condition <code>ValueError</code> <code>min_support</code> \u2264 0 <code>TypeError</code> <code>df</code> is not a pandas or Polars DataFrame"},{"location":"api-reference/#examples","title":"Examples","text":"Dense pandasSparse pandasPolars <pre><code>import pandas as pd\nfrom rusket import fpgrowth\n\ndf = pd.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\n</code></pre> <pre><code>import pandas as pd\nfrom pandas.arrays import SparseArray\nfrom rusket import fpgrowth\n\ndf = pd.DataFrame.sparse.from_spmatrix(my_csr_matrix, columns=items)\nfreq = fpgrowth(df, min_support=0.1, use_colnames=True)\n</code></pre> <pre><code>import polars as pl\nfrom rusket import fpgrowth\n\ndf = pl.DataFrame({\"a\": [1,1,0], \"b\": [1,0,1], \"c\": [0,1,1]})\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\n</code></pre>"},{"location":"api-reference/#association_rules","title":"<code>association_rules</code>","text":"<pre><code>from rusket import association_rules\n\nassociation_rules(\n    df,\n    num_itemsets,\n    df_orig=None,\n    null_values=False,\n    metric=\"confidence\",\n    min_threshold=0.8,\n    support_only=False,\n    return_metrics=ALL_METRICS,\n) -&gt; pd.DataFrame\n</code></pre> <p>Generate association rules from a DataFrame of frequent itemsets. The rule-generation and metric computation is performed in Rust.</p>"},{"location":"api-reference/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> \u2014 Output of <code>fpgrowth()</code> with columns <code>['support', 'itemsets']</code>. <code>num_itemsets</code> <code>int</code> \u2014 Total number of transactions in the original dataset (= <code>len(original_df)</code>). <code>df_orig</code> <code>pd.DataFrame \\| None</code> <code>None</code> Original (non-binarised) DataFrame. Only needed when <code>null_values=True</code>. <code>null_values</code> <code>bool</code> <code>False</code> Apply null-value correction (not yet on the Rust path; falls back gracefully). <code>metric</code> <code>str</code> <code>\"confidence\"</code> Primary filter metric. See Metrics below. <code>min_threshold</code> <code>float</code> <code>0.8</code> Minimum value of <code>metric</code> for a rule to be included in the result. <code>support_only</code> <code>bool</code> <code>False</code> If <code>True</code>, only compute support; fill all other metrics with <code>NaN</code>. <code>return_metrics</code> <code>list[str]</code> all 12 Metric columns to include in the result DataFrame."},{"location":"api-reference/#returns_1","title":"Returns","text":"<p><code>pandas.DataFrame</code> with columns:</p> Column Type Description <code>antecedents</code> <code>frozenset</code> Left-hand side (LHS) of the rule. <code>consequents</code> <code>frozenset</code> Right-hand side (RHS) of the rule. <code>antecedent support</code> <code>float</code> Support of the antecedent alone. <code>consequent support</code> <code>float</code> Support of the consequent alone. <code>support</code> <code>float</code> Support of the full rule (LHS \u222a RHS). <code>confidence</code> <code>float</code> P(RHS | LHS). <code>lift</code> <code>float</code> Confidence / consequent support. <code>representativity</code> <code>float</code> Fraction of transactions covered by the rule. <code>leverage</code> <code>float</code> Support \u2212 antecedent_support \u00d7 consequent_support. <code>conviction</code> <code>float</code> (1 \u2212 consequent_support) / (1 \u2212 confidence). <code>zhangs_metric</code> <code>float</code> Zhang's correlation metric. <code>jaccard</code> <code>float</code> Jaccard similarity of antecedent and consequent. <code>certainty</code> <code>float</code> Certainty factor. <code>kulczynski</code> <code>float</code> Kulczynski measure."},{"location":"api-reference/#metrics","title":"Metrics","text":"<p>The <code>metric</code> parameter accepts any of:</p> <p><code>confidence</code> \u00b7 <code>lift</code> \u00b7 <code>support</code> \u00b7 <code>leverage</code> \u00b7 <code>conviction</code> \u00b7 <code>zhangs_metric</code> \u00b7 <code>jaccard</code> \u00b7 <code>certainty</code> \u00b7 <code>kulczynski</code> \u00b7 <code>representativity</code> \u00b7 <code>antecedent support</code> \u00b7 <code>consequent support</code></p>"},{"location":"api-reference/#raises_1","title":"Raises","text":"Exception Condition <code>ValueError</code> <code>df</code> is missing <code>'support'</code> or <code>'itemsets'</code> columns <code>ValueError</code> <code>df</code> is empty <code>ValueError</code> Unknown <code>metric</code> value and <code>support_only=False</code>"},{"location":"architecture/","title":"Architecture","text":"<p>rusket is structured as a thin Python layer over a Rust core, compiled as a native extension module via PyO3 and maturin.</p>"},{"location":"architecture/#repository-layout","title":"Repository layout","text":"<pre><code>rusket/\n\u251c\u2500\u2500 src/                          # Rust (PyO3)\n\u2502   \u251c\u2500\u2500 lib.rs                    # Module root \u2014 exports to Python\n\u2502   \u251c\u2500\u2500 fpgrowth.rs               # FP-Tree + FP-Growth algorithm\n\u2502   \u251c\u2500\u2500 association_rules.rs      # Rule generation + 12 metrics\n\u2502   \u2514\u2500\u2500 common.rs                 # Shared helpers\n\u251c\u2500\u2500 python/\n\u2502   \u251c\u2500\u2500 rusket/                  # Primary Python package (pyproject.toml name)\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 fpgrowth.py           # Dispatch + numpy conversion\n\u2502   \u2502   \u251c\u2500\u2500 association_rules.py  # Label mapping + Rust call\n\u2502   \u2502   \u2514\u2500\u2500 _validation.py        # Input validation helpers\n\u2502   \u2514\u2500\u2500 fpgrowth_pyo3/            # Legacy compat package\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 test_fpbase.py            # Shared base test classes\n    \u251c\u2500\u2500 test_fpgrowth.py          # FP-Growth tests\n    \u251c\u2500\u2500 test_association_rules.py # Association rules tests\n    \u2514\u2500\u2500 test_benchmark.py         # Performance benchmarks\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<pre><code>flowchart TD\n    A[\"Python caller\\nfpgrowth(df, ...)\"] --&gt; B{Input type?}\n\n    B --&gt;|Dense pandas| C[\"np.ascontiguousarray\\n(uint8, C-order)\"]\n    B --&gt;|Sparse pandas| D[\"df.sparse.to_coo().tocsr()\\nindptr + indices as int32\"]\n    B --&gt;|Polars| E[\"df.to_numpy()\\n(Arrow zero-copy)\"]\n\n    C --&gt; F[\"Rust: fpgrowth_from_dense\\nPyReadonlyArray2&lt;u8&gt;\"]\n    D --&gt; G[\"Rust: fpgrowth_from_csr\\nPyReadonlyArray1&lt;i32&gt;\"]\n    E --&gt; F\n\n    F --&gt; H[\"FP-Tree construction\\n(Rust, single-pass)\"]\n    G --&gt; H\n\n    H --&gt; I[\"Recursive mining\\n(Rayon parallel)\"]\n    I --&gt; J[\"Vec&lt;(count, Vec&lt;usize&gt;)&gt;\"]\n    J --&gt; K[\"Python: build DataFrame\\n(frozensets, support)\"]</code></pre>"},{"location":"architecture/#fp-growth-algorithm","title":"FP-Growth algorithm","text":"<p>The Rust implementation follows the classic Han et al. (2000) FP-Growth algorithm:</p> <ol> <li>Header table scan \u2014 count item frequencies; prune items below <code>min_count</code>.</li> <li>FP-Tree construction \u2014 single-pass over transactions; compress into a prefix-tree structure.</li> <li>Recursive mining \u2014 for each frequent item, extract the conditional pattern base, build a conditional FP-Tree, and mine it recursively.</li> <li>Output \u2014 each leaf path materialises as one frequent itemset <code>(count, items)</code>.</li> </ol>"},{"location":"architecture/#dispatch-paths","title":"Dispatch paths","text":"Path Rust function Input shape Notes Dense pandas <code>fpgrowth_from_dense</code> <code>[n_rows \u00d7 n_cols]</code> uint8 Contiguous C array Sparse pandas <code>fpgrowth_from_csr</code> CSR <code>indptr + indices</code> Zero-copy scipy CSR Polars <code>fpgrowth_from_dense</code> same as dense Arrow \u2192 NumPy view"},{"location":"architecture/#association-rules","title":"Association rules","text":"<p>Rule generation is vectorised in Rust:</p> <ol> <li>For each frequent itemset of length \u2265 2, enumerate all non-empty antecedent / consequent splits.</li> <li>Look up antecedent and consequent supports from a pre-built hash map.</li> <li>Compute all 12 metrics in a single pass; filter by <code>(metric, min_threshold)</code>.</li> <li>Return raw integer index lists to Python; Python maps back to column names / frozensets.</li> </ol>"},{"location":"architecture/#building-from-source","title":"Building from source","text":"<pre><code># Prerequisites: Rust 1.83+, Python 3.10+, uv\nrustup update\nuv sync\n\n# Debug build (fast compile, slower runtime)\nuv run maturin develop\n\n# Release build (optimised)\nuv run maturin develop --release\n\n# Type checking\nuv run pyright python/\n\n# Tests\nuv run pytest tests/ -x -q\n\n# Cargo lint\ncargo check\ncargo clippy\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>rusket is substantially faster than mlxtend on real-world datasets and handles large datasets that cause mlxtend to struggle. These numbers are from an actual benchmark run on Apple M-series (arm64), <code>mlxtend</code> 0.23, <code>rusket</code> 0.1.</p>"},{"location":"benchmarks/#interactive-chart","title":"Interactive Chart","text":"<p>Click on legend entries to show/hide traces. All axes are log-scale for readability.</p>"},{"location":"benchmarks/#results-table","title":"Results Table","text":"<p>Synthetic market-basket data (Faker, power-law product popularity). <code>min_support</code> varies by tier to keep itemset counts reasonable.</p> Dataset <code>rusket</code> (pandas) <code>rusket</code> (polars) <code>mlxtend</code> Speedup tiny \u2014 5 rows \u00d7 11 items 0.005 s 0.002 s 0.002 s \u2014\u00b9 small \u2014 1 k rows \u00d7 50 items 0.007 s 0.006 s 0.166 s 24\u00d7 medium \u2014 10 k rows \u00d7 400 items 0.555 s 0.244 s 8.335 s 15\u00d7 large \u2014 100 k rows \u00d7 1 000 items 0.572 s 0.819 s 18.652 s 33\u00d7 HUGE \u2014 1 M rows \u00d7 2 000 items 3.113 s 6.015 s 104.024 s 33\u00d7 <p>\u00b9 At the \"tiny\" tier (5 rows), PyO3 call overhead dominates \u2014 mlxtend wins. From <code>small</code> onward rusket is always faster.</p> <p>Hardware &amp; settings</p> <p>Apple M-series, arm64. <code>min_support=0.10</code> (tiny/small/HUGE), <code>0.01</code> (medium), <code>0.05</code> (large). Times are single wall-clock runs (tracemalloc active). Polars path uses Arrow zero-copy buffers.</p>"},{"location":"benchmarks/#memory-comparison","title":"Memory comparison","text":"Dataset rusket peak RAM mlxtend peak RAM Ratio small \u2014 1 k \u00d7 50 0.1 MB 1.3 MB 24\u00d7 medium \u2014 10 k \u00d7 400 4.8 MB 92.4 MB 19\u00d7 large \u2014 100 k \u00d7 1 000 100.1 MB 319.7 MB 3\u00d7 HUGE \u2014 1 M \u00d7 2 000 2 000 MB 374.7 MB\u00b2 \u2014 <p>\u00b2 At HUGE scale, mlxtend's tracemalloc measurement only captured the Python process slice; its actual working set is far larger.  </p> <p>With the zero-copy PyArrow backend, rusket's peak RAM equals roughly the size of the input boolean matrix \u2014 no overhead for itemset materialization.</p>"},{"location":"benchmarks/#running-the-benchmarks-yourself","title":"Running the benchmarks yourself","text":"<pre><code># Install dev dependencies\nuv sync\n\n# Build Rust extension\nuv run maturin develop --release\n\n# Run all benchmarks\nuv run pytest tests/test_benchmark.py -v -s\n\n# Run with detailed timing output\nuv run pytest tests/test_benchmark.py --benchmark-sort=mean --benchmark-columns=min,mean,max,rounds\n\n# Regenerate the full interactive HTML report (rusket vs mlxtend vs polars)\nuv run python tests/generate_benchmark_report.py\n</code></pre>"},{"location":"benchmarks/#why-is-rusket-faster","title":"Why is rusket faster?","text":""},{"location":"benchmarks/#1-zero-copy-data-transfer","title":"1. Zero-copy data transfer","text":"<p>The Python-to-Rust boundary is a pointer hand-off, not a copy:</p> <ul> <li>Dense path \u2014 <code>df.values</code> as a contiguous <code>uint8</code> array is passed directly via <code>PyReadonlyArray2&lt;u8&gt;</code>.</li> <li>Sparse path \u2014 CSR <code>indptr</code> and <code>indices</code> arrays are passed as <code>PyReadonlyArray1&lt;i32&gt;</code>.</li> <li>Polars path \u2014 Arrow-backed NumPy buffer from <code>df.to_numpy()</code>.</li> </ul>"},{"location":"benchmarks/#2-no-python-loops","title":"2. No Python loops","text":"<p>The FP-Tree construction, recursive pattern mining, and all metric computations happen entirely within Rust. Python is only invoked at the boundaries (input preparation and output construction).</p>"},{"location":"benchmarks/#3-parallel-mining-rayon","title":"3. Parallel mining (Rayon)","text":"<p>Conditional pattern base mining is distributed across CPU threads via Rayon, automatically using all available cores.</p>"},{"location":"benchmarks/#4-memory-efficiency","title":"4. Memory efficiency","text":"<p>The Rust implementation uses compact integer representations for itemsets internally, avoiding the overhead of Python <code>frozenset</code> objects during mining. Frozensets are only materialised by Python on output.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes are documented here. This project follows Semantic Versioning.</p>"},{"location":"changelog/#010-2026-02-19","title":"[0.1.0] \u2014 2026-02-19","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>fpgrowth()</code> \u2014 FP-Growth frequent itemset mining backed by Rust + PyO3<ul> <li>Dense pandas path: flat <code>uint8</code> buffer via <code>fpgrowth_from_dense</code> (zero-copy)</li> <li>Sparse pandas path: CSR arrays via <code>fpgrowth_from_csr</code> (zero-copy)</li> <li>Polars path: Arrow-backed NumPy buffer via <code>fpgrowth_from_dense</code></li> <li>Parallel mining via Rayon</li> </ul> </li> <li><code>association_rules()</code> \u2014 Association rule generation with 12 metrics:     <code>confidence</code>, <code>lift</code>, <code>support</code>, <code>leverage</code>, <code>conviction</code>,     <code>zhangs_metric</code>, <code>jaccard</code>, <code>certainty</code>, <code>kulczynski</code>,     <code>representativity</code>, <code>antecedent support</code>, <code>consequent support</code></li> <li>Drop-in API compatibility with <code>mlxtend.frequent_patterns</code></li> <li><code>max_len</code> parameter to cap itemset size</li> <li><code>support_only</code> flag for fast support-only mode</li> <li><code>return_metrics</code> selector to include only desired metric columns</li> <li>Full test suite mirroring mlxtend behaviour</li> </ul>"},{"location":"changelog/#performance-vs-mlxtend","title":"Performance (vs mlxtend)","text":"Dataset Speedup Memory Small (5 \u00d7 11) ~10\u00d7 \u2013 Medium (10k \u00d7 400) ~5\u20138\u00d7 ~8\u00d7 less Large (100k \u00d7 1000) N/A (OOM) handles it"},{"location":"cookbook/","title":"Market Basket Analysis Cookbook","text":"<p>Welcome to the <code>rusket</code> cookbook! This guide provides practical examples for performing market basket analysis \u2014 finding frequent itemsets and generating recommendation rules efficiently.</p>"},{"location":"cookbook/#setup","title":"Setup","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport polars as pl\nimport plotly.express as px\nimport pyarrow.compute as pc\nfrom rusket import fpgrowth, association_rules\n</code></pre>"},{"location":"cookbook/#1-synthetic-dataset-generation","title":"1. Synthetic Dataset Generation","text":"<p>Let's generate a synthetic retail dataset \u2014 simulating a supermarket where customers buy various categories of items.</p> <pre><code>np.random.seed(42)\n\nitems = [\n    \"Milk\", \"Bread\", \"Butter\", \"Eggs\", \"Cheese\", \"Yogurt\",\n    \"Coffee\", \"Tea\", \"Sugar\", \"Apples\", \"Bananas\", \"Oranges\",\n    \"Chicken\", \"Beef\", \"Fish\", \"Rice\", \"Pasta\", \"Tomato Sauce\",\n    \"Onions\", \"Garlic\",\n]\n\nn_transactions = 10_000\nn_items = len(items)\n\n# Simulate different purchase frequencies (power-law distribution)\nprobabilities = np.power(np.arange(1, n_items + 1, dtype=float), -0.7)\nprobabilities /= probabilities.max()\nprobabilities = np.clip(probabilities * 0.3, 0.01, 0.8)\n\ndata = np.random.rand(n_transactions, n_items) &lt; probabilities\ndf = pd.DataFrame(data, columns=items)\n\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()\n</code></pre>"},{"location":"cookbook/#2-frequent-pattern-mining","title":"2. Frequent Pattern Mining","text":"<p>Extract frequent itemsets using the blazing-fast <code>fpgrowth</code> algorithm. <code>min_support=0.05</code> means an itemset must appear in at least 5% of all transactions.</p> <pre><code>fi = fpgrowth(df, min_support=0.05, use_colnames=True)\n\nprint(f\"Found {len(fi)} frequent itemsets.\")\nfi.sort_values(by=\"support\", ascending=False).head(10)\n</code></pre>"},{"location":"cookbook/#visualizing-frequent-itemsets","title":"Visualizing Frequent Itemsets","text":"<p>Plot the top 20 most frequent itemsets to understand what items are bought together most often.</p> <pre><code>top_fi = fi.sort_values(by=\"support\", ascending=False).head(20).copy()\ntop_fi[\"itemsets_str\"] = top_fi[\"itemsets\"].apply(lambda x: \" + \".join(list(x)))\n\nfig = px.bar(\n    top_fi,\n    x=\"support\",\n    y=\"itemsets_str\",\n    orientation=\"h\",\n    title=\"Top 20 Frequent Itemsets by Support\",\n    labels={\"support\": \"Support\", \"itemsets_str\": \"Itemset\"},\n    color=\"support\",\n    color_continuous_scale=\"Viridis\",\n)\nfig.update_layout(yaxis={\"categoryorder\": \"total ascending\"})\nfig.show()\n</code></pre>"},{"location":"cookbook/#3-generating-association-rules","title":"3. Generating Association Rules","text":"<p>Generate association rules from frequent itemsets using the <code>confidence</code> metric.</p> <pre><code>rules = association_rules(fi, num_itemsets=len(df), min_threshold=0.3)\n\nprint(f\"Generated {len(rules)} association rules.\")\nrules.sort_values(by=\"lift\", ascending=False).head()\n</code></pre>"},{"location":"cookbook/#filtering-rules","title":"Filtering Rules","text":"<p>Filter for strong rules with high confidence and high lift. High lift (&gt; 1) indicates items are positively correlated.</p> <pre><code>strong_rules = rules[(rules[\"confidence\"] &gt; 0.4) &amp; (rules[\"lift\"] &gt; 1.2)]\nstrong_rules = strong_rules.sort_values(by=\"lift\", ascending=False)\nstrong_rules.head(10)\n</code></pre>"},{"location":"cookbook/#visualizing-association-rules","title":"Visualizing Association Rules","text":"<p>A scatter plot of Support vs. Confidence helps identify the most valuable rules. Color represents <code>lift</code>.</p> <pre><code>fig = px.scatter(\n    rules,\n    x=\"support\",\n    y=\"confidence\",\n    color=\"lift\",\n    hover_data=[\"antecedents\", \"consequents\"],\n    title=\"Association Rules: Support vs Confidence\",\n    color_continuous_scale=\"Plasma\",\n)\nfig.show()\n</code></pre>"},{"location":"cookbook/#4-polars-integration","title":"4. Polars Integration","text":"<p><code>rusket</code> works natively with Polars without requiring expensive conversions.</p> <pre><code>df_pl = pl.from_pandas(df)\nfi_pl = fpgrowth(df_pl, min_support=0.05, use_colnames=True)\n\n# Generate rules directly from a Polars DataFrame\nrules_pl = association_rules(fi_pl, num_itemsets=df_pl.height, min_threshold=0.3)\nrules_pl.head(5)\n</code></pre>"},{"location":"cookbook/#5-working-with-pyarrow-outputs","title":"5. Working with PyArrow Outputs","text":"<p><code>rusket</code> returns itemsets as zero-copy PyArrow <code>ListArray</code> structures. This eliminates Python object overhead and allows you to process millions of rules with minimal memory.</p> <p>PyArrow dtype</p> <p>The <code>itemsets</code> column uses <code>pd.ArrowDtype(pa.list_(pa.string()))</code>. Standard Python <code>set</code> equality won't work directly \u2014 use PyArrow compute functions or <code>.apply(set)</code> on filtered subsets.</p>"},{"location":"cookbook/#querying-itemsets-with-pyarrow-compute","title":"Querying Itemsets with PyArrow Compute","text":"<pre><code>import pyarrow.compute as pc\n\n# Find all itemsets where the first element is 'Milk'\ncontains_milk = pc.list_element(fi[\"itemsets\"].array, 0) == \"Milk\"\nfi[contains_milk].head()\n</code></pre>"},{"location":"cookbook/#converting-to-python-sets","title":"Converting to Python Sets","text":"<p>Only do this on small filtered subsets to avoid materializing Python objects for the full result.</p> <pre><code>top_10 = fi.head(10).copy()\ntop_10[\"python_sets\"] = top_10[\"itemsets\"].apply(set)\n\nprint(\"Zero-Copy PyArrow Array Dtype:\", fi[\"itemsets\"].dtype)\ntop_10.head()\n</code></pre>"},{"location":"migration/","title":"Migration from mlxtend","text":"<p>rusket is designed as a drop-in replacement for <code>mlxtend.frequent_patterns</code>. In the vast majority of cases the only change you need is the import line.</p>"},{"location":"migration/#import-change","title":"Import change","text":"Before (mlxtend)After (rusket) <pre><code>from mlxtend.frequent_patterns import fpgrowth, association_rules\n</code></pre> <pre><code>from rusket import fpgrowth, association_rules\n</code></pre>"},{"location":"migration/#api-comparison","title":"API comparison","text":""},{"location":"migration/#fpgrowth","title":"<code>fpgrowth</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame \\| pl.DataFrame</code> rusket also accepts Polars <code>min_support</code> <code>float</code> <code>float</code> identical <code>use_colnames</code> <code>bool</code> <code>bool</code> identical <code>max_len</code> <code>int\\|None</code> <code>int\\|None</code> identical <code>verbose</code> <code>int</code> <code>int</code> accepted but unused <code>null_values</code> <code>bool</code> <code>bool</code> pandas only"},{"location":"migration/#association_rules","title":"<code>association_rules</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame</code> output of <code>fpgrowth</code> <code>num_itemsets</code> <code>int</code> <code>int</code> identical <code>metric</code> <code>str</code> <code>str</code> identical (12 metrics) <code>min_threshold</code> <code>float</code> <code>float</code> identical <code>support_only</code> <code>bool</code> <code>bool</code> identical <code>return_metrics</code> <code>list[str]</code> <code>list[str]</code> identical"},{"location":"migration/#return-value","title":"Return value","text":"<p>Both functions return identical DataFrame structures:</p> <ul> <li><code>fpgrowth</code> \u2192 <code>pd.DataFrame</code> with <code>['support', 'itemsets']</code></li> <li><code>association_rules</code> \u2192 <code>pd.DataFrame</code> with <code>['antecedents', 'consequents', ...metrics]</code></li> </ul> <p>Itemsets are <code>frozenset</code> objects, exactly as in mlxtend.</p>"},{"location":"migration/#whats-different","title":"What's different?","text":"<p>Behaviour differences</p> <ul> <li>Performance: rusket is significantly faster on medium/large datasets and uses far less memory.</li> <li>Polars input: rusket accepts <code>polars.DataFrame</code> natively; mlxtend does not.</li> <li>Sparse DataFrames: rusket uses the CSR path, which is more memory-efficient than mlxtend for sparse data.</li> <li><code>null_values</code> / Rust path: When <code>null_values=True</code>, rusket currently falls back gracefully (no error), but the Rust path is not yet used for null-containing DataFrames.</li> </ul>"},{"location":"migration/#uninstalling-mlxtend","title":"Uninstalling mlxtend","text":"<p>Once you have validated that rusket produces the same results:</p> <pre><code>pip uninstall mlxtend\n</code></pre> <p>rusket has no runtime dependency on mlxtend.</p>"},{"location":"polars/","title":"Polars Support","text":"<p>rusket accepts <code>polars.DataFrame</code> natively alongside pandas, via the Arrow-backed zero-copy path.</p>"},{"location":"polars/#installation","title":"Installation","text":"<p>Install rusket with the Polars extra:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>This pins <code>polars&gt;=0.20</code>. If you already have Polars installed, you can also just <code>pip install rusket</code>.</p>"},{"location":"polars/#usage","title":"Usage","text":"<p>The <code>fpgrowth</code> function detects Polars DataFrames automatically \u2014 no extra parameters needed:</p> <pre><code>import polars as pl\nfrom rusket import fpgrowth, association_rules\n\n# Build a Polars one-hot DataFrame\ndf = pl.DataFrame({\n    \"milk\":  [True, True,  False, True],\n    \"bread\": [True, False, True,  True],\n    \"eggs\":  [False, True, True,  True],\n})\n\n# Mine frequent itemsets \u2014 identical call as with pandas\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nprint(freq)\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# ...\n\n# association_rules always returns a pandas DataFrame\nrules = association_rules(freq, num_itemsets=len(df), metric=\"lift\", min_threshold=1.0)\nprint(rules)\n</code></pre> <p>Return type</p> <p><code>fpgrowth</code> always returns a pandas DataFrame, regardless of input type. <code>association_rules</code> also returns a pandas DataFrame.</p>"},{"location":"polars/#how-it-works","title":"How it works","text":"<p>The Polars path uses <code>polars.DataFrame.to_numpy()</code> which returns an Arrow-backed NumPy buffer \u2014 zero-copy for numeric dtypes.</p> <pre><code>Polars DataFrame\n    \u2502\n    \u25bc  df.to_numpy()  (zero-copy for bool/int dtypes)\nnumpy uint8 array\n    \u2502\n    \u25bc  fpgrowth_from_dense()  (Rust, PyO3 ReadonlyArray2&lt;u8&gt;)\nRust FP-Tree mining\n    \u2502\n    \u25bc\npandas DataFrame  [support, itemsets]\n</code></pre> <p>No intermediate Python object creation occurs between the Polars input and the Rust mining step.</p>"},{"location":"polars/#supported-dtypes","title":"Supported dtypes","text":"Polars dtype Supported <code>Boolean</code> \u2705 <code>Int8 / Int16 / Int32 / Int64</code> \u2705 (0/1 values) <code>UInt8 / UInt16 / UInt32 / UInt64</code> \u2705 (0/1 values) <code>Float32 / Float64</code> \u26a0\ufe0f (0.0/1.0 values, cast to uint8) Categorical / String \u274c (pre-encode with <code>get_dummies</code>) <p>Lazy frames</p> <p>Pass <code>.collect()</code> before calling <code>fpgrowth</code> if you have a <code>LazyFrame</code>: <pre><code>freq = fpgrowth(lazy_df.collect(), min_support=0.3, use_colnames=True)\n</code></pre></p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#installation","title":"Installation","text":"pipuvconda <pre><code>pip install rusket\n</code></pre> <pre><code>uv add rusket\n</code></pre> <pre><code>pip install rusket  # rusket is not on conda-forge yet\n</code></pre> <p>To also enable Polars support:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>Coming from mlxtend?</p> <p>rusket is a drop-in replacement. In most cases you only need to change your import: <pre><code># Before\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n# After\nfrom rusket import fpgrowth, association_rules\n</code></pre> See the full Migration Guide for details.</p>"},{"location":"quickstart/#step-1-prepare-your-data","title":"Step 1 \u2014 Prepare your data","text":"<p><code>fpgrowth</code> expects a one-hot encoded DataFrame of boolean or 0/1 integer values where rows are transactions and columns are items.</p> <pre><code>import pandas as pd\n\ndataset = [\n    [\"milk\", \"bread\"],\n    [\"milk\", \"eggs\"],\n    [\"bread\", \"eggs\"],\n    [\"milk\", \"bread\", \"eggs\"],\n]\n\n# Build a one-hot DataFrame\nitems = [\"milk\", \"bread\", \"eggs\"]\ndf = pd.DataFrame(\n    [[item in tx for item in items] for tx in dataset],\n    columns=items,\n    dtype=bool,\n)\nprint(df)\n#    milk  bread   eggs\n# 0  True   True  False\n# 1  True  False   True\n# 2 False   True   True\n# 3  True   True   True\n</code></pre>"},{"location":"quickstart/#step-2-mine-frequent-itemsets","title":"Step 2 \u2014 Mine frequent itemsets","text":"<pre><code>from rusket import fpgrowth\n\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nprint(freq)\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# 2     0.75          (eggs,)\n# 3     0.50   (milk, bread,)\n# 4     0.50    (milk, eggs,)\n# 5     0.50   (bread, eggs,)\n# 6     0.25  (milk, bread, eggs,)\n</code></pre>"},{"location":"quickstart/#step-3-generate-association-rules","title":"Step 3 \u2014 Generate association rules","text":"<pre><code>from rusket import association_rules\n\nrules = association_rules(\n    freq,\n    num_itemsets=len(df),\n    metric=\"confidence\",\n    min_threshold=0.6,\n)\nprint(rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n</code></pre> <p>num_itemsets</p> <p>Pass the total transaction count (<code>len(df)</code>) so that support-based metrics are computed correctly.</p>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Migration from mlxtend \u2014 side-by-side comparison</li> <li>API Reference \u2014 all parameters and metrics explained</li> <li>Polars Support \u2014 zero-copy Arrow path</li> <li>Benchmarks \u2014 performance vs mlxtend</li> </ul>"}]}