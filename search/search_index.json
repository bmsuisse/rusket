{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#rusket","title":"rusket","text":"<p>A modern library for Market Basket Analysis and Recommender Engines. Arrow-backed, fully compatible with Spark, and written entirely in Rust.</p> <p> </p>"},{"location":"#what-is-rusket","title":"What is rusket?","text":"<p><code>rusket</code> turns raw transaction logs into revenue intelligence \u2014 \"frequently bought together\" rules, personalised recommendations, high-profit bundle discovery, and sequential customer journey analysis.</p> <p>The core algorithms run entirely in Rust (via PyO3) and accept Pandas, Polars, and Spark DataFrames natively with zero-copy Arrow transfers:</p> Input Rust path Notes Dense pandas DataFrame <code>fpgrowth_from_dense</code> Flat <code>uint8</code> buffer \u2014 zero-copy Sparse pandas DataFrame <code>fpgrowth_from_csr</code> Raw CSR arrays \u2014 zero-copy Polars DataFrame <code>fpgrowth_from_dense</code> Arrow-backed <code>numpy</code> buffer"},{"location":"#why-rusket","title":"Why rusket?","text":"Feature rusket mlxtend Speed (medium dataset) ~0.4 s ~4 s Memory (large dataset) ~3 s OOM Polars support \u2705 \u274c Sparse DataFrame support \u2705 \u26a0\ufe0f limited Zero Python dependencies \u2705 (<code>numpy</code>, <code>pandas</code>) \u274c (many) 12 association metrics \u2705 \u2705"},{"location":"#quick-example-frequently-bought-together","title":"Quick Example \u2014 \"Frequently Bought Together\"","text":"<p>Identify which products are bought together most often \u2014 the foundation of cross-sell widgets on any e-commerce site:</p> <pre><code>import pandas as pd\nfrom rusket import mine, association_rules\n\n# One week of grocery checkout data (1 row = 1 receipt, 1 col = 1 SKU)\nreceipts = pd.DataFrame({\n    \"milk\":    [1, 1, 0, 1, 0, 1],\n    \"bread\":   [1, 0, 1, 1, 1, 0],\n    \"butter\":  [1, 0, 1, 0, 0, 1],\n    \"eggs\":    [0, 1, 1, 0, 1, 1],\n    \"coffee\":  [0, 1, 0, 0, 1, 1],\n}, dtype=bool)\n\n# Step 1 \u2014 find which product combinations appear in \u226540% of receipts\nfreq = mine(receipts, min_support=0.4, use_colnames=True)\n\n# Step 2 \u2014 keep rules with \u226560% confidence\nrules = association_rules(\n    freq, num_itemsets=len(receipts), metric=\"confidence\", min_threshold=0.6\n)\nprint(rules[[\"antecedents\", \"consequents\", \"confidence\", \"lift\"]]\n      .sort_values(\"lift\", ascending=False))\n</code></pre> <p>Get Started API Reference View on GitHub</p>"},{"location":"1b-challenge/","title":"The Road to 1 Billion Rows","text":"<p>A story of memory management, algorithmic trade-offs, and the satisfying click of watching numbers improve.</p> <p>At the core of <code>rusket</code> is the belief that frequent itemset mining should scale on a single machine. No Spark cluster, no distributed coordination \u2014 just one process, tuned to be as efficient as possible.</p> <p>The 1 Billion Row Challenge is how we hold ourselves accountable.</p>"},{"location":"1b-challenge/#where-we-started-sorted-chunks-k-way-merge","title":"Where We Started: Sorted Chunks + K-Way Merge","text":"<p>The first streaming design took inspiration from external-sort databases:</p> <ol> <li>Each call to <code>add_chunk()</code> receives <code>(txn_id, item_id)</code> arrays from Python.</li> <li>Rust sorts each chunk in-place using <code>rayon::par_sort_unstable()</code>.</li> <li>Chunks exceeding the RAM budget are spilled to anonymous <code>tempfile</code> files on disk.</li> <li>When <code>.mine()</code> is called, a k-way heap merge streams all sorted chunks in order, building the CSR matrix on the fly.</li> </ol> <p>The appeal was clear: never hold more than one chunk in RAM at once. </p> <pre><code>Memory: O(chunk_size)\nmine() memory: O(k cursors + CSR output)\n</code></pre>"},{"location":"1b-challenge/#the-first-problem-disk-exhaustion","title":"The first problem: disk exhaustion","text":"<p>Running the 500M \u2192 1B targets, our machine hit 99% disk usage. The culprit: 500k transactions/chunk \u00d7 23 items/txn \u00d7 12 bytes/pair = ~138 MB per chunk. At 1B rows that's ~2,000 chunks = ~276 GB of tempfiles. Oops.</p> <p>We tried raising <code>max_ram_mb</code> to 8,000 \u2014 but the process was OOM-killed at 800M rows (exit code 137).</p>"},{"location":"1b-challenge/#the-second-problem-mine_t-grows-super-linearly","title":"The second problem: <code>mine_t</code> grows super-linearly","text":"<p>Even with disk spilling working, the k-way merge over thousands of files was slow:</p> target_rows add_t mine_t total 300M 36.1s 516.8s 552.9s 500M 65.2s 1543.5s 1608.7s <p>The heap with 1,000+ cursors causes cache thrashing. <code>mine_t</code> was growing super-linearly \u2014 a fundamental problem with the architecture.</p>"},{"location":"1b-challenge/#the-insight-hashmap-aggregation","title":"The Insight: HashMap Aggregation","text":"<p>The key observation was that the sorted-chunk approach stores every pair from every chunk, even if the same transaction appears in 100 different chunks. The real data that matters is:</p> <pre><code>unique_transactions \u00d7 avg_items_per_transaction\n</code></pre> <p>For 1B rows with ~43M unique transactions \u00d7 23 items: that's only ~5GB \u2014 vs the sorted approach's ~12GB.</p> <p>We replaced the entire chunk + merge system with a single <code>AHashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>:</p> <pre><code>pub fn add_chunk(&amp;mut self, txn_ids: ..., item_ids: ...) {\n    for (&amp;t, &amp;i) in txns.iter().zip(items.iter()) {\n        self.txns.entry(t).or_default().push(i);\n    }\n}\n</code></pre> <p><code>mine()</code> now just: 1. Collects <code>(txn_id, &amp;items)</code> from the HashMap 2. <code>par_sort</code> by txn_id 3. Sort+dedup each transaction's item list 4. Build CSR \u2192 feed to algorithm</p> <p>No k-way merge. No disk spill. No tempfiles.</p>"},{"location":"1b-challenge/#initial-numbers-100m-200m-fp-growth","title":"Initial numbers (100M &amp; 200M, FP-Growth)","text":"target_rows add_t mine_t total 100M 5.7s 26.6s 32.3s 200M 10.4s 84.6s 95.0s <p>Compared to the old approach's 300M taking 299.5s \u2014 we were doing 200M in 95s. A 3\u00d7 speedup just from the architecture change.</p>"},{"location":"1b-challenge/#what-we-tried-the-iteration-phase","title":"What We Tried: The Iteration Phase","text":"<p>We then instrumented the benchmark to systematically compare every knob we could turn.</p>"},{"location":"1b-challenge/#approach-smallvec-u16-items-regression","title":"Approach: SmallVec + u16 items (\u274c Regression)","text":"<p>The idea: use <code>SmallVec&lt;[u16; 32]&gt;</code> to store items inline on the stack (avoiding heap allocations for short transactions), and <code>u16</code> instead of <code>i32</code> to halve memory consumption.</p> <p>In theory: 43M \u00d7 (8 + 2\u00d732) = ~3GB vs ~5GB.</p> <p>In practice: a 2\u00d7 slowdown. The 32-element inline size is too large for the CPU stack, causes cache pressure on every HashMap lookup, and the <code>u16\u2192i32</code> conversion at mine time adds hidden overhead.</p> <p>Lesson: measure first, optimize second.</p>"},{"location":"1b-challenge/#knob-chunk-size-100k-vs-500k-vs-2m","title":"Knob: Chunk size (100k vs 500k vs 2M)","text":"chunk add_t mine_t total 100k 10.0s 3.8s 13.8s 500k 12.1s 5.3s 17.4s 2M 12.5s 5.3s 17.8s <p>Chunk size barely matters \u2014 <code>add_t</code> and <code>mine_t</code> are dominated by HashMap operations and algorithm complexity, not chunk boundary overhead.</p>"},{"location":"1b-challenge/#knob-algorithm-fp-growth-vs-eclat","title":"Knob: Algorithm (FP-Growth vs Eclat)","text":"<p>This was the biggest discovery. At 100M rows:</p> method add_t mine_t total M rows/s fpgrowth 10.0s ~55s ~65s 1.5 eclat 10.0s 3.8s 13.8s 7.24 <p>Eclat is ~14\u00d7 faster at mining for dense retail data (23 items/txn). The reason: Eclat works with vertical tidlists \u2014 for dense datasets with many frequent 2-itemsets, the intersection operations are extremely cache-friendly, while FP-Growth's conditional pattern base construction has much higher memory pressure.</p>"},{"location":"1b-challenge/#final-results-the-road-to-1b","title":"Final Results: The Road to 1B","text":"<p>Running with Eclat + 500k chunks on real bootstrapped retail data:</p>"},{"location":"1b-challenge/#dense-retail-data-andi_data-8416-txns-119-items-avg-23-itemstxn","title":"Dense retail data (andi_data: 8,416 txns \u00d7 119 items, avg 23 items/txn)","text":"target_rows add_t mine_t total M rows/s itemsets 100M 12.5s 6.2s 18.7s 5.35 15,218 200M 22.7s 13.4s 36.1s 5.53 15,226 500M 61.1s 38.7s 99.8s 5.01 15,234 1B 173.5s 208.5s 382.1s 2.62 15,233 <p>\u2705 1 Billion rows. 382 seconds. 15,233 frequent itemsets. No OOM. No disk spill.</p> <p>The itemset count is consistent across all scales (15,218\u201315,234), confirming the synthetic sampling faithfully preserves the original item frequency distribution.</p>"},{"location":"1b-challenge/#sparse-catalogue-data-andi_data2-540k-txns-2603-items-avg-44-itemstxn","title":"Sparse catalogue data (andi_data2: 540k txns \u00d7 2,603 items, avg 4.4 items/txn)","text":"target_rows add_t mine_t total M rows/s itemsets 100M 20.2s 4.9s 25.1s 4.00 37 200M 26.2s 9.2s 35.4s 5.67 37 500M \u274c \u274c OOM - - 1B \u274c \u274c OOM - - <p>While sparse data (4.4 items/txn) is faster end-to-end for smaller datasets due to fewer items per transaction, the memory overhead of the HashMap scaling up to 1 billion rows on this specific dataset shape still exceeds our machine's constraints.</p> <p>The 1B challenge for dataset 2 remains open.</p>"},{"location":"1b-challenge/#what-we-learned","title":"What We Learned","text":""},{"location":"1b-challenge/#architecture-beats-micro-optimisation","title":"Architecture beats micro-optimisation","text":"<p>The jump from k-way merge \u2192 HashMap changed 5-minute runs into 30-second runs. No amount of SIMD or loop unrolling would have bridged that gap.</p>"},{"location":"1b-challenge/#eclat-vs-fp-growth-depends-on-density","title":"Eclat vs FP-Growth depends on density","text":"data density winner why Dense (avg &gt;10 items/txn) Eclat Tidlist intersections are O(n), very cache-friendly Sparse (avg &lt;5 items/txn) FP-Growth or similar Fewer candidates, conditional bases are small <p>The default <code>method=\"eclat\"</code> is now recommended for most real-world transaction data.</p>"},{"location":"1b-challenge/#add_t-scales-linearly-mine_t-grows-with-complexity","title":"<code>add_t</code> scales linearly; <code>mine_t</code> grows with complexity","text":"<p><code>add_t</code> is essentially O(total_pairs) \u2014 just HashMap insertions. <code>mine_t</code> grows with the number of candidate itemsets, which is roughly stable once you have a representative sample. This explains why going from 500M \u2192 1B doubles <code>add_t</code> but only doubles <code>mine_t</code>.</p>"},{"location":"1b-challenge/#running-it-yourself","title":"Running It Yourself","text":"<pre><code>from rusket import FPMiner\nimport numpy as np\n\n# Stream your data in chunks\nminer = FPMiner(n_items=your_n_items)\n\nfor txn_ids_chunk, item_ids_chunk in your_data_stream():\n    miner.add_chunk(\n        txn_ids_chunk.astype(np.int64),\n        item_ids_chunk.astype(np.int32),\n    )\n\n# Mine \u2014 Eclat is the winner for dense retail data\nfreq = miner.mine(min_support=0.02, max_len=3, method=\"eclat\")\nprint(f\"Found {len(freq):,} frequent itemsets\")\n</code></pre> <p>The full benchmark script is in <code>benchmarks/bench_fpminer_realistic.py</code>.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This file is auto-generated by <code>scripts/gen_api_reference.py</code>.  Do not edit by hand \u2014 update the Python docstrings instead.</p>"},{"location":"api-reference/#functional-api","title":"Functional API","text":"<p>Convenience module-level functions.  For most use-cases these are the only entry points you need.</p>"},{"location":"api-reference/#mine","title":"<code>mine</code>","text":"<p>Mine frequent itemsets using the optimal algorithm.</p> <p>This module-level function relies on the Object-Oriented APIs.</p> <pre><code>from rusket.mine import mine\n\nmine(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, method: 'str' = 'auto', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#fpgrowth","title":"<code>fpgrowth</code>","text":"<p>Find frequent itemsets using the FP-growth algorithm.</p> <p>This module-level function relies on the Object-Oriented APIs.</p> <pre><code>from rusket.fpgrowth import fpgrowth\n\nfpgrowth(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, method: 'str' = 'fpgrowth', verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#eclat","title":"<code>eclat</code>","text":"<p>Find frequent itemsets using the Eclat algorithm.</p> <p>This module-level function relies on the Object-Oriented APIs.</p> <pre><code>from rusket.eclat import eclat\n\neclat(df: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, column_names: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#association_rules","title":"<code>association_rules</code>","text":"<pre><code>from rusket.association_rules import association_rules\n\nassociation_rules(df: 'pd.DataFrame | Any', num_itemsets: 'int | None' = None, df_orig: 'pd.DataFrame | None' = None, null_values: 'bool' = False, metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, support_only: 'bool' = False, return_metrics: 'list[str]' = ['antecedent support', 'consequent support', 'support', 'confidence', 'lift', 'representativity', 'leverage', 'conviction', 'zhangs_metric', 'jaccard', 'certainty', 'kulczynski']) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#prefixspan","title":"<code>prefixspan</code>","text":"<p>Mine sequential patterns using the PrefixSpan algorithm.</p> <p>This function discovers frequent sequences of items across multiple users/sessions. Currently, this assumes sequences where each event consists of a single item (e.g., a sequence of page views or a sequence of individual products bought over time).</p> <pre><code>from rusket.prefixspan import prefixspan\n\nprefixspan(sequences: 'list[list[int]]', min_support: 'int', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description sequences list of list of int A list of sequences, where each sequence is a list of integers representing items. Example: <code>[[1, 2, 3], [1, 3], [2, 3]]</code>. min_support int The minimum absolute support (number of sequences a pattern must appear in). max_len int, optional The maximum length of the sequential patterns to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'support' and 'sequence' columns."},{"location":"api-reference/#hupm","title":"<code>hupm</code>","text":"<p>Mine high-utility itemsets.</p> <p>This function discovers combinations of items that generate a high total utility (e.g., profit) across all transactions, even if they aren't the most frequent.</p> <pre><code>from rusket.hupm import hupm\n\nhupm(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description transactions list of list of int A list of transactions, where each transaction is a list of item IDs. utilities list of list of float A list of identical structure to <code>transactions</code>, but containing the numeric utility (e.g., profit) of that item in that specific transaction. min_utility float The minimum total utility required to consider a pattern \"high-utility\". max_len int, optional The maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns."},{"location":"api-reference/#sequences_from_event_log","title":"<code>sequences_from_event_log</code>","text":"<p>Convert an event log DataFrame into the sequence format required by PrefixSpan.</p> <p>Accepts Pandas, Polars, or PySpark DataFrames. Data is grouped by <code>user_col</code>, ordered by <code>time_col</code>, and <code>item_col</code> values are collected into sequences.</p> <pre><code>from rusket.prefixspan import sequences_from_event_log\n\nsequences_from_event_log(df: 'Any', user_col: 'str', time_col: 'str', item_col: 'str') -&gt; 'tuple[list[list[int]], dict[int, Any]]'\n</code></pre> <p>Parameters</p> Parameter Type Description df pd.DataFrame | pl.DataFrame | pyspark.sql.DataFrame Event log containing users, timestamps, and items. user_col str Column name identifying the sequence (e.g., user_id or session_id). time_col str Column name for ordering events. item_col str Column name for the items. <p>Returns</p> Name Type Description tuple of (sequences, item_mapping) - sequences: The nested list of integers to pass to <code>prefixspan()</code>. - item_mapping: A dictionary mapping the integer IDs back to the original item labels."},{"location":"api-reference/#mine_hupm","title":"<code>mine_hupm</code>","text":"<p>Mine high-utility itemsets from a long-format DataFrame.</p> <p>Converts a Pandas or Polars DataFrame into the required list-of-lists format and runs the High-Utility Pattern Mining (HUPM) algorithm.</p> <pre><code>from rusket.hupm import mine_hupm\n\nmine_hupm(data: 'Any', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description data pd.DataFrame or pl.DataFrame A long-format DataFrame where each row represents an item in a transaction. transaction_col str Column name identifying the transaction ID. item_col str Column name identifying the item ID (must be numeric integers). utility_col str Column name identifying the numeric utility (e.g. price, profit) of the item. min_utility float The minimum total utility required to consider a pattern \"high-utility\". max_len int, optional Maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns."},{"location":"api-reference/#mine_duckdb","title":"<code>mine_duckdb</code>","text":"<p>Stream directly from a DuckDB query via Arrow RecordBatches.</p> <p>This is extremely memory efficient, bypassing Pandas entirely.</p> <pre><code>from rusket.streaming import mine_duckdb\n\nmine_duckdb(con: 'Any', query: 'str', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, chunk_size: 'int' = 1000000) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#mine_spark","title":"<code>mine_spark</code>","text":"<p>Stream natively from a PySpark DataFrame on Databricks via Arrow.</p> <p>Uses <code>toLocalIterator()</code> to fetch Arrow chunks incrementally directly to the driver node, avoiding massive memory spikes.</p> <pre><code>from rusket.streaming import mine_spark\n\nmine_spark(spark_df: 'Any', n_items: 'int', txn_col: 'str', item_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#from_transactions","title":"<code>from_transactions</code>","text":"<p>Convert long-format transactional data to a one-hot boolean matrix.</p> <pre><code>from rusket.transactions import from_transactions\n\nfrom_transactions(data: 'DataFrame | Sequence[Sequence[str | int]] | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description data One of:  - Pandas / Polars / Spark DataFrame with (at least) two columns: one for the transaction identifier and one for the item. - List of lists where each inner list contains the items of a single transaction, e.g. <code>[[\"bread\", \"milk\"], [\"bread\", \"eggs\"]]</code>. transaction_col Name of the column that identifies transactions.  If <code>None</code> the first column is used.  Ignored for list-of-lists input. item_col Name of the column that contains item values.  If <code>None</code> the second column is used.  Ignored for list-of-lists input. <p>Returns</p> Name Type Description pd.DataFrame A boolean DataFrame ready for :func:<code>rusket.fpgrowth</code> or :func:<code>rusket.eclat</code>.  Column names correspond to the unique items. <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"order_id\": [1, 1, 1, 2, 2, 3],\n...     \"item\": [3, 4, 5, 3, 5, 8],\n... })\n&gt;&gt;&gt; ohe = rusket.from_transactions(df)\n&gt;&gt;&gt; freq = rusket.fpgrowth(ohe, min_support=0.5, use_colnames=True)\n</code></pre>"},{"location":"api-reference/#from_transactions_csr","title":"<code>from_transactions_csr</code>","text":"<p>Convert long-format transactional data to a CSR matrix + column names.</p> <p>Unlike :func:<code>from_transactions</code>, this returns a raw <code>scipy.sparse.csr_matrix</code> that can be passed directly to :func:<code>rusket.fpgrowth</code> or :func:<code>rusket.eclat</code> \u2014 no pandas overhead.</p> <p>For billion-row datasets, this processes data in chunks of <code>chunk_size</code> rows, keeping peak memory to one chunk + the running CSR.</p> <pre><code>from rusket.transactions import from_transactions_csr\n\nfrom_transactions_csr(data: 'DataFrame | str | Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None, chunk_size: 'int' = 10000000) -&gt; 'tuple[Any, list[str]]'\n</code></pre> <p>Parameters</p> Parameter Type Description data One of:  - Pandas DataFrame with (at least) two columns. - Polars DataFrame or Spark DataFrame (converted internally). - File path (str / Path) to a Parquet file \u2014 read in chunks. transaction_col Name of the transaction-id column. Defaults to the first column. item_col Name of the item column. Defaults to the second column. chunk_size Number of rows per chunk. Lower values use less memory. Default: 10 million rows. <p>Returns</p> Name Type Description tuple[scipy.sparse.csr_matrix, list[str]] A CSR matrix and the list of column (item) names.  Pass directly::  csr, names = from_transactions_csr(df) freq = fpgrowth(csr, min_support=0.001, use_colnames=True, column_names=names) <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; csr, names = rusket.from_transactions_csr(\"orders.parquet\")\n&gt;&gt;&gt; freq = rusket.fpgrowth(csr, min_support=0.001,\n...                        use_colnames=True, column_names=names)\n</code></pre>"},{"location":"api-reference/#from_pandas","title":"<code>from_pandas</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_pandas\n\nfrom_pandas(df: 'pd.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#from_polars","title":"<code>from_polars</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_polars\n\nfrom_polars(df: 'pl.DataFrame', transaction_col: 'str | None' = None, item_col: 'str | None' = None, verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#from_spark","title":"<code>from_spark</code>","text":"<p>Shorthand for <code>from_transactions(df, transaction_col, item_col)</code>.</p> <pre><code>from rusket.transactions import from_spark\n\nfrom_spark(df: 'Any', transaction_col: 'str | None' = None, item_col: 'str | None' = None) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#oop-mining-api","title":"OOP Mining API","text":"<p>All mining classes share a common <code>Miner.from_transactions()</code> / <code>.mine()</code> interface. <code>FPGrowth</code>, <code>Eclat</code>, <code>AutoMiner</code>, and <code>HUPM</code> also inherit <code>RuleMinerMixin</code> which adds <code>.association_rules()</code> and <code>.recommend_items()</code> helpers.</p>"},{"location":"api-reference/#fpgrowth_1","title":"<code>FPGrowth</code>","text":"<p>FP-Growth frequent itemset miner.</p> <p>This class wraps the fast, core Rust FP-Growth implementation.</p> <pre><code>from rusket.fpgrowth import FPGrowth\n\nFPGrowth(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>"},{"location":"api-reference/#fpgrowthmine","title":"<code>FPGrowth.mine</code>","text":"<p>Execute the FP-growth algorithm on the stored data.</p> <pre><code>from rusket.fpgrowth import FPGrowth.mine\n\nFPGrowth.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names)."},{"location":"api-reference/#eclat_1","title":"<code>Eclat</code>","text":"<p>Eclat frequent itemset miner.</p> <p>Eclat is typically faster than FP-growth on dense datasets due to efficient vertical bitset intersection logic.</p> <pre><code>from rusket.eclat import Eclat\n\nEclat(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>"},{"location":"api-reference/#eclatmine","title":"<code>Eclat.mine</code>","text":"<p>Execute the Eclat algorithm on the stored data.</p> <pre><code>from rusket.eclat import Eclat.mine\n\nEclat.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names)."},{"location":"api-reference/#autominer","title":"<code>AutoMiner</code>","text":"<p>Automatic frequent itemset miner.</p> <p>Selects the optimal miner (FP-Growth or Eclat) based on matrix density.</p> <pre><code>from rusket.mine import AutoMiner\n\nAutoMiner(data: 'pd.DataFrame | pl.DataFrame | np.ndarray | Any', item_names: 'list[str] | None' = None, min_support: 'float' = 0.5, null_values: 'bool' = False, use_colnames: 'bool' = False, max_len: 'int | None' = None, verbose: 'int' = 0, **kwargs: 'Any')\n</code></pre>"},{"location":"api-reference/#autominermine","title":"<code>AutoMiner.mine</code>","text":"<p>Execute the optimal algorithm on the stored data.</p> <pre><code>from rusket.mine import AutoMiner.mine\n\nAutoMiner.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pandas.DataFrame DataFrame with two columns: - <code>support</code>: the support score. - <code>itemsets</code>: list of items (indices or column names)."},{"location":"api-reference/#prefixspan_1","title":"<code>PrefixSpan</code>","text":"<p>Sequential Pattern Mining (PrefixSpan) model.</p> <p>This class discovers frequent sequences of items across multiple users/sessions.</p> <pre><code>from rusket.prefixspan import PrefixSpan\n\nPrefixSpan(data: 'list[list[int]]', min_support: 'int', max_len: 'int | None' = None, item_mapping: 'dict[int, Any] | None' = None)\n</code></pre>"},{"location":"api-reference/#prefixspanmine","title":"<code>PrefixSpan.mine</code>","text":"<p>Mine sequential patterns using PrefixSpan.</p> <pre><code>from rusket.prefixspan import PrefixSpan.mine\n\nPrefixSpan.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'support' and 'sequence' columns. Sequences are mapped back to original item names if <code>from_transactions</code> was used."},{"location":"api-reference/#hupm_1","title":"<code>HUPM</code>","text":"<p>High-Utility Pattern Mining (HUPM) model.</p> <p>This class discovers combinations of items that generate a high total utility (e.g., profit) across all transactions, even if they aren't the most frequent.</p> <pre><code>from rusket.hupm import HUPM\n\nHUPM(transactions: 'list[list[int]]', utilities: 'list[list[float]]', min_utility: 'float', max_len: 'int | None' = None)\n</code></pre>"},{"location":"api-reference/#hupmmine","title":"<code>HUPM.mine</code>","text":"<p>Mine high-utility itemsets.</p> <pre><code>from rusket.hupm import HUPM.mine\n\nHUPM.mine(**kwargs: 'Any') -&gt; 'pd.DataFrame'\n</code></pre> <p>Returns</p> Name Type Description pd.DataFrame A DataFrame containing 'utility' and 'itemset' columns."},{"location":"api-reference/#fpminer","title":"<code>FPMiner</code>","text":"<p>Streaming FP-Growth / Eclat accumulator for billion-row datasets.</p> <p>Feeds (transaction_id, item_id) integer arrays to Rust one chunk at a time.  Rust accumulates per-transaction item lists in a <code>HashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>.  Peak Python memory = one chunk.</p> <pre><code>from rusket.streaming import FPMiner\n\nFPMiner(n_items: 'int', max_ram_mb: 'int | None' = -1, hint_n_transactions: 'int | None' = None) -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description n_items int Number of distinct items (column count).  All item IDs fed via :meth:<code>add_chunk</code> must be in <code>[0, n_items)</code>. <p>Examples</p> <pre><code>Process a Parquet file 10 M rows at a time:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from rusket import FPMiner\n&gt;&gt;&gt; miner = FPMiner(n_items=500_000)\n&gt;&gt;&gt; for chunk in pd.read_parquet(\"orders.parquet\", chunksize=10_000_000):\n...     txn = chunk[\"txn_id\"].to_numpy(dtype=\"int64\")\n...     item = chunk[\"item_idx\"].to_numpy(dtype=\"int32\")\n...     miner.add_chunk(txn, item)\n&gt;&gt;&gt; freq = miner.mine(min_support=0.001, max_len=3, use_colnames=False)\n</code></pre>"},{"location":"api-reference/#fpmineradd_arrow_batch","title":"<code>FPMiner.add_arrow_batch</code>","text":"<p>Feed a PyArrow RecordBatch directly into the miner. Zero-copy extraction is used if types match (Int64/Int32).</p> <pre><code>from rusket.streaming import FPMiner.add_arrow_batch\n\nFPMiner.add_arrow_batch(batch: 'Any', txn_col: 'str', item_col: 'str') -&gt; 'FPMiner'\n</code></pre>"},{"location":"api-reference/#fpmineradd_chunk","title":"<code>FPMiner.add_chunk</code>","text":"<p>Feed a chunk of (transaction_id, item_id) pairs.</p> <pre><code>from rusket.streaming import FPMiner.add_chunk\n\nFPMiner.add_chunk(txn_ids: 'np.ndarray', item_ids: 'np.ndarray') -&gt; 'FPMiner'\n</code></pre> <p>Parameters</p> Parameter Type Description txn_ids np.ndarray[int64] 1-D array of transaction identifiers (arbitrary 64-bit integers). item_ids np.ndarray[int32] 1-D array of item column indices (0-based). <p>Returns</p> Name Type Description self  (for chaining)"},{"location":"api-reference/#fpminermine","title":"<code>FPMiner.mine</code>","text":"<p>Mine frequent itemsets from all accumulated transactions.</p> <pre><code>from rusket.streaming import FPMiner.mine\n\nFPMiner.mine(min_support: 'float' = 0.5, max_len: 'int | None' = None, use_colnames: 'bool' = False, column_names: 'list[str] | None' = None, method: \"typing.Literal['fpgrowth', 'eclat', 'auto']\" = 'auto', verbose: 'int' = 0) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description min_support float Minimum support threshold in <code>(0, 1]</code>. max_len int | None Maximum itemset length. use_colnames bool If <code>True</code>, itemsets contain column names instead of indices. column_names list[str] | None Column names to use when <code>use_colnames=True</code>. method \"fpgrowth\" | \"eclat\" | \"auto\" Mining algorithm to use.  <code>\"auto\"</code> (default) picks the best algorithm automatically based on data density after pre-filtering rare items (Borgelt 2003 heuristic: density &lt; 15% \u2192 Eclat, else FPGrowth). verbose int Level of verbosity: &gt;0 prints progress logs and times. <p>Returns</p> Name Type Description pd.DataFrame Columns <code>support</code> and <code>itemsets</code>."},{"location":"api-reference/#fpminerreset","title":"<code>FPMiner.reset</code>","text":"<p>Free all accumulated data.</p> <pre><code>from rusket.streaming import FPMiner.reset\n\nFPMiner.reset() -&gt; 'None'\n</code></pre>"},{"location":"api-reference/#ruleminermixin-shared-miner-interface","title":"<code>RuleMinerMixin</code> \u2014 Shared Miner Interface","text":"<p><code>FPGrowth</code>, <code>Eclat</code>, <code>AutoMiner</code>, and <code>HUPM</code> all inherit these methods from <code>RuleMinerMixin</code>.  You do not construct <code>RuleMinerMixin</code> directly.</p>"},{"location":"api-reference/#ruleminermixinassociation_rules","title":"<code>RuleMinerMixin.association_rules</code>","text":"<p>Generate association rules from the mined frequent itemsets.</p> <pre><code>from rusket.model import RuleMinerMixin.association_rules\n\nRuleMinerMixin.association_rules(metric: 'str' = 'confidence', min_threshold: 'float' = 0.8, return_metrics: 'list[str] | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description metric str, default='confidence' The metric to evaluate if a rule is of interest. min_threshold float, default=0.8 The minimum threshold for the evaluation metric. return_metrics list[str] | None, default=None List of metrics to include in the resulting DataFrame. Defaults to all available metrics. <p>Returns</p> Name Type Description pd.DataFrame DataFrame of strong association rules."},{"location":"api-reference/#ruleminermixinrecommend_items","title":"<code>RuleMinerMixin.recommend_items</code>","text":"<p>Suggest items to add to an active cart using association rules.</p> <pre><code>from rusket.model import RuleMinerMixin.recommend_items\n\nRuleMinerMixin.recommend_items(items: 'list[Any]', n: 'int' = 5) -&gt; 'list[Any]'\n</code></pre> <p>Parameters</p> Parameter Type Description items list[Any] The items currently in the cart or basket. n int, default=5 The maximum number of items to recommend. <p>Returns</p> Name Type Description list[Any] List of recommended items, ordered by lift and then confidence. <p>Notes Rules are computed once and cached with the key <code>(metric=\"lift\", min_threshold=1.0)</code>.  Call :meth:<code>_invalidate_rules_cache</code> to force a re-computation after re-mining.</p>"},{"location":"api-reference/#ruleminermixin_invalidate_rules_cache","title":"<code>RuleMinerMixin._invalidate_rules_cache</code>","text":"<p>Clear the cached association rules (call after re-mining).</p> <pre><code>from rusket.model import RuleMinerMixin._invalidate_rules_cache\n\nRuleMinerMixin._invalidate_rules_cache() -&gt; 'None'\n</code></pre>"},{"location":"api-reference/#recommenders","title":"Recommenders","text":""},{"location":"api-reference/#als","title":"<code>ALS</code>","text":"<p>Implicit ALS collaborative filtering model.</p> <pre><code>from rusket.als import ALS\n\nALS(factors: 'int' = 64, regularization: 'float' = 0.01, alpha: 'float' = 40.0, iterations: 'int' = 15, seed: 'int' = 42, verbose: 'bool' = False, cg_iters: 'int' = 10, use_cholesky: 'bool' = False, anderson_m: 'int' = 0, **kwargs: 'Any') -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description factors int Number of latent factors. regularization float L2 regularisation weight. alpha float Confidence scaling: <code>confidence = 1 + alpha * r</code>. iterations int Number of ALS outer iterations. seed int Random seed. cg_iters int Conjugate Gradient iterations per user/item solve (ignored when <code>use_cholesky=True</code>).  Reduce to 3 for very large datasets. use_cholesky bool Use a direct Cholesky solve instead of iterative CG. Exact solution; faster when users have many interactions relative to <code>factors</code>. anderson_m int History window for Anderson Acceleration of the outer ALS loop (default 0 = disabled).  Recommended value: 5.  ALS is a fixed-point iteration <code>(U,V) \u2192 F(U,V)</code>.  Anderson mixing extrapolates over the last <code>m</code> residuals to reach the fixed point faster, typically reducing the number of outer iterations by 30\u201350 % at identical recommendation quality::  # Baseline: 15 iterations model = ALS(iterations=15, cg_iters=3)  # Anderson-accelerated: 10 iterations, ~2.5\u00d7 faster, same quality model = ALS(iterations=10, cg_iters=3, anderson_m=5)  Memory overhead: <code>m</code> copies of the full <code>(U \u2225 V)</code> matrix (~57 MB per copy at 25M ratings, k=64)."},{"location":"api-reference/#alsfit","title":"<code>ALS.fit</code>","text":"<p>Fit the model to the user-item interaction matrix.</p> <pre><code>from rusket.als import ALS.fit\n\nALS.fit(interactions: 'Any') -&gt; 'ALS'\n</code></pre>"},{"location":"api-reference/#alsrecommend_items","title":"<code>ALS.recommend_items</code>","text":"<p>Top-N items for a user. Set exclude_seen=False to include already-seen items.</p> <pre><code>from rusket.als import ALS.recommend_items\n\nALS.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -&gt; 'tuple[Any, Any]'\n</code></pre>"},{"location":"api-reference/#alsrecommend_users","title":"<code>ALS.recommend_users</code>","text":"<p>Top-N users for an item.</p> <pre><code>from rusket.als import ALS.recommend_users\n\nALS.recommend_users(item_id: 'int', n: 'int' = 10) -&gt; 'tuple[Any, Any]'\n</code></pre>"},{"location":"api-reference/#bpr","title":"<code>BPR</code>","text":"<p>Bayesian Personalized Ranking (BPR) model for implicit feedback.</p> <p>BPR optimizes for ranking rather than reconstruction error (like ALS). It works by drawing positive items the user interacted with, and negative items they haven't, and adjusting latent factors to ensure the positive item scores higher.</p> <pre><code>from rusket.bpr import BPR\n\nBPR(factors: 'int' = 64, learning_rate: 'float' = 0.05, regularization: 'float' = 0.01, iterations: 'int' = 150, seed: 'int' = 42, verbose: 'bool' = False, **kwargs: 'Any') -&gt; 'None'\n</code></pre> <p>Parameters</p> Parameter Type Description factors int Number of latent factors (default: 64). learning_rate float SGD learning rate (default: 0.05). regularization float L2 regularization weight (default: 0.01). iterations int Number of passes over the entire interaction dataset (default: 150). seed int Random seed for Hogwild! SGD sampling (default: 42)."},{"location":"api-reference/#bprfit","title":"<code>BPR.fit</code>","text":"<p>Fit the BPR model to the user-item interaction matrix.</p> <pre><code>from rusket.bpr import BPR.fit\n\nBPR.fit(interactions: 'Any') -&gt; 'BPR'\n</code></pre>"},{"location":"api-reference/#bprrecommend_items","title":"<code>BPR.recommend_items</code>","text":"<p>Top-N items for a user.</p> <pre><code>from rusket.bpr import BPR.recommend_items\n\nBPR.recommend_items(user_id: 'int', n: 'int' = 10, exclude_seen: 'bool' = True) -&gt; 'tuple[Any, Any]'\n</code></pre>"},{"location":"api-reference/#recommender","title":"<code>Recommender</code>","text":"<p>Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.</p> <pre><code>from rusket.recommend import Recommender\n\nRecommender(als_model: 'ALS | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)\n</code></pre>"},{"location":"api-reference/#recommenderpredict_next_chunk","title":"<code>Recommender.predict_next_chunk</code>","text":"<p>Batch-rank the next best products for every user in user_history_df.</p> <pre><code>from rusket.recommend import Recommender.predict_next_chunk\n\nRecommender.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#recommenderrecommend_for_cart","title":"<code>Recommender.recommend_for_cart</code>","text":"<p>Suggest items to add to an active cart using association rules.</p> <pre><code>from rusket.recommend import Recommender.recommend_for_cart\n\nRecommender.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -&gt; 'list[int]'\n</code></pre>"},{"location":"api-reference/#recommenderrecommend_for_user","title":"<code>Recommender.recommend_for_user</code>","text":"<p>Top-N recommendations for a user via Hybrid ALS + Semantic.</p> <pre><code>from rusket.recommend import Recommender.recommend_for_user\n\nRecommender.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -&gt; 'tuple[np.ndarray, np.ndarray]'\n</code></pre> <p>Parameters</p> Parameter Type Description user_id int The user ID to generate recommendations for. n int, default=5 Number of items to return. alpha float, default=0.5 Weight blending CF vs Semantic. <code>alpha=1.0</code> is pure CF. <code>alpha=0.0</code> is pure semantic. target_item_for_semantic int | None, default=None If provided, semantic similarity is computed against this item. If None, and alpha &lt; 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF."},{"location":"api-reference/#nextbestaction","title":"<code>NextBestAction</code>","text":"<p>Hybrid recommender combining ALS collaborative filtering, semantic similarities, and association rules.</p> <pre><code>from rusket.recommend import NextBestAction\n\nNextBestAction(als_model: 'ALS | None' = None, rules_df: 'pd.DataFrame | None' = None, item_embeddings: 'np.ndarray | None' = None)\n</code></pre>"},{"location":"api-reference/#nextbestactionpredict_next_chunk","title":"<code>NextBestAction.predict_next_chunk</code>","text":"<p>Batch-rank the next best products for every user in user_history_df.</p> <pre><code>from rusket.recommend import NextBestAction.predict_next_chunk\n\nNextBestAction.predict_next_chunk(user_history_df: 'pd.DataFrame', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'pd.DataFrame'\n</code></pre>"},{"location":"api-reference/#nextbestactionrecommend_for_cart","title":"<code>NextBestAction.recommend_for_cart</code>","text":"<p>Suggest items to add to an active cart using association rules.</p> <pre><code>from rusket.recommend import NextBestAction.recommend_for_cart\n\nNextBestAction.recommend_for_cart(cart_items: 'list[int]', n: 'int' = 5) -&gt; 'list[int]'\n</code></pre>"},{"location":"api-reference/#nextbestactionrecommend_for_user","title":"<code>NextBestAction.recommend_for_user</code>","text":"<p>Top-N recommendations for a user via Hybrid ALS + Semantic.</p> <pre><code>from rusket.recommend import NextBestAction.recommend_for_user\n\nNextBestAction.recommend_for_user(user_id: 'int', n: 'int' = 5, alpha: 'float' = 0.5, target_item_for_semantic: 'int | None' = None) -&gt; 'tuple[np.ndarray, np.ndarray]'\n</code></pre> <p>Parameters</p> Parameter Type Description user_id int The user ID to generate recommendations for. n int, default=5 Number of items to return. alpha float, default=0.5 Weight blending CF vs Semantic. <code>alpha=1.0</code> is pure CF. <code>alpha=0.0</code> is pure semantic. target_item_for_semantic int | None, default=None If provided, semantic similarity is computed against this item. If None, and alpha &lt; 1.0, it computes semantic similarity against the user's most recently interacted item (if history is available) or falls back to pure CF."},{"location":"api-reference/#analytics-utilities","title":"Analytics &amp; Utilities","text":""},{"location":"api-reference/#score_potential","title":"<code>score_potential</code>","text":"<p>Cross-selling potential scores \u2014 shape <code>(n_users, n_items)</code> or <code>(n_users, len(target_categories))</code>.</p> <p>Items the user has already interacted with are masked to <code>-inf</code>.</p> <pre><code>from rusket.recommend import score_potential\n\nscore_potential(user_history: 'list[list[int]]', als_model: 'ALS', target_categories: 'list[int] | None' = None) -&gt; 'np.ndarray'\n</code></pre>"},{"location":"api-reference/#similar_items","title":"<code>similar_items</code>","text":"<p>Find the most similar items to a given item ID based on ALS latent factors.</p> <p>Computes cosine similarity between the specified item's latent vector and all other item vectors in the <code>item_factors</code> matrix.</p> <pre><code>from rusket.similarity import similar_items\n\nsimilar_items(als_model: rusket.als.ALS, item_id: int, n: int = 5) -&gt; tuple[numpy.ndarray, numpy.ndarray]\n</code></pre> <p>Parameters</p> Parameter Type Description als_model ALS A fitted <code>rusket.ALS</code> model instance. item_id int The internal integer index of the target item. n int Number of most similar items to return. <p>Returns</p> Name Type Description tuple[np.ndarray, np.ndarray] <code>(item_ids, cosine_similarities)</code> sorted in descending order."},{"location":"api-reference/#find_substitutes","title":"<code>find_substitutes</code>","text":"<p>Substitute/cannibalizing products via negative association rules.</p> <p>Items with high individual support but low co-occurrence (lift &lt; 1.0) likely cannibalize each other.</p> <pre><code>from rusket.analytics import find_substitutes\n\nfind_substitutes(rules_df: 'pd.DataFrame', max_lift: 'float' = 0.8) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description rules_df DataFrame output from <code>rusket.association_rules</code>. max_lift Upper bound for lift; lift &lt; 1.0 implies negative correlation. <p>Returns</p> Name Type Description pd.DataFrame sorted ascending by lift (most severe cannibalization first)."},{"location":"api-reference/#customer_saturation","title":"<code>customer_saturation</code>","text":"<p>Customer saturation by unique items/categories bought, split into deciles.</p> <pre><code>from rusket.analytics import customer_saturation\n\ncustomer_saturation(df: 'pd.DataFrame', user_col: 'str', category_col: 'str | None' = None, item_col: 'str | None' = None) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description df Interaction DataFrame. user_col Column identifying the user. category_col Category column (optional; at least one of category/item required). item_col Item column (optional). <p>Returns</p> Name Type Description pd.DataFrame with <code>unique_count</code>, <code>saturation_pct</code>, and <code>decile</code> columns."},{"location":"api-reference/#export_item_factors","title":"<code>export_item_factors</code>","text":"<p>Exports ALS latent item factors as a Pandas DataFrame for Vector DBs.</p> <p>This format is ideal for ingesting into FAISS, Pinecone, or Qdrant for Retrieval-Augmented Generation (RAG) and semantic search.</p> <pre><code>from rusket.export import export_item_factors\n\nexport_item_factors(als_model: 'ALS', include_labels: 'bool' = True) -&gt; 'pd.DataFrame'\n</code></pre> <p>Parameters</p> Parameter Type Description als_model ALS A fitted <code>rusket.ALS</code> model instance. include_labels bool, default=True Whether to include the string item labels (if available from <code>ALS.from_transactions</code>). <p>Returns</p> Name Type Description pd.DataFrame A DataFrame where each row is an item with columns <code>item_id</code>, optionally <code>item_label</code>, and <code>vector</code> (a dense 1-D numpy array of the item's latent factors). <p>Examples</p> <pre><code>&gt;&gt;&gt; model = rusket.ALS(factors=32).fit(interactions)\n&gt;&gt;&gt; df = rusket.export_item_factors(model)\n&gt;&gt;&gt; # Ingest into FAISS / Pinecone / Qdrant\n&gt;&gt;&gt; vectors = np.stack(df[\"vector\"].values)\n</code></pre>"},{"location":"api-reference/#visualization-rusketviz","title":"Visualization (<code>rusket.viz</code>)","text":"<p>Graph and visualization utilities.  Requires <code>networkx</code> (<code>pip install networkx</code>).</p>"},{"location":"api-reference/#rusketvizto_networkx","title":"<code>rusket.viz.to_networkx</code>","text":"<p>Convert a Rusket association rules DataFrame into a NetworkX Directed Graph.</p> <p>Nodes represent individual items. Directed edges represent rules (antecedent \u2192 consequent). Edge weights are set by the <code>edge_attr</code> parameter (typically lift or confidence).</p> <p>This is extremely useful for running community detection algorithms (e.g., Louvain, Girvan-Newman) to automatically discover product clusters, or for visualising cross-selling patterns as a force-directed graph.</p> <pre><code>from rusket.viz import rusket.viz.to_networkx\n\nrusket.viz.to_networkx(rules_df: 'pd.DataFrame', source_col: 'str' = 'antecedents', target_col: 'str' = 'consequents', edge_attr: 'str' = 'lift') -&gt; 'networkx.DiGraph'\n</code></pre> <p>Parameters</p> Parameter Type Description rules_df pd.DataFrame A Pandas DataFrame generated by <code>rusket.association_rules()</code>. source_col str, default='antecedents' Column name containing antecedents (graph edge sources). target_col str, default='consequents' Column name containing consequents (graph edge targets). edge_attr str, default='lift' The metric to use as edge weight/thickness. <p>Returns</p> Name Type Description networkx.DiGraph A directed graph of the association rules. If <code>rules_df</code> is empty, returns an empty <code>DiGraph</code>. <p>Notes Requires the <code>networkx</code> package (<code>pip install networkx</code>). When multiple rules produce the same directed edge, only the highest-weight rule is retained.</p> <p>Examples</p> <pre><code>&gt;&gt;&gt; import rusket\n&gt;&gt;&gt; G = rusket.viz.to_networkx(rules_df, edge_attr=\"lift\")\n&gt;&gt;&gt; # Community detection with networkx\n&gt;&gt;&gt; import networkx.algorithms.community as nx_comm\n&gt;&gt;&gt; communities = nx_comm.greedy_modularity_communities(G.to_undirected())\n</code></pre>"},{"location":"api-reference/#distributed-spark-api-rusketspark","title":"Distributed Spark API (<code>rusket.spark</code>)","text":"<p>All functions in <code>rusket.spark</code> distribute computation across PySpark partitions using Apache Arrow (zero-copy) for maximum throughput.</p>"},{"location":"api-reference/#rusketsparkmine_grouped","title":"<code>rusket.spark.mine_grouped</code>","text":"<p>Distribute Market Basket Analysis across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>rusket.mine</code> to each group concurrently across the cluster.</p> <p>It assumes the input PySpark DataFrame is formatted like a dense boolean matrix (One-Hot Encoded) per group, where rows are transactions.</p> <pre><code>from rusket.spark import rusket.spark.mine_grouped\n\nrusket.spark.mine_grouped(df: 'Any', group_col: 'str', min_support: 'float' = 0.5, max_len: 'int | None' = None, method: 'str' = 'auto', use_colnames: 'bool' = True) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). min_support Minimum support threshold. max_len Maximum itemset length. method Algorithm to use: 'auto', 'fpgrowth', or 'eclat'. use_colnames If True, returns item names instead of column indices. Must be True for PySpark <code>applyInArrow</code> schema consistency. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>support</code> (float) - <code>itemsets</code> (array of strings)"},{"location":"api-reference/#rusketsparkrules_grouped","title":"<code>rusket.spark.rules_grouped</code>","text":"<p>Distribute Association Rule Mining across PySpark partitions.</p> <p>This takes the frequent itemsets DataFrame (output of <code>mine_grouped</code>) and applies <code>association_rules</code> uniformly across the groups.</p> <pre><code>from rusket.spark import rusket.spark.rules_grouped\n\nrusket.spark.rules_grouped(df: 'Any', group_col: 'str', num_itemsets: 'dict[Any, int] | int', metric: 'str' = 'confidence', min_threshold: 'float' = 0.8) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The PySpark <code>DataFrame</code> containing frequent itemsets. group_col The column to group by. num_itemsets A dictionary mapping group IDs to their total transaction count, or a single integer if all groups have the same number of transactions. metric The metric to filter by (e.g. \"confidence\", \"lift\"). min_threshold The minimal threshold for the evaluation metric. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A DataFrame containing antecedents, consequents, and all rule metrics, prepended with the <code>group_col</code>."},{"location":"api-reference/#rusketsparkprefixspan_grouped","title":"<code>rusket.spark.prefixspan_grouped</code>","text":"<p>Distribute Sequential Pattern Mining (PrefixSpan) across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>PrefixSpan.from_transactions</code> to each group concurrently across the cluster.</p> <pre><code>from rusket.spark import rusket.spark.prefixspan_grouped\n\nrusket.spark.prefixspan_grouped(df: 'Any', group_col: 'str', user_col: 'str', time_col: 'str', item_col: 'str', min_support: 'int' = 1, max_len: 'int | None' = None) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). user_col The column identifying the sequence within each group (e.g., <code>user_id</code> or <code>session_id</code>). time_col The column used for ordering events within a sequence. item_col The column containing the items. min_support The minimum absolute support (number of sequences a pattern must appear in). max_len Maximum length of the sequential patterns to mine. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>support</code> (long/int64) - <code>sequence</code> (array of strings)"},{"location":"api-reference/#rusketsparkhupm_grouped","title":"<code>rusket.spark.hupm_grouped</code>","text":"<p>Distribute High-Utility Pattern Mining (HUPM) across PySpark partitions.</p> <p>This function groups a PySpark DataFrame by <code>group_col</code> and applies <code>HUPM.from_transactions</code> to each group concurrently across the cluster.</p> <pre><code>from rusket.spark import rusket.spark.hupm_grouped\n\nrusket.spark.hupm_grouped(df: 'Any', group_col: 'str', transaction_col: 'str', item_col: 'str', utility_col: 'str', min_utility: 'float', max_len: 'int | None' = None) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The input <code>pyspark.sql.DataFrame</code>. group_col The column to group by (e.g. <code>store_id</code>). transaction_col The column identifying the transaction within each group. item_col The column containing the numeric item IDs. utility_col The column containing the numeric utility (e.g., profit) of the item in the transaction. min_utility The minimum total utility required to consider a pattern \"high-utility\". max_len Maximum length of the itemsets to mine. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A PySpark DataFrame containing: - <code>group_col</code> - <code>utility</code> (double/float64) - <code>itemset</code> (array of longs/int64)"},{"location":"api-reference/#rusketsparkrecommend_batches","title":"<code>rusket.spark.recommend_batches</code>","text":"<p>Distribute Batch Recommendations across PySpark partitions.</p> <p>This function uses <code>mapInArrow</code> to process partitions of users concurrently, applying a pre-fitted <code>Recommender</code> (or <code>ALS</code>) to each chunk.</p> <pre><code>from rusket.spark import rusket.spark.recommend_batches\n\nrusket.spark.recommend_batches(df: 'Any', model: 'Any', user_col: 'str' = 'user_id', k: 'int' = 5) -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description df The PySpark <code>DataFrame</code> containing user histories (must contain <code>user_col</code>). model The pre-trained <code>Recommender</code> or <code>ALS</code> model instance to use for scoring. user_col The column identifying the user. k The number of top recommendations to return per user. <p>Returns</p> Name Type Description pyspark.sql.DataFrame A DataFrame with two columns: - <code>user_col</code> - <code>recommended_items</code> (array of longs/int64)"},{"location":"api-reference/#rusketsparkto_spark","title":"<code>rusket.spark.to_spark</code>","text":"<p>Convert a Pandas or Polars DataFrame into a PySpark DataFrame.</p> <pre><code>from rusket.spark import rusket.spark.to_spark\n\nrusket.spark.to_spark(spark_session: 'Any', df: 'Any') -&gt; 'Any'\n</code></pre> <p>Parameters</p> Parameter Type Description spark_session The active PySpark <code>SparkSession</code>. df The <code>pd.DataFrame</code> or <code>pl.DataFrame</code> to convert. <p>Returns</p> Name Type Description pyspark.sql.DataFrame The resulting PySpark DataFrame."},{"location":"architecture/","title":"Architecture","text":"<p>rusket is structured as a thin Python layer over a Rust core, compiled as a native extension module via PyO3 and maturin.</p>"},{"location":"architecture/#repository-layout","title":"Repository layout","text":"<pre><code>rusket/\n\u251c\u2500\u2500 src/                          # Rust (PyO3)\n\u2502   \u251c\u2500\u2500 lib.rs                    # Module root \u2014 exports to Python\n\u2502   \u251c\u2500\u2500 fpgrowth.rs               # FP-Tree + FP-Growth algorithm\n\u2502   \u251c\u2500\u2500 association_rules.rs      # Rule generation + 12 metrics\n\u2502   \u2514\u2500\u2500 common.rs                 # Shared helpers\n\u251c\u2500\u2500 python/\n\u2502   \u251c\u2500\u2500 rusket/                  # Primary Python package (pyproject.toml name)\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 fpgrowth.py           # Dispatch + numpy conversion\n\u2502   \u2502   \u251c\u2500\u2500 association_rules.py  # Label mapping + Rust call\n\u2502   \u2502   \u2514\u2500\u2500 _validation.py        # Input validation helpers\n\u2502   \u2514\u2500\u2500 fpgrowth_pyo3/            # Legacy compat package\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 test_fpbase.py            # Shared base test classes\n    \u251c\u2500\u2500 test_fpgrowth.py          # FP-Growth tests\n    \u251c\u2500\u2500 test_association_rules.py # Association rules tests\n    \u2514\u2500\u2500 test_benchmark.py         # Performance benchmarks\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<pre><code>graph TD\n    classDef python fill:#4B8BBE,stroke:#306998,stroke-width:2px,color:white;\n    classDef rust fill:#DEA584,stroke:#000,stroke-width:2px,color:black;\n    classDef data fill:#FFD43B,stroke:#306998,stroke-width:2px,color:black;\n\n    A[\"Python Caller&lt;br/&gt;rusket.mine(df)\"]:::python --&gt; B{\"Input Data Type\"}:::python\n\n    B --&gt;|Dense Pandas| C[\"C-Contiguous Array&lt;br/&gt;(uint8)\"]:::data\n    B --&gt;|Sparse Pandas| D[\"CSR Matrix&lt;br/&gt;(indptr, indices)\"]:::data\n    B --&gt;|Polars| E[\"Numpy View&lt;br/&gt;(Zero-copy)\"]:::data\n\n    C --&gt; F[\"Rust FFI&lt;br/&gt;fpgrowth_from_dense\"]:::rust\n    D --&gt; G[\"Rust FFI&lt;br/&gt;fpgrowth_from_csr\"]:::rust\n    E --&gt; F\n\n    F --&gt; H[\"Tree Construction&lt;br/&gt;(Single Pass)\"]:::rust\n    G --&gt; H\n\n    H --&gt; I[\"Recursive Mining&lt;br/&gt;(Rayon Parallel)\"]:::rust\n    I --&gt; J[\"Raw Vectors&lt;br/&gt;Vec&lt;(count, Vec&lt;usize&gt;)&gt;\"]:::data\n\n    J --&gt; K[\"Python Transformation&lt;br/&gt;Build pd.DataFrame\"]:::python</code></pre>"},{"location":"architecture/#fp-growth-algorithm","title":"FP-Growth algorithm","text":"<p>The Rust implementation follows the classic Han et al. (2000) FP-Growth algorithm:</p> <ol> <li>Header table scan \u2014 count item frequencies; prune items below <code>min_count</code>.</li> <li>FP-Tree construction \u2014 single-pass over transactions; compress into a prefix-tree structure.</li> <li>Recursive mining \u2014 for each frequent item, extract the conditional pattern base, build a conditional FP-Tree, and mine it recursively.</li> <li>Output \u2014 each leaf path materialises as one frequent itemset <code>(count, items)</code>.</li> </ol>"},{"location":"architecture/#dispatch-paths","title":"Dispatch paths","text":"Path Rust function Input shape Notes Dense pandas <code>fpgrowth_from_dense</code> <code>[n_rows \u00d7 n_cols]</code> uint8 Contiguous C array Sparse pandas <code>fpgrowth_from_csr</code> CSR <code>indptr + indices</code> Zero-copy scipy CSR Polars <code>fpgrowth_from_dense</code> same as dense Arrow \u2192 NumPy view"},{"location":"architecture/#association-rules","title":"Association rules","text":"<p>Rule generation is vectorised in Rust:</p> <ol> <li>For each frequent itemset of length \u2265 2, enumerate all non-empty antecedent / consequent splits.</li> <li>Look up antecedent and consequent supports from a pre-built hash map.</li> <li>Compute all 12 metrics in a single pass; filter by <code>(metric, min_threshold)</code>.</li> <li>Return raw integer index lists to Python; Python maps back to column names / frozensets.</li> </ol>"},{"location":"architecture/#building-from-source","title":"Building from source","text":"<pre><code># Prerequisites: Rust 1.83+, Python 3.10+, uv\nrustup update\nuv sync\n\n# Debug build (fast compile, slower runtime)\nuv run maturin develop\n\n# Release build (optimised)\nuv run maturin develop --release\n\n# Type checking\nuv run pyright python/\n\n# Tests\nuv run pytest tests/ -x -q\n\n# Cargo lint\ncargo check\ncargo clippy\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>rusket includes two mining algorithms (FP-Growth and Eclat), both implemented in Rust. These benchmarks compare rusket against mlxtend (pure Python) on synthetic and real-world datasets.</p> <p>Measured on Apple M-series (arm64).</p>"},{"location":"benchmarks/#scale-benchmarks-1m-1b-rows","title":"Scale Benchmarks: 1M \u2192 1B Rows \ud83d\ude80","text":""},{"location":"benchmarks/#interactive-chart","title":"Interactive Chart","text":""},{"location":"benchmarks/#three-input-paths","title":"Three Input Paths","text":"<p>rusket supports three ways to ingest data at scale:</p> <ol> <li><code>from_transactions</code> \u2192 sparse DataFrame \u2014 returns a pandas DataFrame, easy API</li> <li>Direct CSR \u2192 Rust \u2014 pass <code>scipy.sparse.csr_matrix</code> directly to <code>fpgrowth()</code>, skips pandas entirely</li> <li><code>FPMiner</code> Streaming \u2014 memory-safe accumulator for 100M+ rows that don't fit in RAM</li> </ol>"},{"location":"benchmarks/#in-memory-scale-fpgrowth","title":"In-Memory Scale (fpgrowth)","text":"Scale <code>from_transactions</code> \u2192 fpgrowth Direct CSR \u2192 fpgrowth Speedup 1M rows (200k txns \u00d7 10k items) 5.0s 0.1s 50\u00d7 10M rows (2M txns \u00d7 50k items) 24.4s 1.2s 20\u00d7 50M rows (10M txns \u00d7 100k items) 63.1s 4.0s 15\u00d7 100M rows (20M txns \u00d7 200k items) 134.2s 10.1s 13\u00d7 200M rows (40M txns \u00d7 200k items) 246.8s 17.6s 14\u00d7 <p>Direct CSR is the power-user path</p> <p>At 100M rows, direct CSR mining takes 1.2 seconds \u2014 the bottleneck is entirely the CSR build (24.5s). Compare to the pandas sparse path where mining alone takes 9.0s due to <code>sparse.to_coo().tocsr()</code> overhead.</p>"},{"location":"benchmarks/#out-of-core-scale-fpminer-streaming","title":"Out-of-Core Scale (FPMiner Streaming)","text":"<p>For real-world retail datasets scaling to 1 Billion rows, <code>FPMiner</code> uses a memory-safe chunks approach (per-chunk sort + k-way merge).</p> <p>Benchmark: 2,603 retail items, avg 4.4 items/basket, min_support = 0.1%</p> Scale add_chunk() mine() Total Time Itemsets Found 50M rows 4.8s 5.6s 10.4s 1,260 100M rows 10.6s 13.9s 24.6s 1,254 200M rows 22.7s 33.2s 55.9s 1,261 300M rows 30.0s 55.4s 85.4s 1,259"},{"location":"benchmarks/#vs-mlxtend","title":"vs mlxtend","text":"<p>Comparing the <code>FPMiner</code> streaming accumulator against <code>mlxtend</code>'s standard pandas One-Hot Encoded pipeline:</p> Dataset <code>rusket</code> (prep + mine) <code>mlxtend</code> (prep + mine) Speedup 50k rows (10k txns, 100 items) 0.0 s 0.1 s ~5\u00d7 500k rows (50k txns, 500 items) 0.2 s 1.8 s ~9\u00d7 2M rows (500k txns, 2k items) 0.2 s 16.0 s 80\u00d7 <p>Note: <code>mlxtend</code> starts to struggle heavily with the One-Hot Encoding pandas <code>groupby/unstack</code> memory overhead at scale. <code>rusket</code>'s streaming <code>add_chunk</code> combined with <code>eclat</code> processes 2M rows in 0.2s flat.</p>"},{"location":"benchmarks/#the-auto-routine-algorithm","title":"The \"Auto\" Routine Algorithm","text":"<p><code>rusket.mine(method=\"auto\")</code> dynamically selects the algorithm that performs best based on the dataset density (Borgelt 2003 heuristic).</p> <ul> <li>Density &gt; 0.15 (Dense): Automatically routes to FP-Growth. Tree-traversal performs exceptionally well when most transactions contain a massive overlap of identical items.</li> <li>Density &lt; 0.15 (Sparse): Automatically routes to Eclat. On sparse data (like retail baskets), traversing an enormous tree is memory-intensive. Eclat directly uses hardware SIMD array-intersections (<code>popcnt</code>) on the TID-lists, resulting in massive speedups (often 5\u00d7 to 15\u00d7 faster on sparse arrays).</li> </ul>"},{"location":"benchmarks/#real-world-datasets","title":"Real-World Datasets","text":"<p>Datasets from andi611/Apriori-and-Eclat-Frequent-Itemset-Mining.</p> Dataset Transactions Items <code>rusket</code> <code>mlxtend</code> Speedup andi_data.txt 8,416 119 9.7 s (22.8M itemsets) TIMEOUT \ud83d\udca5 \u221e andi_data2.txt 540,455 2,603 7.9 s 16.2 s 2\u00d7 <p>Dense data</p> <p>On <code>andi_data.txt</code> (~23 items/basket), <code>mlxtend</code> can't finish in 60s. <code>rusket</code> mines 22.8M itemsets in under 10s.</p>"},{"location":"benchmarks/#the-power-user-pipeline","title":"The Power-User Pipeline","text":"<p>For maximum performance at 100M+ scale, skip pandas entirely:</p> <pre><code>import numpy as np\nfrom scipy import sparse as sp\nfrom rusket import fpgrowth\n\n# Your data: integer arrays of (txn_id, item_id)\ntxn_ids = np.array([...], dtype=np.int64)  # transaction IDs\nitem_ids = np.array([...], dtype=np.int32)  # item IDs\n\n# Build CSR directly (fast!)\ncsr = sp.csr_matrix(\n    (np.ones(len(txn_ids), dtype=np.int8), (txn_ids, item_ids)),\n    shape=(n_transactions, n_items),\n)\n\n# Mine directly from CSR \u2014 no pandas overhead\nfreq = fpgrowth(csr, min_support=0.001, use_colnames=True,\n                max_len=3, column_names=item_names)\n</code></pre> <p>At 100M rows, the mining step takes 1.2 seconds (not a typo).</p>"},{"location":"benchmarks/#conquering-the-1-billion-row-challenge","title":"\ud83c\udfc6 Conquering the 1 Billion Row Challenge","text":"<p>Scaling to 1,000,000,000 transactions presents three distinct software engineering bottlenecks. <code>rusket</code> was specifically architected to eliminate all three of them.</p>"},{"location":"benchmarks/#bottleneck-1-memory-exhaustion-during-ingestion","title":"Bottleneck 1: Memory Exhaustion during Ingestion","text":"<p>If you attempt to load 1 Billion rows into Pandas or construct a sparse CSR matrix in memory, Python will crash with an Object Memory Error (OOM) well before the mining phase even begins. The Solution: The <code>FPMiner</code> class provides an out-of-core streaming API. It accepts chunks of <code>(txn_id, item_id)</code> pairs, performs a fast $O(k \\log k)$ sort in Rust, buffers them, and uses a k-way merge to stream directly into the final compressed CSR memory block\u2014guaranteeing peak memory overhead remains strictly identical to the final object size.</p>"},{"location":"benchmarks/#bottleneck-2-algorithmic-memory-thrashing","title":"Bottleneck 2: Algorithmic Memory Thrashing","text":"<p>Traditional Eclat architectures allocate a new <code>BitSet</code> ($&gt;100\\text{ MB}$ at 1B scale) for every single item pair intersection, evaluating the support count, and discarding the allocation if it falls below the threshold. Across millions of recursive combinations, this obliterates the allocator and memory bandwidth. The Solution: <code>rusket</code> employs a zero-allocation <code>intersect_count_into()</code> kernel. It pre-allocates a thread-local scratch buffer, intersected in-place. Crucially, it tracks the running popcount and utilizes an early-exit heuristic\u2014aborting the memory scan the exact moment it proves the remaining bits mathematically cannot satisfy the <code>min_support</code> threshold.</p>"},{"location":"benchmarks/#bottleneck-3-sequential-seriality","title":"Bottleneck 3: Sequential Seriality","text":"<p>FP-Growth typically shines on dense data, but building millions of conditional FP-trees creates a massive sequential bottleneck before parallel processing can begin. The Solution: <code>rusket</code> merges tree construction into the parallel worker loop. Conditional trees are collected and mined concurrently inside the rayon thread pool, replacing the standard serial loop and eliminating the master thread bottleneck.</p>"},{"location":"benchmarks/#the-1-billion-row-architecture","title":"\ud83c\udfd7 The 1 Billion Row Architecture","text":"<p>To pass the \"1 Billion Row\" threshold without OOM crashes, <code>rusket</code> employs a zero-allocation mining loop:</p> <ol> <li>Eclat Scratch Buffers: <code>intersect_count_into</code> writes intersections directly into thread-local pre-allocated memory bytes and computes <code>popcnt</code> in a single pass. It implements early-exit loop termination the moment it proves a combination cannot reach <code>min_support</code>.</li> <li>FPGrowth Parallel Tree Build: Conditional FP-trees are collected concurrently inside the rayon parallel mining step, replacing the standard sequential loop and eliminating memory contention bottlenecks.</li> <li><code>AHashMap</code> Deduplication: Extremely fast O(N) duplicate basket counting replaces standard O(N log N) unstable sorts in the core pipeline.</li> </ol>"},{"location":"benchmarks/#running-the-benchmarks","title":"Running the benchmarks","text":"<pre><code>uv run maturin develop --release\nuv run python benchmarks/bench_scale.py    # Scale benchmark + Plotly chart\nuv run python benchmarks/bench_realworld.py  # Real-world datasets\nuv run python benchmarks/bench_vs_mlxtend.py # FPMiner streaming vs mlxtend\nuv run pytest tests/test_benchmark.py -v -s  # pytest-benchmark\n</code></pre>"},{"location":"benchmarks/#why-is-rusket-faster","title":"Why is rusket faster?","text":"Technique Description Zero-copy CSR <code>indptr</code>/<code>indices</code> passed to Rust as pointer hand-offs Arena FP-Tree Flat children arena, incremental <code>is_path()</code> tracking Rayon Parallel conditional mining across CPU cores Eclat popcount <code>Vec&lt;u64&gt;</code> bitsets + hardware <code>popcnt</code> for support No Python loops FP-Tree, mining, and metrics all in Rust <code>pd.factorize</code> O(n) integer encoding, faster than <code>pd.Categorical</code> at scale"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes are documented here. This project follows Semantic Versioning.</p>"},{"location":"changelog/#style","title":"Style","text":"<ul> <li>run ruff format</li> </ul>"},{"location":"changelog/#style_1","title":"Style","text":"<ul> <li>apply ruff formatting and fixes</li> <li>Update logo colors from purple to orange.</li> <li>refine logos with orange theme, update mkdocs palette and extra.css</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>resolve PySpark ChunkedArray fallback warning and implement BPR fit_transactions</li> <li>fix pyright errors reported on ci</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add Polars/PySpark PrefixSpan tests and cookbook examples</li> <li>improve API documentation, update marketing copy, and setup PySpark skips</li> <li>enhance PrefixSpan and HUPM cookbook sections with clearer descriptions, business scenarios, and updated Python code examples.</li> </ul>"},{"location":"changelog/#miscellaneous","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>commit remaining unstaged files from previous sessions</li> <li>bump version to 0.1.21</li> <li>bump version to 0.1.22</li> <li>bump version to 0.1.23</li> </ul>"},{"location":"changelog/#refactoring","title":"\ud83d\udd27 Refactoring","text":"<ul> <li>simplify BaseModel and remove implicit recommender duplication</li> <li>update logo SVG basket elements to use curved paths and refined wire details.</li> </ul>"},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>core algorithms via Faer, HUPM, Arrow Streams, and Hybrid Recommender</li> <li>complete PySpark and Polars integration for PrefixSpan via native PyArrow sequences</li> <li>implement recommend_items for association rule models</li> <li>Introduce new documentation notebooks, update PySpark integration documentation, and add a notebook conversion workflow.</li> <li>automated doc sync scripts (changelog, API ref, llm.txt)</li> <li>enhance recommender system documentation and examples, update core logic, and refresh logos.</li> <li>merge feature/fpgrowth-mlxtend-api</li> </ul>"},{"location":"changelog/#performance","title":"\u26a1 Performance","text":"<ul> <li>Boost FPGrowth performance with a new architecture, update benchmarks and documentation, add new logos, and remove temporary test files.\"</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>skip mlxtend comparison at &gt;1M rows to prevent CI timeout</li> </ul>"},{"location":"changelog/#documentation_1","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add genai and lancedb integration examples to cookbook</li> <li>add cookbook examples for ALS PCA visualization and Spark MLlib translation</li> <li>conquer 1 billion row challenge architecture and bump v0.1.20</li> </ul>"},{"location":"changelog/#cicd","title":"\ud83d\udd04 CI/CD","text":"<ul> <li>trigger Deploy Docs on benchmarks/** changes too</li> </ul>"},{"location":"changelog/#refactoring_1","title":"\ud83d\udd27 Refactoring","text":"<ul> <li>clean Python layer \u2014 remove stale timing vars, dead code, AI-slop comments</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Loosen numerical tolerance for parallel Hogwild! BPR test to fix CI</li> </ul>"},{"location":"changelog/#documentation_2","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>use relative path for logo in README</li> </ul>"},{"location":"changelog/#documentation_3","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Comprehensive Interactive Cookbook with Real-World Datasets</li> </ul>"},{"location":"changelog/#bench","title":"Bench","text":"<ul> <li>add Cholesky to ALS benchmark script and fix pyright</li> </ul>"},{"location":"changelog/#documentation_4","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>feature rusket.mine as the primary public api endpoint across mkdocs and readme</li> <li>append comprehensive cookbook examples for prefixspan, hupm, bpr, similarity, and recommender modules</li> </ul>"},{"location":"changelog/#miscellaneous_1","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>safe checkpoint</li> </ul>"},{"location":"changelog/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>add method='auto' routing to dynamically select eclat or fpgrowth based on dataset density</li> </ul>"},{"location":"changelog/#features_2","title":"\ud83d\ude80 Features","text":"<ul> <li>YOLO release v0.1.16</li> </ul>"},{"location":"changelog/#performance_1","title":"\u26a1 Performance","text":"<ul> <li>implement rayon multi-threading for FPMiner chunk ingestion</li> <li>revert SmallVec regression, clean HashMap FPMiner + scale to 1B benchmark</li> <li>item pre-filter + with_capacity hint in FPMiner</li> <li>fix freq-sort to ascending (Eclat-optimal: least-frequent items first)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>pyright unbound variables correctly initialized</li> <li>pyright complaints about unbound variables and missing als_fit_implicit argument</li> <li>benchmark now uses 8GB in-memory limit instead of disk-spilling at scale</li> <li>streaming.py cleanup + als_fit_implicit cg_iters stub + psutil available RAM strategy</li> <li>batched mining at 250M rows per batch to avoid OOM at 800M+</li> <li>SCALE_TARGETS scoping + launch 1B Eclat scale-up</li> <li>restore SEP in benchmark f-strings</li> </ul>"},{"location":"changelog/#documentation_5","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add FPMiner out-of-core streaming section and 300M benchmark</li> <li>add ALS feature and market basket analysis to README</li> </ul>"},{"location":"changelog/#features_3","title":"\ud83d\ude80 Features","text":"<ul> <li>add verbose mode to fpgrowth, eclat, and FPMiner for large-scale feedback</li> <li>implement hybrid memory/disk out-of-core FPMiner with dynamic RAM limit</li> <li>add verbose iteration timing + out-of-core 1B support</li> <li>comprehensive cookbook + ALS speed improvements</li> <li>HashMap FPMiner + creative benchmark (method \u00d7 chunk-size \u00d7 scale)</li> <li>frequency-sorted remap + mine_auto + hint_n_transactions (Borgelt 2003)</li> <li>Anderson Acceleration for ALS outer loop (anderson_m param)</li> </ul>"},{"location":"changelog/#features_4","title":"\ud83d\ude80 Features","text":"<ul> <li>FPMiner streaming accumulator v0.1.14</li> </ul>"},{"location":"changelog/#features_5","title":"\ud83d\ude80 Features","text":"<ul> <li>direct scipy CSR support in fpgrowth/eclat + pd.factorize + scale benchmarks</li> </ul>"},{"location":"changelog/#features_6","title":"\ud83d\ude80 Features","text":"<ul> <li>automated scale benchmark with Plotly chart (1M-500M rows)</li> </ul>"},{"location":"changelog/#features_7","title":"\ud83d\ude80 Features","text":"<ul> <li>sparse CSR from_transactions + million-scale benchmarks (66\u00d7 faster)</li> </ul>"},{"location":"changelog/#bench_1","title":"Bench","text":"<ul> <li>add real-world dataset benchmark (auto-downloads, with timeouts)</li> </ul>"},{"location":"changelog/#documentation_6","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add Eclat API, real-world benchmarks, and usage examples</li> </ul>"},{"location":"changelog/#features_8","title":"\ud83d\ude80 Features","text":"<ul> <li>add from_transactions, from_pandas, from_polars, from_spark helpers</li> </ul>"},{"location":"changelog/#test","title":"Test","text":"<ul> <li>add dedicated test_eclat.py for standalone eclat() function</li> </ul>"},{"location":"changelog/#performance_2","title":"\u26a1 Performance","text":"<ul> <li>arena-based FPNode with flat children storage (7.8x speedup)</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>add readme and license to pyproject.toml for PyPI, bump to 0.1.9</li> </ul>"},{"location":"changelog/#features_9","title":"\ud83d\ude80 Features","text":"<ul> <li>add Eclat algorithm (method='eclat') with 2.4-2.8x speedup on sparse data</li> <li>make eclat the default method (faster in all benchmarks)</li> <li>expose eclat() as standalone public function</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>remove orphaned FPGrowth import after FP-TDA removal</li> </ul>"},{"location":"changelog/#miscellaneous_2","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>remove FP-TDA implementation</li> <li>add MIT license</li> <li>add dependabot.yml to match httprx structure</li> </ul>"},{"location":"changelog/#features_10","title":"\ud83d\ude80 Features","text":"<ul> <li>implement zero-copy slice algorithm for FP-TDA</li> </ul>"},{"location":"changelog/#miscellaneous_3","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>remove tracked pycache / .pyc files</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>remove target-cpu=native from .cargo/config.toml to fix CI SIGILL crashes</li> <li>exclude test_benchmark.py from regular pytest run to prevent mlxtend timeouts</li> <li>increase CI timeout to 45min for slow free-threaded Python builds</li> <li>benchmark CI - conditional baseline compare + PyPI trusted publishing (OIDC)</li> <li>fptda iterative mining to avoid stack overflow on sparse data</li> </ul>"},{"location":"changelog/#documentation_7","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>compact logo, remove fast pattern mining subtitle</li> </ul>"},{"location":"changelog/#miscellaneous_4","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>merge feat/regression-benchmarks into main</li> <li>bump version to 0.1.5</li> </ul>"},{"location":"changelog/#refactoring_2","title":"\ud83d\udd27 Refactoring","text":"<ul> <li>extract FPBase, add FPTda class, FP-TDA in benchmarks</li> </ul>"},{"location":"changelog/#features_11","title":"\ud83d\ude80 Features","text":"<ul> <li>regression benchmark tests + fix warnings</li> <li>add FP-TDA algorithm (IJISRT25NOV1256)\\n\\nImplements the Frequent-Pattern Two-Dimensional Array algorithm as a\\ndrop-in alternative to FP-Growth. Uses right-to-left column projection\\non sorted transaction lists instead of conditional subtree construction.\\n\\n- src/fptda.rs: Rust core (fptda_from_dense / fptda_from_csr)\\n- rusket/fptda.py: Python wrapper, identical API to fpgrowth()\\n- rusket/init.py: export rusket.fptda\\n- tests/test_fptda.py: 22 tests (mix-ins + cross-check vs fpgrowth)\\n- src/fpgrowth.rs: made process_item_counts/flatten_results pub(crate)\\n- src/lib.rs: register new pyfunction bindings</li> </ul>"},{"location":"changelog/#style_2","title":"Style","text":"<ul> <li>apply ruff format and fix lint errors</li> </ul>"},{"location":"changelog/#bug-fixes_7","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>remove tracked site/ dir, rename fpgrowth-pyo3\u2192rusket, fix docs workflow</li> </ul>"},{"location":"changelog/#documentation_8","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add CI/CD workflow guidance to AGENTS.md</li> <li>publish real benchmark numbers with Plotly interactive chart</li> <li>add GitHub Pages enable step to AGENTS.md</li> <li>replace cookbook notebook with clean markdown, simplify docs workflow</li> <li>add YOLO section to AGENTS.md; merge feat/regression-benchmarks</li> </ul>"},{"location":"changelog/#features_12","title":"\ud83d\ude80 Features","text":"<ul> <li>add benchmark against efficient-apriori</li> <li>Bump version to 0.1.3, refine FPGrowth Arrow data type handling, update dependencies, and refactor test and project files.</li> </ul>"},{"location":"changelog/#bug-fixes_8","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>add mkdocs-jupyter dependency for github pages</li> </ul>"},{"location":"changelog/#miscellaneous_5","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>fix docs deployment and format readme</li> </ul>"},{"location":"changelog/#performance_3","title":"\u26a1 Performance","text":"<ul> <li>zero-copy pyarrow backend implementation</li> </ul>"},{"location":"changelog/#bug-fixes_9","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>resolve SIGABRT panic in fpgrowth.rs and restore missing validation checks in python port</li> </ul>"},{"location":"changelog/#documentation_9","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>add comprehensive Jupyter cookbook with Plotly graphs and benchmark results</li> <li>add pyarrow zero-copy dataframe slicing examples</li> </ul>"},{"location":"changelog/#miscellaneous_6","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>add pytest-timeout to dev dependencies</li> <li>bump version to 0.1.1</li> </ul>"},{"location":"changelog/#documentation_10","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>emphasize ultimate blazing speed in README</li> </ul>"},{"location":"changelog/#miscellaneous_7","title":"\ud83d\udce6 Miscellaneous","text":"<ul> <li>add maturin and pyright to dev dependencies for CI</li> </ul>"},{"location":"changelog/#cicd_1","title":"\ud83d\udd04 CI/CD","text":"<ul> <li>configure automated pypi release and github tags workflow</li> </ul>"},{"location":"changelog/#features_13","title":"\ud83d\ude80 Features","text":"<ul> <li>optimised FP-Growth (mimalloc + SmallVec + PAR_ITEMS_CUTOFF=4 + parallel freq count + dedup)</li> </ul>"},{"location":"cookbook/","title":"Rusket Cookbook","text":"<p>A hands-on guide to every feature in <code>rusket</code> \u2014 from market basket analysis to billion-scale collaborative filtering.</p>"},{"location":"cookbook/#setup","title":"Setup","text":"<pre><code>pip install rusket\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport polars as pl\nfrom rusket import mine, eclat, association_rules, ALS, BPR\nfrom rusket import prefixspan, sequences_from_event_log, hupm, similar_items, Recommender, score_potential\n</code></pre>"},{"location":"cookbook/#1-market-basket-analysis-grocery-retail","title":"1. Market Basket Analysis \u2014 Grocery Retail","text":""},{"location":"cookbook/#business-context","title":"Business context","text":"<p>A supermarket chain wants to identify which product combinations appear most frequently in customer baskets across all checkout terminals. The output drives:</p> <ul> <li>\"Frequently Bought Together\" widgets on the self-checkout screen</li> <li>Shelf adjacency decisions (place high-lift pairs closer together)</li> <li>Promotional bundles (discount pairs with high confidence but low current margin)</li> </ul>"},{"location":"cookbook/#prepare-the-basket-data","title":"Prepare the basket data","text":"<p>In practice this comes from your POS system as a long-format order table. For demonstration we build a plausible synthetic dataset:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom rusket import from_transactions\n\nnp.random.seed(42)\n\n# Realistic grocery catalogue with purchase probabilities (power-law distributed)\ncategories = {\n    \"Milk\": 0.55, \"Bread\": 0.52, \"Butter\": 0.36, \"Eggs\": 0.41,\n    \"Cheese\": 0.28, \"Yogurt\": 0.22, \"Coffee\": 0.31, \"Tea\": 0.18,\n    \"Sugar\": 0.20, \"Apples\": 0.25, \"Bananas\": 0.30, \"Oranges\": 0.15,\n    \"Chicken\": 0.35, \"Pasta\": 0.27, \"Tomato Sauce\": 0.26, \"Onions\": 0.40,\n}\n\nn_receipts = 10_000\ndf_long = pd.DataFrame(\n    [(receipt, product)\n     for receipt in range(n_receipts)\n     for product, prob in categories.items()\n     if np.random.rand() &lt; prob],\n    columns=[\"receipt_id\", \"product\"],\n)\nprint(f\"Simulated {n_receipts:,} receipts, {len(df_long):,} line items\")\n\n# Convert to one-hot basket matrix\nbasket = from_transactions(df_long, transaction_col=\"receipt_id\", item_col=\"product\")\n</code></pre>"},{"location":"cookbook/#find-frequent-product-combinations","title":"Find frequent product combinations","text":"<pre><code>from rusket import mine\n\n# Keep combinations appearing in \u22655% of receipts\nfreq = mine(basket, min_support=0.05, use_colnames=True)\nprint(f\"Found {len(freq):,} frequent product combinations\")\n\ntop_combos = freq.sort_values(\"support\", ascending=False)\nprint(top_combos.head(10))\n# e.g. (Milk,) 55%, (Bread,) 52%, (Milk, Bread) 28% ...\n</code></pre>"},{"location":"cookbook/#generate-cross-sell-rules","title":"Generate cross-sell rules","text":"<pre><code>from rusket import association_rules\n\nrules = association_rules(freq, num_itemsets=n_receipts, min_threshold=0.3)\n\n# Filter for actionable rules: high confidence AND lift (better than random)\nactionable = rules[(rules[\"confidence\"] &gt; 0.45) &amp; (rules[\"lift\"] &gt; 1.2)]\nprint(actionable.sort_values(\"lift\", ascending=False).head(10))\n# antecedents     consequents   confidence  lift\n# (Butter,)       (Bread,)      0.72        1.38   \u2192 72% of Butter buyers also buy Bread\n</code></pre>"},{"location":"cookbook/#limit-itemset-length-for-large-catalogues","title":"Limit itemset length for large catalogues","text":"<pre><code># For a full supermarket with 5000 SKUs: cap at pairs and triples to avoid explosion\nfreq_pairs = mine(basket, min_support=0.02, max_len=2, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#2-eclat-when-to-use-vs-fpgrowth","title":"2. ECLAT \u2014 When to Use vs FPGrowth","text":"<p>ECLAT uses a vertical bitset representation. It is faster than FPGrowth for sparse datasets with many unique items and a low support threshold.</p> <pre><code>fi_ec = eclat(df, min_support=0.05, use_colnames=True)\n</code></pre> <p>Rule of thumb:</p> Condition Recommended algorithm Dense dataset, few items <code>mine(method=\"auto\")</code> Sparse dataset, many items, low support <code>mine(method=\"auto\")</code> Very large dataset (100M+ rows) <code>FPMiner</code> with streaming"},{"location":"cookbook/#3-transaction-helpers","title":"3. Transaction Helpers","text":"<p>Convert long-format order data (e.g., from a database) to the one-hot boolean matrix format required by <code>mine</code> (which automatically routes to <code>fpgrowth</code> or <code>eclat</code>).</p>"},{"location":"cookbook/#from-a-pandas-dataframe","title":"From a Pandas DataFrame","text":"<pre><code>from rusket import from_transactions\n\norders = pd.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\n# Converts long-format \u2192 wide boolean matrix\nbasket = from_transactions(orders, user_col=\"order_id\", item_col=\"item\")\nfi = mine(basket, min_support=0.3, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#from-a-polars-dataframe","title":"From a Polars DataFrame","text":"<pre><code>orders_pl = pl.DataFrame({\n    \"order_id\": [1, 1, 1, 2, 2, 3],\n    \"item\":     [\"Milk\", \"Bread\", \"Eggs\", \"Milk\", \"Butter\", \"Eggs\"],\n})\n\nbasket = from_transactions(orders_pl, user_col=\"order_id\", item_col=\"item\")\n</code></pre>"},{"location":"cookbook/#from-a-spark-dataframe","title":"From a Spark DataFrame","text":"<pre><code># Works with PySpark DataFrames via .toPandas() under the hood\nbasket = from_transactions(spark_df, user_col=\"order_id\", item_col=\"item\")\n</code></pre>"},{"location":"cookbook/#4-collaborative-filtering-with-als-for-you-personalisation","title":"4. Collaborative Filtering with ALS \u2014 \"For You\" Personalisation","text":"<p><code>ALS</code> (Alternating Least Squares) learns latent user and item embeddings from implicit feedback (purchases, clicks, plays) and enables personalised \"For You\" recommendations.</p>"},{"location":"cookbook/#business-context_1","title":"Business context","text":"<p>An e-commerce platform wants to show a personalised homepage to each logged-in user. ALS learns from past purchase history which categories of products each user affinity group prefers \u2014 without any explicit ratings.</p>"},{"location":"cookbook/#fit-from-purchase-history-event-log","title":"Fit from purchase history (event log)","text":"<pre><code>from rusket import ALS\n\n# Purchase history from your order management system\npurchases = pd.DataFrame({\n    \"customer_id\": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1003],\n    \"sku\":         [\"A10\", \"B22\", \"C15\",  \"A10\", \"D33\",  \"B22\", \"C15\", \"E07\"],\n    \"revenue\":     [29.99, 49.00, 9.99,  29.99, 15.00, 49.00, 9.99, 22.00],\n})\n\n# revenue used as confidence weight (alpha scaling)\nmodel = ALS(factors=64, iterations=15, alpha=40.0, cg_iters=3, verbose=True)\nmodel = ALS.from_transactions(\n    purchases,\n    transaction_col=\"customer_id\",\n    item_col=\"sku\",\n    rating_col=\"revenue\",\n)\n</code></pre>"},{"location":"cookbook/#get-personalised-recommendations","title":"Get personalised recommendations","text":"<pre><code># Top-5 SKUs for customer 1002, excluding already-purchased items\nskus, scores = model.recommend_items(user_id=1002, n=5, exclude_seen=True)\nprint(f\"Recommended SKUs for customer 1002: {skus}\")\n\n# Target: which customers should receive a promo for SKU B22 (high-margin item)?\ntop_customers, scores = model.recommend_users(item_id=\"B22\", n=100)\nprint(f\"Top customers to target with B22 promo: {top_customers[:5]}\")\n</code></pre>"},{"location":"cookbook/#fit-from-transaction-data","title":"Fit from transaction data","text":"<pre><code># If you have pre-built purchase integers from your warehouse:\nmodel2 = ALS(factors=32, iterations=10, verbose=True)\nmodel2 = ALS.from_transactions(purchases, transaction_col=\"customer_id\", item_col=\"sku\")\n</code></pre>"},{"location":"cookbook/#access-latent-factors-directly","title":"Access latent factors directly","text":"<pre><code># NumPy arrays (n_users \u00d7 factors) and (n_items \u00d7 factors)\nprint(model.user_factors.shape)  # (10000, 64)\nprint(model.item_factors.shape)  # (5000, 64)\n</code></pre>"},{"location":"cookbook/#5-out-of-core-als-for-1b-ratings","title":"5. Out-of-Core ALS for 1B+ Ratings","text":"<p>When the interaction matrix exceeds available RAM, use the out-of-core streaming loader. The CSR matrix is stored on SSD and the OS pages data into RAM on demand.</p>"},{"location":"cookbook/#download-and-prepare-the-movielens-1b-dataset","title":"Download and prepare the MovieLens 1B dataset","text":"<pre><code>import urllib.request, tarfile, os\n\n# Download (~1.4 GB)\nurl = \"https://files.grouplens.org/datasets/movielens/ml-20mx16x32.tar\"\nurllib.request.urlretrieve(url, \"ml-1b.tar\")\n\nwith tarfile.open(\"ml-1b.tar\") as t:\n    t.extractall(\"data/ml-1b/\")\n</code></pre>"},{"location":"cookbook/#build-the-out-of-core-csr-matrix","title":"Build the out-of-core CSR matrix","text":"<pre><code>import numpy as np\nfrom scipy import sparse\nfrom pathlib import Path\n\ndata_dir = Path(\"data/ml-1b/ml-20mx16x32\")\nnpz_files = sorted(data_dir.glob(\"trainx*.npz\"))\n\n# Pass 1 \u2014 count ratings per user\nmax_user, max_item, nnz = 0, 0, 0\ncounts = np.zeros(100_000_000, dtype=np.int64)  # pre-allocate for max users\n\nfor f in npz_files:\n    arr = np.load(f)[\"arr_0\"]          # shape (N, 2) \u2014 [user_id, item_id]\n    uids, iids = arr[:, 0], arr[:, 1]\n    max_user = max(max_user, int(uids.max()))\n    max_item = max(max_item, int(iids.max()))\n    chunk_counts = np.bincount(uids, minlength=max_user + 1)\n    counts[:len(chunk_counts)] += chunk_counts\n    nnz += len(uids)\n\nn_users, n_items = max_user + 1, max_item + 1\nindptr = np.zeros(n_users + 1, dtype=np.int64)\nnp.cumsum(counts[:n_users], out=indptr[1:])\n\n# Pass 2 \u2014 write indices/data to SSD memory maps\nmmap_indices = np.memmap(\"indices.mmap\", dtype=np.int32, mode=\"w+\", shape=(nnz,))\nmmap_data    = np.memmap(\"data.mmap\",    dtype=np.float32, mode=\"w+\", shape=(nnz,))\npos = indptr[:-1].copy()\n\nfor f in npz_files:\n    arr = np.load(f)[\"arr_0\"]\n    uids, iids = arr[:, 0].astype(np.int64), arr[:, 1].astype(np.int32)\n    for u, i in zip(uids, iids):\n        p = pos[u]\n        mmap_indices[p] = i\n        mmap_data[p]    = 1.0\n        pos[u] += 1\n\nmmap_indices.flush()\nmmap_data.flush()\n</code></pre>"},{"location":"cookbook/#fit-als-on-the-out-of-core-matrix","title":"Fit ALS on the out-of-core matrix","text":"<pre><code># Bypass scipy's int32 limits by direct property assignment\nmat = sparse.csr_matrix((n_users, n_items))\nmat.indptr  = indptr\nmat.indices = mmap_indices\nmat.data    = mmap_data\n\nmodel = ALS(\n    factors=64,\n    iterations=5,      # fewer iterations for 1B \u2014 each takes hours on SSD\n    alpha=40.0,\n    verbose=True,\n    cg_iters=3,\n)\nmodel.fit(mat)\n</code></pre> <p>Hardware sizing</p> <p>On a machine with \u2265 32 GB RAM the mmap working set stays hot in OS page cache and each iteration completes in ~5 minutes. On 8 GB RAM each iteration is disk-bound and takes hours.</p>"},{"location":"cookbook/#6-bayesian-personalized-ranking-bpr","title":"6. Bayesian Personalized Ranking (BPR)","text":"<p>Unlike ALS which tries to reconstruct the full interaction matrix, BPR explicitly optimizes the model to rank positive observed items higher than unobserved items. This makes BPR excellent for top-N ranking tasks on implicit data (like clicks or views).</p> <pre><code>from rusket import BPR\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\n# Prepare sparse interaction matrix\nrows = np.random.randint(0, 1000, size=5000)\ncols = np.random.randint(0, 500, size=5000)\nmat = csr_matrix((np.ones(5000), (rows, cols)), shape=(1000, 500))\n\n# Initialize and fit BPR with Hogwild! parallel SGD\nmodel = BPR(\n    factors=64,\n    learning_rate=0.01,\n    regularization=0.01,\n    iterations=100,\n    seed=42,\n)\nmodel.fit(mat)\n\n# Recommend items just like ALS\nitems, scores = model.recommend_items(user_id=10, n=5)\n</code></pre>"},{"location":"cookbook/#7-sequential-pattern-mining-prefixspan","title":"7. Sequential Pattern Mining (PrefixSpan)","text":"<p>PrefixSpan discovers frequent sequences of events over time. Unlike standard market basket analysis (which looks at what is bought together), PrefixSpan finds patterns across ordered events \u2014 ideal for customer journey analysis, funnel optimisation, and churn prediction.</p> <p>Business scenario: A SaaS company wants to understand which product page navigation sequences lead customers to checkout. Which paths are most common? Where do users drop off?</p> <pre><code>import pandas as pd\nfrom rusket import prefixspan, sequences_from_event_log\n\n# 1. Website clickstream log \u2014 each row is one page visit\nclickstream = pd.DataFrame({\n    \"session_id\": [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n    \"timestamp\":  [10, 20, 30, 15, 25, 5, 15, 35, 10, 18, 40],\n    \"page\": [\n        \"Home\", \"Pricing\", \"Checkout\",\n        \"Home\", \"Pricing\",\n        \"Features\", \"Pricing\", \"Checkout\",\n        \"Home\", \"Features\", \"Checkout\",\n    ],\n})\n\n# 2. Convert to sequence format expected by the Rust miner\nseqs, mapping = sequences_from_event_log(\n    clickstream, user_col=\"session_id\", time_col=\"timestamp\", item_col=\"page\"\n)\n\n# 3. Mine navigation sequences seen in \u22652 sessions (absolute count)\npatterns_df = prefixspan(seqs, min_support=2, max_len=4)\n\n# 4. Map integer IDs back to readable page names\npatterns_df[\"path\"] = patterns_df[\"sequence\"].apply(\n    lambda seq: \" \u2192 \".join(mapping[s] for s in seq)\n)\nprint(patterns_df[[\"support\", \"path\"]].sort_values(\"support\", ascending=False).head(10))\n# support  path\n# 3        Home\n# 3        Pricing\n# 3        Checkout\n# 3        Home \u2192 Pricing\n# 3        Pricing \u2192 Checkout\n# 2        Home \u2192 Checkout                  \u2190 users who skipped Pricing\n# 2        Features \u2192 Pricing \u2192 Checkout   \u2190 high-intent funnel path\n</code></pre>"},{"location":"cookbook/#8-high-utility-pattern-mining-hupm-profit-driven-bundle-discovery","title":"8. High-Utility Pattern Mining (HUPM) \u2014 Profit-Driven Bundle Discovery","text":"<p>Frequent itemsets aren't always the most profitable. HUPM accounts for the utility (e.g., margin, revenue, quantity \u00d7 price) of items to find sets that generate the highest total business value \u2014 even if they appear infrequently.</p> <p>Business scenario: A wine shop wants to identify high-margin product bundles for a \"Sommelier's Selection\" gift box. Standard FP-Growth would surface budget items (e.g., sparkling water) because they're bought often. HUPM surfaces the highest-revenue product combinations instead:</p> <pre><code>from rusket import HUPM\n\n# Receipt data from the EPOS system \u2014 product_id and margin per line item\nimport pandas as pd\n\nreceipts = pd.DataFrame({\n    \"receipt_id\": [1, 1, 1, 2, 2, 3, 3, 4, 4, 4],\n    \"product\":    [\"champagne\", \"foie_gras\", \"truffle_oil\",\n                   \"champagne\", \"truffle_oil\",\n                   \"foie_gras\", \"truffle_oil\",\n                   \"champagne\", \"foie_gras\", \"truffle_oil\"],\n    \"margin\":     [18.50, 14.00, 8.00,   # receipt 1 margins\n                   18.50, 8.00,            # receipt 2\n                   14.00, 8.00,            # receipt 3\n                   18.50, 14.00, 8.00],    # receipt 4\n})\n\n# Discover all product bundles generating \u2265 \u20ac30 total margin\nhigh_value = HUPM.from_transactions(\n    receipts,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    utility_col=\"margin\",\n    min_utility=30.0,\n).mine()\n\nprint(high_value.sort_values(\"utility\", ascending=False))\n# utility   itemset\n# 122.0     [champagne, foie_gras, truffle_oil]  \u2190 ideal gift box\n# 74.0      [champagne, foie_gras]\n# 62.0      [champagne, truffle_oil]\n</code></pre>"},{"location":"cookbook/#9-native-polars-integration","title":"9. Native Polars Integration","text":"<p><code>rusket</code> returns itemsets as zero-copy PyArrow <code>ListArray</code> structures, making Polars interoperability very efficient.</p> <pre><code>df_pl = pl.from_pandas(df)\nfi_pl = mine(df_pl, min_support=0.05, use_colnames=True)\n\n# LazyFrame works too:\nlazy = df_pl.lazy()\n# (convert to eager first before passing to fpgrowth)\nfi_pl2 = mine(lazy.collect(), min_support=0.05, use_colnames=True)\n</code></pre>"},{"location":"cookbook/#query-itemsets-with-pyarrow-compute","title":"Query itemsets with PyArrow compute","text":"<pre><code>import pyarrow.compute as pc\n\n# Find itemsets containing \"Milk\" as first element\ncontains_milk = pc.list_element(fi[\"itemsets\"].array, 0) == \"Milk\"\nfi[contains_milk].head()\n</code></pre>"},{"location":"cookbook/#convert-to-python-sets-only-for-small-subsets","title":"Convert to Python sets (only for small subsets)","text":"<pre><code>top_10 = fi.head(10).copy()\ntop_10[\"sets\"] = top_10[\"itemsets\"].apply(set)\n</code></pre>"},{"location":"cookbook/#10-spark-databricks-integration","title":"10. Spark / Databricks Integration","text":"<p><code>rusket</code> provides native integration with PySpark out of the box, leaning heavily on Apache Arrow to completely bypass Python-to-JVM serialization and Pandas memory bloat.</p>"},{"location":"cookbook/#101-streaming-1b-rows-from-spark-zero-copy","title":"10.1 Streaming 1B+ Rows from Spark (Zero-Copy)","text":"<p>Instead of using <code>.toPandas()</code> (which will OOM driver nodes instantly on large tables), use <code>mine_spark</code>, which streams Arrow partitions dynamically.</p> <pre><code>from rusket import mine_spark\n\nspark_df = spark.table(\"silver_transactions\")\n\n# Streams Arrow RecordBatches directly to the Rust backend\nfrequent_itemsets = mine_spark(\n    spark_df, \n    n_items=500_000, \n    txn_col=\"transaction_id\", \n    item_col=\"product_id\", \n    min_support=0.001\n)\n</code></pre>"},{"location":"cookbook/#102-distributed-parallel-mining-grouped","title":"10.2 Distributed Parallel Mining (Grouped)","text":"<p>If you have multi-tenant data (e.g., you want to mine distinct association rules per region, store, or customer segment), you can distribute <code>rusket</code> across your entire Databricks cluster using PySpark's <code>applyInArrow</code>.</p> <pre><code>from rusket.spark import mine_grouped\n\nspark_df = spark.table(\"retail_transactions\")\n\n# Rusket maps the workload across executor nodes. Each node runs pure Rust \n# on its Spark partition and yields the regional itemsets back.\nregional_rules_df = mine_grouped(\n    spark_df, \n    group_col=\"store_id\", \n    min_support=0.05\n)\n\ndisplay(regional_rules_df)\n</code></pre>"},{"location":"cookbook/#103-collaborative-filtering-als-from-spark","title":"10.3 Collaborative Filtering (ALS) from Spark","text":"<p>For recommendation workloads, you can seamlessly feed Spark DataFrames containing <code>(user, item, rating)</code> triplets into ALS.</p> <pre><code>from rusket import ALS\n\nratings_spark = spark.table(\"implicit_ratings\") \nmodel = ALS(factors=64, iterations=10, verbose=True)\n\n# Coerces to Pandas internally for fit (only for tables that fit in driver RAM)\nmodel = ALS.from_transactions(\n    ratings_spark, \n    transaction_col=\"user_id\", \n    item_col=\"item_id\", \n    rating_col=\"clicks\",\n    factors=64,\n    iterations=10,\n    verbose=True\n)\n</code></pre> <p>Out-of-Core Models</p> <p>For Spark tables spanning &gt;100M rows, use <code>mine_spark</code> for Frequent Pattern mining, or export the table to an Out-of-Core disk map (Section 5) for ALS factorisation.</p>"},{"location":"cookbook/#11-tuning-guide","title":"11. Tuning Guide","text":""},{"location":"cookbook/#fpgrowth-eclat","title":"FPGrowth / ECLAT","text":"Parameter Default Effect <code>min_support</code> required Lower \u2192 more itemsets, slower <code>max_len</code> None Cap itemset size \u2014 huge speedup on large catalogs <code>use_colnames</code> False Return column names instead of indices"},{"location":"cookbook/#als","title":"ALS","text":"Parameter Default Notes <code>factors</code> 64 Higher \u2192 better quality, more RAM, slower <code>iterations</code> 15 5\u201315 is typical; diminishing returns after 20 <code>alpha</code> 40.0 Higher \u2192 stronger signal from implicit feedback <code>regularization</code> 0.01 Increase if overfitting; decrease for denser data <code>cg_iters</code> 3 CG solver steps per ALS step \u2014 3 is almost always optimal <code>verbose</code> False Set <code>True</code> to print per-iteration timing"},{"location":"cookbook/#bpr","title":"BPR","text":"Parameter Default Notes <code>factors</code> 64 Higher \u2192 better quality, more RAM. BPR requires more factors than ALS typically <code>iterations</code> 100 BPR uses SGD and requires more iterations than ALS. Try 100-500. <code>learning_rate</code> 0.01 SGD learning rate. Decrease if unstable, increase if slow convergence. <code>regularization</code> 0.01 Increase if overfitting"},{"location":"cookbook/#prefixspan-hupm","title":"PrefixSpan &amp; HUPM","text":"Parameter Default Notes <code>min_support</code> required Defines frequency for PrefixSpan or total value for HUPM <code>max_len</code> None Cap itemset/sequence size to avoid combinatorial explosions"},{"location":"cookbook/#recommendation-quality-tips","title":"Recommendation quality tips","text":"<ul> <li>Use <code>regularization=0.1</code> for very sparse matrices (&lt; 5 interactions/user)</li> <li><code>alpha=10</code> works better for rating-weighted data vs binary implicit feedback for ALS</li> <li>For top-N ranking optimization directly, use BPR instead of ALS. ALS is better for score prediction and serendipity.</li> <li>For the best cold-start handling, combine ALS/BPR with popularity-based fallback</li> <li>Lower <code>cg_iters</code> (e.g., 1\u20132) for faster but noisier convergence on huge ALS datasets</li> </ul>"},{"location":"cookbook/#9-sequential-pattern-mining-prefixspan","title":"9. Sequential Pattern Mining (PrefixSpan)","text":"<p>When the order of events matters (e.g., website navigation paths, sequential purchases over time), use PrefixSpan instead of FP-Growth.</p> <p><code>rusket</code> has native, out-of-the-box integration for routing sequence generation across Pandas, Polars, and PySpark <code>DataFrame</code>s. It utilizes lightning-fast internal grouped maps to feed the integers into the Rust parser.</p>"},{"location":"cookbook/#prepare-the-event-log-pandas-polars-or-spark","title":"Prepare the Event Log (Pandas, Polars, or Spark)","text":"<pre><code>from rusket import prefixspan, sequences_from_event_log\nimport pandas as pd\n\n# Let's say this is your PySpark, Polars, or Pandas DataFrame\nevents = pd.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 2, 3, 3, 3],\n    \"timestamp\": [\n        \"2024-01-01 10:00\", \"2024-01-01 10:05\", \"2024-01-01 10:10\",\n        \"2024-01-02 11:00\", \"2024-01-02 11:05\",\n        \"2024-01-03 09:00\", \"2024-01-03 09:05\", \"2024-01-03 09:10\"\n    ],\n    \"page_id\": [\"home\", \"products\", \"checkout\", \"home\", \"products\", \"home\", \"products\", \"checkout\"]\n})\n\n# Convert timestamp to datetime for correct sorting\nevents[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"])\n\n# Convert to sequence format required by prefixspan\n# Passes natively to Arrow if events is a PySpark or Polars dataframe!\nsequences, idx_to_item = sequences_from_event_log(\n    events, \n    user_col=\"user_id\", \n    time_col=\"timestamp\", \n    item_col=\"page_id\"\n)\n</code></pre>"},{"location":"cookbook/#mine-sequential-patterns","title":"Mine Sequential Patterns","text":"<pre><code># min_support is an absolute number of sequences\npatterns = prefixspan(sequences, min_support=2, max_len=3)\n\n# Map integer IDs back to original page names\npatterns[\"sequence_names\"] = patterns[\"sequence\"].apply(\n    lambda seq: [idx_to_item[idx] for seq]\n)\n\nprint(patterns[[\"support\", \"sequence_names\"]])\n#    support               sequence_names\n# 0        3                       [home]\n# 1        3                   [products]\n# 2        3             [home, products]\n# 3        2                   [checkout]\n# 4        2             [home, checkout]\n# 5        2         [products, checkout]\n# 6        2   [home, products, checkout]\n</code></pre>"},{"location":"cookbook/#10-high-utility-pattern-mining-hupm","title":"10. High-Utility Pattern Mining (HUPM)","text":"<p>Standard Frequent Itemset Mining (FP-Growth/Eclat) treats all items equally. HUPM considers the profit or utility of items, discovering itemsets that generate high total revenue even if they are bought infrequently.</p> <pre><code>from rusket import hupm\n\n# Item IDs bought\ntransactions = [\n    [1, 2, 3], \n    [1, 3],    \n    [2, 3]     \n]\n\n# Profit (or quantity * price) of each item in the respective transaction\nutilities = [\n    [5.0, 10.0, 2.0], # Transaction 1 profits\n    [5.0, 2.0],       # Transaction 2 profits\n    [10.0, 2.0]       # Transaction 3 profits\n]\n\n# Mine itemsets with at least 15.0 total utility\nhigh_utility_itemsets = hupm(transactions, utilities, min_utility=15.0, max_len=3)\nprint(high_utility_itemsets)\n#    utility    itemset\n# 0     20.0       [2]\n# 1     24.0    [2, 3]\n# 2     17.0 [1, 2, 3]\n</code></pre>"},{"location":"cookbook/#11-bayesian-personalized-ranking-bpr","title":"11. Bayesian Personalized Ranking (BPR)","text":"<p>BPR is a matrix factorization model that optimizes for ranking metrics rather than reconstruction error (like ALS). It works by sampling positive (interacted) and negative (unseen) items and ensuring the positive items are ranked higher. Use it when interaction data is purely binary implicit feedback.</p> <pre><code>from rusket import BPR\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\n# Create an implicit feedback matrix (users x items)\nmat = csr_matrix((np.ones(10), ([0,0,0,1,1,2,2,3,3,4], [1,3,4,1,2,2,3,1,4,4])), shape=(5, 5))\n\nmodel = BPR(\n    factors=32,\n    learning_rate=0.05,\n    iterations=200,\n    regularization=0.01,\n    seed=42\n)\n\n# Fit the BPR model\nmodel.fit(mat)\n\n# Recommend 3 items for user 0, excluding items they already interacted with\nitem_ids, scores = model.recommend_items(user_id=0, n=3, exclude_seen=True)\nprint(item_ids, scores)\n</code></pre>"},{"location":"cookbook/#12-item-similarity-and-cross-selling-potential","title":"12. Item Similarity and Cross-Selling Potential","text":"<p>Once you have fitted an ALS or BPR model, the learned latent factors are incredibly useful for measuring item similarity and predicting missed cross-sell opportunities.</p>"},{"location":"cookbook/#find-similar-products-item-to-item","title":"Find Similar Products (Item-to-Item)","text":"<pre><code>from rusket import similar_items\n\n# Given an ALS model fitted on purchases\nitem_ids, match_scores = similar_items(model, item_id=102, n=5)\n\nprint(f\"Items similar to {102}: {item_ids}\")\n# =&gt; Items similar to 102: [105, 99, 110, 87, 10]\n</code></pre>"},{"location":"cookbook/#calculate-cross-selling-potential","title":"Calculate Cross-Selling Potential","text":"<p>Identify the probability that a user should have bought an item by now, but hasn't (<code>score_potential</code>).</p> <pre><code>from rusket import score_potential\n\nuser_purchase_history = [\n    [0, 1, 5], # User 0 bought items 0, 1, 5\n    [1, 3],    # User 1 bought items 1, 3\n    [0]        # User 2 bought item 0\n]\n\n# Provide specific categories/items you want to cross-sell\ntarget_items = [2, 4, 6]\n\n# Matrix of shape (n_users, len(target_items))\nscores = score_potential(\n    user_purchase_history, \n    als_model=model, \n    target_categories=target_items\n)\n\n# The highest scores correspond to the users most primed to buy those specific targets\nprint(\"Cross-sell potential scores:\")\nprint(scores)\n</code></pre>"},{"location":"cookbook/#13-hybrid-recommender-als-association-rules","title":"13. Hybrid Recommender (ALS + Association Rules)","text":"<p>The <code>Recommender</code> workflow class wraps both your collaborative filtering models (ALS) and Frequent Pattern Mining rules into a single API. This easily enables the two most common placement strategies in e-commerce: \"For You\" (ALS) and \"Frequently Bought Together\" (Association Rules).</p> <pre><code>from rusket import Recommender\n\n# Initialize with both your fitted ALS model and Rules DataFrame\nrec = Recommender(als_model=model, rules_df=strong_rules)\n\n# 1. \"For You\" (Personalized cross-selling based on user history)\nitem_ids, scores = rec.recommend_for_user(user_id=125, n=5)\n\n# 2. \"Frequently Bought Together\" (Cart-based additions)\nactive_cart = [10, 15] # User just added items 10 and 15\nsuggested_additions = rec.recommend_for_cart(active_cart, n=3)\n</code></pre>"},{"location":"cookbook/#14-genai-llm-stack-integration","title":"14. GenAI / LLM Stack Integration","text":"<p><code>rusket</code> provides native utilities to export its learned representations and rules into the modern Generative AI and graph analytics stack.</p>"},{"location":"cookbook/#vector-export-vector-databases-lancedb","title":"Vector Export &amp; Vector Databases (LanceDB)","text":"<p>You can easily export ALS latent user or item factors as vector embeddings to power RAG (Retrieval-Augmented Generation) or fast semantic similarity search in vector databases like LanceDB, FAISS, or Qdrant.</p> <pre><code>import lancedb\nfrom rusket import export_item_factors\n\n# Export ALS item factors to a Pandas DataFrame\n# Returns columns: ['item_id', 'vector'] (and 'item_label' if available)\ndf_vectors = export_item_factors(als_model)\n\n# Connect to a local LanceDB instance\ndb = lancedb.connect(\"./lancedb\")\n\n# Ingest the embeddings into a table\ntable = db.create_table(\"item_embeddings\", data=df_vectors, mode=\"overwrite\")\n\n# Perform a vector similarity search (e.g., finding items similar to a given query embedding)\nquery_vector = df_vectors.iloc[0][\"vector\"]\nresults = table.search(query_vector).limit(5).to_pandas()\nprint(results)\n</code></pre>"},{"location":"cookbook/#fast-item-to-item-similarity","title":"Fast Item-to-Item Similarity","text":"<p>If you don't need a full vector database and just want fast, in-memory cosine similarity between items based on their ALS embeddings:</p> <pre><code>from rusket import similar_items\n\n# Find the top 5 most similar items to item_id=42 using Cosine Similarity\nsimilar_ids, similarity_scores = similar_items(als_model, item_id=42, n=5)\n\nprint(f\"Similar items: {similar_ids}\")\nprint(f\"Cosine similarities: {similarity_scores}\")\n</code></pre>"},{"location":"cookbook/#graph-generation-for-community-detection","title":"Graph Generation for Community Detection","text":"<p>Frequent Pattern Mining rules can be naturally represented as a directed graph. You can automatically convert them into a <code>networkx</code> graph to run community detection (like Louvain) and discover \"Product Clusters\" or \"Categories\".</p> <pre><code>import networkx as nx\nfrom rusket.viz import to_networkx\n\n# rules is a Pandas DataFrame from rusket.association_rules()\n# We use 'lift' as the edge weight connecting antecedents to consequents\nG = to_networkx(rules_df, source_col=\"antecedents\", target_col=\"consequents\", edge_attr=\"lift\")\n\n# Run basic graph analytics\nprint(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n\n# E.g., calculate PageRank to find the most influential products\ncentrality = nx.pagerank(G, weight='weight')\ntop_items = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]\nprint(\"Top central products:\", top_items)\n</code></pre>"},{"location":"cookbook/#15-visualizing-latent-spaces-pca","title":"15. Visualizing Latent Spaces (PCA)","text":"<p>When using <code>ALS</code>, raw embeddings capture both magnitude (how frequently an item is bought) and direction (the \"taste\" or behavioral profile of who buys it).</p> <p>To map these multidimensional factors down to a 3D Plotly visualization for dashboarding, we apply L2 Normalization (to focus solely on Cosine Similarity / direction) followed by PCA (Principal Component Analysis).</p> <p>Because <code>rusket</code> exposes ALS factors directly as NumPy arrays, you can do this without adding dependencies like <code>scikit-learn</code> or PySpark:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom rusket import ALS\n\n# 1. Load the Online Retail dataset\nurl = \"https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/refs/heads/master/data/retail-data/all/online-retail-dataset.csv\"\ndf_purchases = pd.read_csv(url)\ndf_purchases = df_purchases.dropna(subset=[\"CustomerID\", \"Description\"])\n\n# 2. Fit an ALS model\nmodel = ALS.from_transactions(df_purchases, transaction_col=\"CustomerID\", item_col=\"StockCode\", factors=64, iterations=15, alpha=40.0, seed=42)\n\n# 3. L2 Normalization (Unit Sphere Projection) using pure NumPy\n# Divide each latent factor row by its L2 norm (magnitude)\nitem_factors = model.item_factors\nitem_norms = np.linalg.norm(item_factors, axis=1, keepdims=True)\nitem_factors_norm = item_factors / np.clip(item_norms, a_min=1e-10, a_max=None)\n\n# 4. PCA Reduction (e.g. 64D -&gt; 3D) using Singular Value Decomposition\ndef compute_pca_3d(data):\n    # Mean centering\n    data_centered = data - np.mean(data, axis=0)\n\n    # SVD\n    U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)\n\n    # Extract the top 3 principal components map\n    components = Vt[:3]\n    return np.dot(data_centered, components.T)\n\nitem_pca = compute_pca_3d(item_factors_norm)\n\n# 5. Bind arrays back to a Pandas DataFrame for Plotly\ndf_viz = pd.DataFrame({\n    \"StockCode\": model._item_labels, # The original dataset IDs mapped back\n    \"pca_1\": item_pca[:, 0],\n    \"pca_2\": item_pca[:, 1],\n    \"pca_3\": item_pca[:, 2]\n})\n\n# Merge descriptions back in for hover labels\ndf_items = df_purchases[[\"StockCode\", \"Description\"]].drop_duplicates(\"StockCode\")\ndf_viz = df_viz.merge(df_items, on=\"StockCode\", how=\"inner\")\n\nfig = px.scatter_3d(\n    df_viz, x=\"pca_1\", y=\"pca_2\", z=\"pca_3\",\n    hover_name=\"Description\",\n    title=\"ALS Latent Space (3D PCA Mapping)\"\n)\nfig.update_traces(marker=dict(size=3, opacity=0.7))\nfig.show()\n</code></pre> <p>This workflow matches Spark MLlib's dimensionality reduction pipelines seamlessly while executing locally in microseconds.</p>"},{"location":"cookbook/#16-translating-spark-mllib-to-rusket","title":"16. Translating Spark MLlib to <code>rusket</code>","text":"<p>For users migrating from Databricks or PySpark, <code>rusket</code> offers a highly similar API without the distributed computing overhead. </p> <p>This example translates the famous Recommendation example from Chapter 28 of Spark: The Definitive Guide directly into <code>rusket</code> using pure Python and Pandas.</p>"},{"location":"cookbook/#spark-version-original","title":"Spark Version (Original)","text":"<pre><code>from pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nratings = spark.read.text(\"/data/sample_movielens_ratings.txt\") \\\n  .selectExpr(\"split(value, '::') as col\") \\\n  .selectExpr(\n      \"cast(col[0] as int) as userId\",\n      \"cast(col[1] as int) as movieId\",\n      \"cast(col[2] as float) as rating\"\n  )\n\ntraining, test = ratings.randomSplit([0.8, 0.2])\n\nals = ALS().setMaxIter(5).setRegParam(0.01) \\\n  .setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\n\nalsModel = als.fit(training)\npredictions = alsModel.transform(test)\n\nevaluator = RegressionEvaluator().setMetricName(\"rmse\") \\\n  .setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n\nprint(\"RMSE =\", evaluator.evaluate(predictions))\n</code></pre>"},{"location":"cookbook/#rusket-version-equivalent","title":"<code>rusket</code> Version (Equivalent)","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom rusket import ALS\n\n# 1. Load the data using Pandas\nurl = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt\"\nratings = pd.read_csv(url, sep=\"::\", engine=\"python\", \n                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n\n# 2. Random Split (80/20)\nshuffled = ratings.sample(frac=1.0, random_state=42)\nsplit_idx = int(len(shuffled) * 0.8)\ntraining = shuffled.iloc[:split_idx]\ntest = shuffled.iloc[split_idx:]\n\n# 3. Initialize and Fit the ALS Model\n# Note: rusket uses `factors` instead of `rank`, and `iterations` instead of `maxIter`.\nmodel = ALS.from_transactions(training, transaction_col=\"userId\", item_col=\"movieId\", rating_col=\"rating\", factors=10, iterations=5, regularization=0.01, seed=42)\n\n# 4. Generate Predictions for the test set\n# rusket has a built-in vectorized score_potential helper for evaluating target vectors\nfrom rusket.recommend import score_potential\n\n# We reconstruct the user's history from the training set to mask known interactions\nuser_histories = training.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n# Ensure all users in the test set exist in our history mapping, even if empty\nhistory_list = [user_histories.get(uid, []) for uid in range(model._n_users)]\n\n# Calculate raw prediction scores across all users and all items\nall_predictions = score_potential(history_list, model)\n\n# 5. Evaluate RMSE\n# Extract only the actual ratings we care about from the test set\ntest_users = test[\"userId\"].values\ntest_movies = test[\"movieId\"].values\nactual_ratings = test[\"rating\"].values\n\n# Map the raw pandas IDs to rusket's internal 0-indexed matrix IDs\ntry:\n    internal_user_ids = np.array([model._user_labels.index(u) for u in test_users])\n    internal_movie_ids = np.array([model._item_labels.index(str(m)) for m in test_movies])\n\n    # Extract predicted ratings\n    predicted_ratings = all_predictions[internal_user_ids, internal_movie_ids]\n\n    # Calculate RMSE\n    valid_mask = ~np.isinf(predicted_ratings) &amp; ~np.isnan(predicted_ratings)\n    rmse = np.sqrt(np.mean((predicted_ratings[valid_mask] - actual_ratings[valid_mask]) ** 2))\n    print(f\"Root-mean-square error = {rmse:.4f}\")\n\nexcept ValueError as e:\n    # Handle cold-start users/items in the test set not seen in training\n    print(\"Cold start warning: Some users/items in test set were not in training.\")\n</code></pre>"},{"location":"cookbook/#explanation-of-key-differences","title":"Explanation of Key Differences","text":"<ol> <li>No Distributed Execution: PySpark builds physical query plans (<code>.transform()</code>, <code>.show()</code>). <code>rusket</code> executes completely eagerly in memory, heavily relying on Rust arrays and <code>numpy</code> for C-level vector operations.</li> <li>Cold Starts: <code>rusket</code> is designed for implicit feedback recommendations, and its <code>transform</code>/prediction step expects <code>user_id</code> and <code>item_id</code> values to have been seen during <code>.fit()</code>. Proper production code should handle cold starts with popularity backups instead of omitting them.</li> <li>Implicit vs Explicit Feedback: The Databricks exact example uses ALS for explicit ratings out of 5 stars to calculate Regression Error (RMSE). <code>rusket</code> focuses entirely on implicit feedback (clicks, purchases), so it's optimized for calculating Ranking Metrics (like Precision@K) rather than Regression Error. The math works identically, but it scales differently.</li> </ol>"},{"location":"migration/","title":"Migration from mlxtend","text":"<p>rusket is designed as a drop-in replacement for <code>mlxtend.frequent_patterns</code>. In the vast majority of cases the only change you need is the import line.</p>"},{"location":"migration/#import-change","title":"Import change","text":"Before (mlxtend)After (rusket) <pre><code>from mlxtend.frequent_patterns import fpgrowth, association_rules\n</code></pre> <pre><code>from rusket import fpgrowth, association_rules\n</code></pre>"},{"location":"migration/#api-comparison","title":"API comparison","text":""},{"location":"migration/#fpgrowth","title":"<code>fpgrowth</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame \\| pl.DataFrame</code> rusket also accepts Polars <code>min_support</code> <code>float</code> <code>float</code> identical <code>use_colnames</code> <code>bool</code> <code>bool</code> identical <code>max_len</code> <code>int\\|None</code> <code>int\\|None</code> identical <code>verbose</code> <code>int</code> <code>int</code> accepted but unused <code>null_values</code> <code>bool</code> <code>bool</code> pandas only"},{"location":"migration/#association_rules","title":"<code>association_rules</code>","text":"Parameter mlxtend rusket Notes <code>df</code> <code>pd.DataFrame</code> <code>pd.DataFrame</code> output of <code>fpgrowth</code> <code>num_itemsets</code> <code>int</code> <code>int</code> identical <code>metric</code> <code>str</code> <code>str</code> identical (12 metrics) <code>min_threshold</code> <code>float</code> <code>float</code> identical <code>support_only</code> <code>bool</code> <code>bool</code> identical <code>return_metrics</code> <code>list[str]</code> <code>list[str]</code> identical"},{"location":"migration/#return-value","title":"Return value","text":"<p>Both functions return identical DataFrame structures:</p> <ul> <li><code>fpgrowth</code> \u2192 <code>pd.DataFrame</code> with <code>['support', 'itemsets']</code></li> <li><code>association_rules</code> \u2192 <code>pd.DataFrame</code> with <code>['antecedents', 'consequents', ...metrics]</code></li> </ul> <p>Itemsets are <code>frozenset</code> objects, exactly as in mlxtend.</p>"},{"location":"migration/#whats-different","title":"What's different?","text":"<p>Behaviour differences</p> <ul> <li>Performance: rusket is significantly faster on medium/large datasets and uses far less memory.</li> <li>Polars input: rusket accepts <code>polars.DataFrame</code> natively; mlxtend does not.</li> <li>Sparse DataFrames: rusket uses the CSR path, which is more memory-efficient than mlxtend for sparse data.</li> <li><code>null_values</code> / Rust path: When <code>null_values=True</code>, rusket currently falls back gracefully (no error), but the Rust path is not yet used for null-containing DataFrames.</li> </ul>"},{"location":"migration/#uninstalling-mlxtend","title":"Uninstalling mlxtend","text":"<p>Once you have validated that rusket produces the same results:</p> <pre><code>pip uninstall mlxtend\n</code></pre> <p>rusket has no runtime dependency on mlxtend.</p>"},{"location":"polars/","title":"Polars Support","text":"<p>rusket accepts <code>polars.DataFrame</code> natively alongside pandas, via the Arrow-backed zero-copy path.</p>"},{"location":"polars/#installation","title":"Installation","text":"<p>Install rusket with the Polars extra:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>This pins <code>polars&gt;=0.20</code>. If you already have Polars installed, you can also just <code>pip install rusket</code>.</p>"},{"location":"polars/#usage","title":"Usage","text":"<p>The <code>fpgrowth</code> function detects Polars DataFrames automatically \u2014 no extra parameters needed:</p> <pre><code>import polars as pl\nfrom rusket import fpgrowth, association_rules\n\n# Build a Polars one-hot DataFrame\ndf = pl.DataFrame({\n    \"milk\":  [True, True,  False, True],\n    \"bread\": [True, False, True,  True],\n    \"eggs\":  [False, True, True,  True],\n})\n\n# Mine frequent itemsets \u2014 identical call as with pandas\nfreq = fpgrowth(df, min_support=0.5, use_colnames=True)\nprint(freq)\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# ...\n\n# association_rules always returns a pandas DataFrame\nrules = association_rules(freq, num_itemsets=len(df), metric=\"lift\", min_threshold=1.0)\nprint(rules)\n</code></pre> <p>Return type</p> <p><code>fpgrowth</code> always returns a pandas DataFrame, regardless of input type. <code>association_rules</code> also returns a pandas DataFrame.</p>"},{"location":"polars/#how-it-works","title":"How it works","text":"<p>The Polars path uses <code>polars.DataFrame.to_numpy()</code> which returns an Arrow-backed NumPy buffer \u2014 zero-copy for numeric dtypes.</p> <pre><code>Polars DataFrame\n    \u2502\n    \u25bc  df.to_numpy()  (zero-copy for bool/int dtypes)\nnumpy uint8 array\n    \u2502\n    \u25bc  fpgrowth_from_dense()  (Rust, PyO3 ReadonlyArray2&lt;u8&gt;)\nRust FP-Tree mining\n    \u2502\n    \u25bc\npandas DataFrame  [support, itemsets]\n</code></pre> <p>No intermediate Python object creation occurs between the Polars input and the Rust mining step.</p>"},{"location":"polars/#supported-dtypes","title":"Supported dtypes","text":"Polars dtype Supported <code>Boolean</code> \u2705 <code>Int8 / Int16 / Int32 / Int64</code> \u2705 (0/1 values) <code>UInt8 / UInt16 / UInt32 / UInt64</code> \u2705 (0/1 values) <code>Float32 / Float64</code> \u26a0\ufe0f (0.0/1.0 values, cast to uint8) Categorical / String \u274c (pre-encode with <code>get_dummies</code>) <p>Lazy frames</p> <p>Pass <code>.collect()</code> before calling <code>fpgrowth</code> if you have a <code>LazyFrame</code>: <pre><code>freq = fpgrowth(lazy_df.collect(), min_support=0.3, use_colnames=True)\n</code></pre></p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#installation","title":"Installation","text":"pipuvconda <pre><code>pip install rusket\n</code></pre> <pre><code>uv add rusket\n</code></pre> <pre><code>pip install rusket  # rusket is not on conda-forge yet\n</code></pre> <p>To also enable Polars support:</p> pipuv <pre><code>pip install \"rusket[polars]\"\n</code></pre> <pre><code>uv add \"rusket[polars]\"\n</code></pre> <p>Coming from mlxtend?</p> <p>rusket is a drop-in replacement. In most cases you only need to change your import: <pre><code># Before\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n# After\nfrom rusket import mine, association_rules\n</code></pre> See the full Migration Guide for details.</p>"},{"location":"quickstart/#business-scenario-supermarket-cross-selling","title":"Business Scenario \u2014 Supermarket Cross-Selling","text":"<p>Suppose a supermarket chain wants to identify which products to promote together. The raw data arrives as a checkout log where each row is one receipt.</p>"},{"location":"quickstart/#step-1-prepare-your-data","title":"Step 1 \u2014 Prepare your data","text":"<p><code>mine</code> expects a one-hot encoded DataFrame where rows are transactions and columns are products.</p> <p>Most real-world data comes as long-format order lines (one row per product per order). <code>from_transactions</code> converts that in one call:</p> <pre><code>import pandas as pd\nfrom rusket import from_transactions\n\n# Raw checkout log from a POS system\norders = pd.DataFrame({\n    \"receipt_id\": [1001, 1001, 1001, 1002, 1002, 1003, 1003, 1004],\n    \"product\":    [\"milk\", \"bread\", \"butter\",\n                   \"milk\", \"eggs\",\n                   \"bread\", \"butter\",\n                   \"milk\", \"bread\", \"eggs\", \"coffee\"],\n})\n\n# One-hot encode: rows = receipts, columns = products\nbasket = from_transactions(orders, transaction_col=\"receipt_id\", item_col=\"product\")\nprint(basket)\n#        milk  bread  butter  eggs  coffee\n# 1001   True   True    True False   False\n# 1002   True  False   False  True   False\n# 1003  False   True    True False   False\n# 1004   True   True   False  True    True\n</code></pre>"},{"location":"quickstart/#step-2-mine-frequent-product-combinations","title":"Step 2 \u2014 Mine frequent product combinations","text":"<pre><code>from rusket import mine\n\n# method=\"auto\" automatically selects FP-Growth or Eclat based on catalogue density\nfreq = mine(basket, min_support=0.4, use_colnames=True)\nprint(freq.sort_values(\"support\", ascending=False))\n#    support          itemsets\n# 0     0.75          (milk,)\n# 1     0.75         (bread,)\n# 2     0.50         (butter,)\n# 3     0.50    (milk, bread,)\n# 4     0.50  (bread, butter,)\n</code></pre>"},{"location":"quickstart/#step-2b-or-use-eclat-for-sparse-large-catalogues","title":"Step 2b \u2014 Or use Eclat for sparse, large catalogues","text":"<p>ECLAT is faster for retailers with thousands of SKUs and typical basket sizes of 3\u20135 items.</p> <pre><code>from rusket import eclat\n\nfreq = eclat(basket, min_support=0.4, use_colnames=True)\nprint(freq)  # identical output to fpgrowth\n</code></pre> <p>When to use which?</p> <p><code>mine(method=\"auto\")</code> handles this automatically: it picks <code>eclat</code> for sparse data (density &lt; 0.15, typical for large SKU catalogs) and <code>fpgrowth</code> for dense data.</p>"},{"location":"quickstart/#step-3-generate-frequently-bought-together-rules","title":"Step 3 \u2014 Generate \"Frequently Bought Together\" rules","text":"<pre><code>from rusket import association_rules\n\nrules = association_rules(\n    freq,\n    num_itemsets=len(basket),\n    metric=\"confidence\",\n    min_threshold=0.6,\n)\nprint(rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]])\n# antecedents consequents  support  confidence  lift\n# (bread,)    (butter,)      0.50        0.67   1.33\n# (butter,)   (bread,)       0.50        1.00   1.33\n</code></pre> <p>Interpreting the output:</p> <ul> <li>Confidence 0.67 \u2192 67% of customers who buy bread also buy butter.</li> <li>Lift 1.33 \u2192 customers who buy bread are 1.33\u00d7 more likely to buy butter compared to random.</li> </ul> <p>Use these rules to power your \"Customers also buy\" widgets, shelf placement decisions, or promotional bundles.</p> <p>num_itemsets</p> <p>Pass the total transaction count (<code>len(basket)</code>) so that support-based metrics are computed correctly.</p>"},{"location":"quickstart/#oop-api-fluent-pipeline","title":"OOP API \u2014 Fluent Pipeline","text":"<p>If you prefer a single chained API from raw data down to recommendations:</p> <pre><code>from rusket import AutoMiner\n\nmodel = AutoMiner.from_transactions(orders, transaction_col=\"receipt_id\", item_col=\"product\", min_support=0.4)\nfreq  = model.mine(use_colnames=True)\nrules = model.association_rules(metric=\"lift\", min_threshold=1.0)\n\n# \"What else should go in the basket?\" \u2014 cart recommendations\nbasket_contents = [\"milk\", \"bread\"]\nsuggestions = model.recommend_items(basket_contents, n=3)\nprint(suggestions)  # e.g. [\"butter\", \"eggs\", \"coffee\"]\n</code></pre>"},{"location":"quickstart/#billion-scale-streaming","title":"Billion-Scale Streaming","text":"<p>For retailers with hundreds of millions of transactions that don't fit in memory, use <code>FPMiner</code> to stream data chunk-by-chunk:</p> <pre><code>from rusket import FPMiner\n\nminer = FPMiner(n_items=500_000)  # total distinct SKUs\n\n# Read your fact table in chunks \u2014 e.g. from S3 Parquet or an API cursor\nfor chunk in pd.read_parquet(\"sales_fact.parquet\", chunksize=10_000_000):\n    txn  = chunk[\"receipt_id\"].to_numpy(dtype=\"int64\")\n    item = chunk[\"product_idx\"].to_numpy(dtype=\"int32\")  # 0-based SKU index\n    miner.add_chunk(txn, item)\n\n# Mine \u2014 all data in Rust, output is a normal pandas DataFrame\nfreq  = miner.mine(min_support=0.001, max_len=3)\nrules = association_rules(freq, num_itemsets=miner.n_transactions)\n</code></pre> <p>Peak memory</p> <p>Peak Python memory = one chunk (typically 1\u20132 GB). Rust holds the per-transaction item lists (~5 GB for 200M transactions). The final mining step passes CSR arrays directly \u2014 zero copies.</p>"},{"location":"quickstart/#direct-csr-path","title":"Direct CSR path","text":"<p>If you already have integer arrays from a data warehouse query, skip <code>from_transactions</code> entirely:</p> <pre><code>from scipy import sparse as sp\nfrom rusket import mine\n\ncsr = sp.csr_matrix(\n    (np.ones(len(receipt_ids), dtype=np.int8), (receipt_ids, sku_indices)),\n    shape=(n_receipts, n_skus),\n)\nfreq = mine(csr, min_support=0.001, column_names=sku_names)\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Migration from mlxtend \u2014 side-by-side comparison</li> <li>API Reference \u2014 all parameters and metrics explained</li> <li>Polars Support \u2014 zero-copy Arrow path</li> <li>Recommender Workflows \u2014 personalised recommendations for users</li> <li>Benchmarks \u2014 performance vs mlxtend</li> </ul>"},{"location":"recommender/","title":"Recommender Workflows","text":"<p><code>rusket</code> provides three complementary recommendation strategies that cover the most common revenue-generating use cases in e-commerce, retail, and content platforms.</p> Strategy Best for API \"Frequently Bought Together\" Cart add-ons, shelf placement <code>FPGrowth</code> / <code>AutoMiner</code> \"For You\" (Personalised) Homepage, email, loyalty <code>ALS</code> / <code>BPR</code> Hybrid Blend both signals <code>Recommender</code>"},{"location":"recommender/#frequently-bought-together-cart-recommendations","title":"\"Frequently Bought Together\" \u2014 Cart Recommendations","text":"<p>The fastest path to cart cross-selling: mine association rules from checkout history, then call <code>recommend_items</code> with the current basket contents.</p> <pre><code>import pandas as pd\nfrom rusket import AutoMiner  # or FPGrowth, Eclat\n\n# POS checkout log \u2014 one row per line item\ncheckouts = pd.DataFrame({\n    \"receipt_id\": [1, 1, 2, 2, 2, 3, 3, 4, 4, 4],\n    \"product\":    [\"espresso_beans\", \"grinder\",\n                   \"espresso_beans\", \"milk_frother\", \"travel_mug\",\n                   \"grinder\", \"milk_frother\",\n                   \"espresso_beans\", \"grinder\", \"descaler\"],\n})\n\nmodel = AutoMiner.from_transactions(\n    checkouts,\n    transaction_col=\"receipt_id\",\n    item_col=\"product\",\n    min_support=0.3,\n)\n\n# User just added an espresso grinder to their cart\nbasket   = [\"grinder\"]\nadd_ons  = model.recommend_items(basket, n=3)\nprint(add_ons)\n# e.g. [\"espresso_beans\", \"milk_frother\", \"descaler\"]\n\n# Or inspect the full rule table\nrules = model.association_rules(metric=\"lift\", min_threshold=1.0)\n</code></pre> <p><code>num_itemsets</code> is inferred automatically \u2014 no extra wiring needed.</p>"},{"location":"recommender/#for-you-personalised-recommendations-with-als-bpr","title":"\"For You\" \u2014 Personalised Recommendations with ALS / BPR","text":"<p>Collaborative Filtering builds a latent-space model of user taste from implicit signals (purchases, clicks, plays). Two algorithms are available:</p> <ul> <li>ALS \u2014 best for score prediction and serendipitous discovery (matrix reconstruction)  </li> <li>BPR \u2014 best when you care only about top-N ranking (optimises pairwise ranking loss)</li> </ul>"},{"location":"recommender/#fitting-from-a-purchase-log","title":"Fitting from a purchase log","text":"<pre><code>from rusket import ALS, BPR\n\n# E-commerce purchase history (one row per purchase)\npurchases = pd.DataFrame({\n    \"customer_id\": [1001, 1001, 1001, 1002, 1002, 1003],\n    \"sku\":         [\"A10\", \"B22\", \"C15\",  \"A10\", \"D33\",  \"B22\"],\n    \"revenue\":     [29.99, 49.00, 9.99,  29.99, 15.00, 49.00],  # optional weight\n})\n\n# Option A \u2014 fit directly from the event log\nals = ALS(factors=64, iterations=15, alpha=40.0).from_transactions(\n    purchases,\n    user_col=\"customer_id\",\n    item_col=\"sku\",\n    rating_col=\"revenue\",   # use revenue as confidence weight; omit for binary\n)\n\n# Option B \u2014 fit from a pre-built scipy CSR matrix\n# als = ALS(factors=64, iterations=15, cg_iters=3, anderson_m=5).fit(user_item_csr)\n\n# BPR is a drop-in alternative\nbpr = BPR(factors=64, learning_rate=0.05, iterations=150).fit(user_item_csr)\n</code></pre>"},{"location":"recommender/#getting-recommendations","title":"Getting recommendations","text":"<pre><code># Top-5 SKUs for customer 1001 (hiding already-purchased items)\nitems, scores = als.recommend_items(user_id=1001, n=5, exclude_seen=True)\nprint(f\"Recommended SKUs: {items}\")\n# \u2192 e.g. [\"D33\", \"E11\", \"F02\", \"A45\", \"C99\"]\n\n# Which customers are most likely to buy a specific product? \u2014 useful for email targeting\ntop_customers, scores = als.recommend_users(item_id=\"D33\", n=100)\n</code></pre>"},{"location":"recommender/#the-hybrid-recommender","title":"The Hybrid Recommender","text":"<p>Blend Collaborative Filtering (ALS/BPR) with Association Rules to handle both the personalised homepage and the active shopping cart in a single engine.</p> <pre><code>from rusket import ALS, Recommender, mine, association_rules\n\n# 1. Personalised model trained on full purchase history\nals  = ALS(factors=64, iterations=15).fit(user_item_csr)\n\n# 2. Association rules mined from basket data\nfreq  = mine(basket_ohe, min_support=0.01)\nrules = association_rules(freq, num_itemsets=n_receipts)\n\n# 3. Combine into one engine\nrec = Recommender(als_model=als, rules_df=rules)\n</code></pre>"},{"location":"recommender/#1-personalised-homepage-for-you","title":"1. Personalised homepage (\"For You\")","text":"<pre><code># Returns the 5 most relevant SKUs for customer 1001\nitems, scores = rec.recommend_for_user(user_id=1001, n=5)\nprint(f\"Homepage picks for customer 1001: {items}\")\n</code></pre>"},{"location":"recommender/#2-hybrid-cf-product-embeddings","title":"2. Hybrid \u2014 CF + product embeddings","text":"<p>When you have product description vectors (e.g. from a sentence-transformer model or your PIM), blend semantic similarity into the CF score:</p> <pre><code>rec = Recommender(als_model=als, rules_df=rules, item_embeddings=product_vectors)\n\n# alpha=0.7 \u2192 70% CF preference signal + 30% product similarity\nitems, scores = rec.recommend_for_user(\n    user_id=1001,\n    n=5,\n    alpha=0.7,\n    target_item_for_semantic=\"B22\",  # anchor the similarity to the last viewed item\n)\n</code></pre>"},{"location":"recommender/#3-cart-based-frequently-bought-together","title":"3. Cart-based \"Frequently Bought Together\"","text":"<pre><code># Customer has espresso beans and a grinder in the cart\ncart = [\"espresso_beans\", \"grinder\"]\nadd_ons = rec.recommend_for_cart(cart, n=3)\nprint(f\"Add to cart suggestions: {add_ons}\")\n# \u2192 [\"milk_frother\", \"descaler\", \"travel_mug\"]\n</code></pre>"},{"location":"recommender/#4-batch-scoring-email-campaign-targeting","title":"4. Batch scoring \u2014 email campaign targeting","text":"<p>Score the entire customer base overnight and write results to your CRM:</p> <pre><code># user_history_df: one row per customer with their purchase history\nbatch = rec.predict_next_chunk(user_history_df, user_col=\"customer_id\", k=5)\n# Returns: DataFrame[customer_id, recommended_items]\nbatch.to_parquet(\"s3://data-lake/recommendations/daily_picks.parquet\")\n</code></pre>"},{"location":"recommender/#item-to-item-similarity-you-may-also-like","title":"Item-to-Item Similarity \u2014 \"You May Also Like\"","text":"<p>For anonymous visitors (no login, no history), fall back to latent-space item similarity:</p> <pre><code>from rusket import similar_items, ALS\n\nals = ALS(factors=128).fit(interactions)\n\n# Customer is viewing product B22 (a coffee grinder)\nsimilar_skus, similarity_scores = similar_items(als, item_id=\"B22\", n=4)\nprint(similar_skus)      # \u2192 [\"B25\", \"B18\", \"C10\", \"D05\"]\nprint(similarity_scores) # \u2192 [0.97, 0.93, 0.89, 0.84]\n</code></pre> <p>Note: Latent-space similarity discovers implicit relationships \u2014 a premium coffee grinder may cluster tightly with an espresso machine even if they're rarely purchased in the same basket, because the same type of customer buys both.</p>"},{"location":"recommender/#cross-selling-potential-scoring","title":"Cross-Selling Potential Scoring","text":"<p>Quantify the \"missed opportunity\" \u2014 how likely is a customer to buy a product they haven't bought yet, based on their overall purchasing pattern? Perfect for targeting high-intent customers with a retargeting ad or a personalised email.</p> <pre><code>from rusket import score_potential\n\n# Purchase history for 3 customers (item IDs they've already bought)\npurchase_histories = [\n    [10, 22, 51],   # customer 1001 \u2014 bought SKUs 10, 22, 51\n    [10, 33],        # customer 1002\n    [51],            # customer 1003\n]\n\n# Target: which customers should receive a \"Coffee Machine Accessories\" promo?\naccessory_skus = [60, 61, 62]  # descaler, portafilter, tamper\n\n# Shape: (n_customers, len(accessory_skus))\n# Already-purchased items are masked to -\u221e so they never appear in rankings\npotential = score_potential(\n    user_history=purchase_histories,\n    als_model=als,\n    target_categories=accessory_skus,\n)\n\n# Sort customers by their accessory affinity\nimport pandas as pd\ndf_potential = pd.DataFrame(potential, columns=accessory_skus)\ntop_targets  = df_potential.mean(axis=1).sort_values(ascending=False)\nprint(\"Customers to target:\", top_targets.head(10).index.tolist())\n</code></pre>"},{"location":"recommender/#analytics-helpers","title":"Analytics Helpers","text":""},{"location":"recommender/#substitute-cannibalising-products","title":"Substitute / Cannibalising Products","text":"<p>Items that are individually popular but rarely bought together (lift &lt; 1.0) likely compete with each other. Useful for assortment rationalisation:</p> <pre><code>from rusket import find_substitutes\n\n# Identify products that cannibalise each other's sales\nsubs = find_substitutes(rules_df, max_lift=0.8)\n# Returns a DataFrame sorted ascending by lift (strongest cannibals first)\n#  antecedents  consequents  lift\n#  (Cola A,)    (Cola B,)    0.61\n</code></pre>"},{"location":"recommender/#customer-saturation","title":"Customer Saturation","text":"<p>Segment your customer base by how deeply they've penetrated a category \u2014 essential for deciding where to focus expansion campaigns vs. loyalty programmes:</p> <pre><code>from rusket import customer_saturation\n\nsaturation = customer_saturation(\n    purchases_df,\n    user_col=\"customer_id\",\n    category_col=\"category_id\",\n)\n# Returns: customer_id | unique_count | saturation_pct | decile\n# Decile 10 = customers who already buy almost everything in the category (defend)\n# Decile 1  = low engagement \u2014 high growth potential (acquire/activate)\n</code></pre>"},{"location":"recommender/#vector-db-export","title":"Vector DB Export","text":"<p>Export ALS/BPR item factors as embeddings, ready for FAISS, Qdrant, or Pinecone \u2014 connect your recommender to a Generative AI retrieval pipeline:</p> <pre><code>from rusket import export_item_factors\n\n# Each row: one SKU, columns = latent dimensions\ndf_vectors = export_item_factors(als, include_labels=True)\n\n# Write directly to your vector store\nimport lancedb\ndb    = lancedb.connect(\"./product_vectors\")\ntable = db.create_table(\"skus\", data=df_vectors, mode=\"overwrite\")\n</code></pre>"},{"location":"recommender/#graph-analytics-product-community-detection","title":"Graph Analytics \u2014 Product Community Detection","text":"<p>Convert association rules into a NetworkX directed graph to discover product communities \u2014 groups of products that form a natural ecosystem (e.g., \"barista toolkit\", \"home baking essentials\"):</p> <pre><code>from rusket.viz import to_networkx\nimport networkx as nx\n\nG = to_networkx(rules_df, edge_attr=\"lift\")\n\n# PageRank highlights the most \"gateway\" products in the catalogue\ncentrality = nx.pagerank(G, weight=\"lift\")\ntop_gateway = sorted(centrality, key=centrality.get, reverse=True)[:5]\nprint(\"Gateway products:\", top_gateway)\n# \u2192 e.g. [\"espresso_beans\", \"grinder\", \"milk_frother\", ...]\n</code></pre>"},{"location":"spark/","title":"PySpark Integration","text":"<p><code>rusket</code> integrates with PySpark clusters via zero-copy Apache Arrow transfers, enabling distributed execution of all its core algorithms across a Databricks or on-prem Hadoop cluster without manual serialisation.</p> <p>All distributed functions live in <code>rusket.spark</code> and use <code>applyInArrow</code> (Spark 3.4+) with <code>applyInPandas</code> as a fallback for older versions.</p>"},{"location":"spark/#setup","title":"Setup","text":"<pre><code>spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n</code></pre>"},{"location":"spark/#mine_grouped-distributed-market-basket-analysis-per-store-region","title":"<code>mine_grouped</code> \u2014 Distributed Market Basket Analysis per Store / Region","text":"<p>A retail chain with 50 stores wants per-store \"Frequently Bought Together\" rules \u2014 different stores, different assortments, different shopping habits. <code>mine_grouped</code> distributes <code>rusket.mine</code> across a Spark cluster, running each store's rules in parallel on separate executor nodes.</p> <pre><code>import rusket.spark\n\n# Delta table: receipt_id | store_id | &lt;one column per SKU (boolean)&gt;\nspark_df = spark.table(\"gold.baskets_ohe\")\n\n# Each store gets its own frequent itemsets \u2014 runs fully in parallel on the cluster\nfreq_df = rusket.spark.mine_grouped(\n    df=spark_df,\n    group_col=\"store_id\",\n    min_support=0.05,\n    method=\"auto\",      # \"auto\" | \"fpgrowth\" | \"eclat\"\n    use_colnames=True,  # must be True for schema safety\n    max_len=3,          # pairs and triples only \u2014 avoids combinatorial explosion\n)\n# Output: store_id (string) | support (double) | itemsets (array&lt;string&gt;)\n</code></pre>"},{"location":"spark/#rules_grouped-distributed-association-rules-per-segment","title":"<code>rules_grouped</code> \u2014 Distributed Association Rules per Segment","text":"<p>Takes the output of <code>mine_grouped</code> and generates association rules per group. Ideal for feeding per-store rules into a recommendation API or a merchandising dashboard.</p> <pre><code>rules_df = rusket.spark.rules_grouped(\n    df=freq_df,\n    group_col=\"store_id\",\n    num_itemsets={\"store_A\": 45_000, \"store_B\": 12_300},  # receipts per store\n    metric=\"confidence\",\n    min_threshold=0.6,\n)\n# Output: store_id | antecedents | consequents | confidence | lift | ...\n# (all 11 metrics are always included)\n</code></pre> <p>Full end-to-end regional pipeline:</p> <pre><code># Step 1 \u2014 mine per store\nfreq_df  = rusket.spark.mine_grouped(spark_df, group_col=\"store_id\", min_support=0.05)\n\n# Step 2 \u2014 generate \"Frequently Bought Together\" rules per store\nrules_df = rusket.spark.rules_grouped(freq_df, group_col=\"store_id\", num_itemsets=20_000)\n\n# Step 3 \u2014 write to Delta for downstream serving\nrules_df.write.mode(\"overwrite\").saveAsTable(\"gold.per_store_rules\")\n</code></pre>"},{"location":"spark/#prefixspan_grouped-distributed-customer-journey-analysis","title":"<code>prefixspan_grouped</code> \u2014 Distributed Customer Journey Analysis","text":"<p>A telco or media company wants to discover how customers navigate their product portfolio over time \u2014 which services do customers typically subscribe to before churning to a competitor?</p> <p>PrefixSpan mines sequential patterns (ordered events) across users, grouped by region or segment.</p> <pre><code>seq_df = rusket.spark.prefixspan_grouped(\n    df=spark_df,               # event_log: customer_id | region | timestamp | product_id\n    group_col=\"region\",\n    user_col=\"customer_id\",    # sequence identifier\n    time_col=\"event_ts\",       # ordering column (timestamp)\n    item_col=\"product_id\",\n    min_support=100,           # at least 100 customers must follow this path\n    max_len=4,                 # journeys up to 4 steps long\n)\n# Output: region | support (long) | sequence (array&lt;string&gt;)\n# Example:\n# EMEA | 432 | [broadband, mobile, tv_bundle, cancel]\n# APAC | 218 | [mobile, broadband, upgrade_premium]\n</code></pre>"},{"location":"spark/#hupm_grouped-distributed-high-profit-bundle-discovery","title":"<code>hupm_grouped</code> \u2014 Distributed High-Profit Bundle Discovery","text":"<p>A specialty foods retailer wants to discover which product combinations generate the most revenue per region, even if they aren't bought very frequently \u2014 a classic use case for High-Utility Pattern Mining.</p> <pre><code>hupm_df = rusket.spark.hupm_grouped(\n    df=spark_df,               # receipt_id | region | product_id | margin\n    group_col=\"region\",\n    transaction_col=\"receipt_id\",\n    item_col=\"product_id\",\n    utility_col=\"margin\",      # gross margin per line item\n    min_utility=500.0,         # only bundles generating \u2265 \u20ac500 total margin\n    max_len=3,\n)\n# Output: region | utility (double) | itemset (array&lt;long&gt;)\n# Example:\n# NORTH | 1840.0 | [aged_cheese, wine_flight, charcuterie]\n</code></pre>"},{"location":"spark/#recommend_batches-overnight-batch-personalisation-at-scale","title":"<code>recommend_batches</code> \u2014 Overnight Batch Personalisation at Scale","text":"<p>Score millions of users overnight with a pre-trained ALS model, then write personalised recommendations to your CRM or marketing automation platform.</p> <pre><code># 1. Train ALS on driver once (or load from a checkpoint)\nfrom rusket import ALS\nals = ALS(factors=64, iterations=15).fit(user_item_csr)\n\n# 2. Broadcast to every Spark worker and score all users in parallel\nrec_df = rusket.spark.recommend_batches(\n    df=spark.table(\"silver.user_sessions\"),  # must contain user_col\n    model=als,\n    user_col=\"customer_id\",\n    k=10,   # top-10 personalised SKU recommendations per customer\n)\n# Output: customer_id (string) | recommended_items (array&lt;int&gt;)\n\n# 3. Write to CRM / email platform\nrec_df.write.mode(\"overwrite\").saveAsTable(\"gold.daily_recommendations\")\n</code></pre>"},{"location":"spark/#mine_spark-global-mining-via-fpminer-streaming","title":"<code>mine_spark</code> \u2014 Global Mining via FPMiner Streaming","text":"<p>When you need global patterns (not per-group) but the raw event log is too large for <code>.toPandas()</code>, stream integer <code>(txn_id, item_id)</code> chunks from Spark workers into the <code>FPMiner</code> accumulator on the driver via zero-copy Arrow transfers.</p> <p>Use case: a marketplace with 200M+ order lines needs one global set of \"Customers also buy\" rules.</p> <pre><code>from rusket.streaming import mine_spark\n\nfreq_df = mine_spark(\n    spark_df=spark.table(\"silver.order_lines\"),\n    n_items=200_000,          # distinct SKU count\n    txn_col=\"order_id\",       # integer-typed order identifier\n    item_col=\"sku_index\",     # 0-based integer SKU index\n    min_support=0.001,        # 0.1% of all orders\n    max_len=3,\n)\n# Returns a standard Pandas DataFrame on the driver\n# ready for association_rules() or to be written to Delta\nprint(freq_df.head())\n</code></pre>"},{"location":"spark/#to_spark","title":"<code>to_spark</code>","text":"<p>Convert a Pandas or Polars DataFrame to a PySpark DataFrame:</p> <pre><code>spark_df = rusket.spark.to_spark(spark_session, df)\n</code></pre>"},{"location":"streaming/","title":"Streaming and Big Data","text":"<p>When dealing with extremely large transactional datasets\u2014such as billions of rows of e-commerce clickstreams or year-long retail logs\u2014loading the entire one-hot encoded matrix into RAM (even as a sparse matrix) may exhaust your system's memory.</p> <p>To solve this, <code>rusket</code> includes <code>FPMiner</code>, a highly optimized streaming accumulator written completely in Rust.</p>"},{"location":"streaming/#the-streaming-concept","title":"The Streaming Concept","text":"<p>Instead of converting a \"long-format\" event log <code>(user_id, item_id)</code> into a massive <code>N \u00d7 M</code> sparse matrix, the <code>FPMiner</code> accepts small, memory-safe chunks of raw integers.</p> <p>Rust accumulates these <code>(transaction_id, item_id)</code> pairs internally using a highly efficient <code>HashMap&lt;i64, Vec&lt;i32&gt;&gt;</code>. Because this happens incrementally:</p> <ol> <li>Python Memory overhead is strictly limited to the size of a single chunk (e.g., 10 million rows).</li> <li>Matrix Pivoting (Group-By operations) are avoided entirely.</li> </ol> <pre><code>graph LR\n    A[\"Parquet File&lt;br/&gt;(1 Billion Rows)\"] --&gt; B[\"Python Chunk&lt;br/&gt;(10M Rows)\"]\n    B --&gt;|Stream| C[\"Rust FPMiner&lt;br/&gt;(Accumulates inside JVM-like Heap)\"]\n    C --&gt;|Next Chunk| A\n    C --&gt;|mine()| D[\"Frequent Itemsets&lt;br/&gt;pd.DataFrame\"]</code></pre>"},{"location":"streaming/#reading-from-disk-parquet-csv","title":"Reading from Disk (Parquet / CSV)","text":"<p>The simplest way to handle large files is to use the Pandas <code>chunksize</code> parameter combined with the high-level <code>FPMiner</code> API.</p> <pre><code>import pandas as pd\nfrom rusket import FPMiner\n\n# Initialize the miner. You must provide the maximum number of distinct items.\nminer = FPMiner(n_items=100_000)\n\n# Process a massive Parquet file in chunks\nfor chunk in pd.read_parquet(\"massive_event_log.parquet\", chunksize=10_000_000):\n    # Extract integer identifiers\n    txn_ids = chunk[\"user_session\"].to_numpy(dtype=\"int64\")\n    item_ids = chunk[\"product_id\"].to_numpy(dtype=\"int32\")\n\n    # Pass directly into the Rust accumulator\n    miner.add_chunk(txn_ids, item_ids)\n\n# Once all chunks are fed, execute the mining algorithm\nfreq_itemsets = miner.mine(\n    min_support=0.005, \n    max_len=4, \n    method=\"auto\"\n)\n</code></pre>"},{"location":"streaming/#arrow-and-duckdb-integrations","title":"Arrow and DuckDB Integrations","text":"<p>For even higher performance, you can bypass Pandas entirely by using <code>pyarrow</code> underneath a DuckDB query engine. <code>rusket</code> provides a convenience helper function:</p> <pre><code>import duckdb\nfrom rusket.streaming import mine_duckdb\n\ncon = duckdb.connect(\"my_analytics_db.duckdb\")\n\n# Stream query results directly via Arrow buffers to Rust\nfreq = mine_duckdb(\n    con=con,\n    query=\"SELECT session_id, product_id FROM sales WHERE region = 'EMEA'\",\n    n_items=50_000,\n    txn_col=\"session_id\",\n    item_col=\"product_id\",\n    min_support=0.01,\n    chunk_size=5_000_000\n)\n</code></pre>"},{"location":"notebooks/1_online_retail_basket_analysis/","title":"Rusket vs MLxtend: Market Basket Analysis at Scale","text":"<p>In this notebook we use a realistic synthetic retail dataset \u2014 with genuine co-purchase correlations and a pair of competing substitute brands \u2014 to show why <code>rusket</code> is the fastest association-rule library in Python.</p> <p>We then use the discovered rules to perform Assortment Optimization (Cannibalization Detection) and visualize the results with Plotly.</p> <pre><code>import os\nimport pathlib\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\nfrom mlxtend.frequent_patterns import fpgrowth as mlxtend_fpgrowth\n\nfrom rusket import association_rules, mine\nfrom rusket.analytics import find_substitutes\n\n# Crisp dark theme for all charts\npio.templates.default = \"plotly_dark\"\n\n# Nicer float display in DataFrames\npd.options.display.float_format = \"{:.3f}\".format\n\n# Charts saved as self-contained HTML for MkDocs embedding\nCHARTS_DIR = pathlib.Path(\"docs/notebooks/charts\")\nCHARTS_DIR.mkdir(parents=True, exist_ok=True)\n\ndef save_chart(fig, name: str) -&gt; None:\n    path = CHARTS_DIR / f\"{name}.html\"\n    fig.write_html(str(path), include_plotlyjs=\"cdn\", full_html=True)\n    print(f\"Chart saved \u2192 {path}\")\n</code></pre>"},{"location":"notebooks/1_online_retail_basket_analysis/#1-generating-a-realistic-correlated-dataset","title":"1. Generating a Realistic Correlated Dataset","text":"<p>A purely random basket matrix (the typical benchmark approach) has no real signal: every item pair will have lift \u2248 1.0, and no rules will pass a meaningful confidence threshold. Instead we generate baskets from three customer segments with strong co-purchase behaviour, plus two competing cola brands that are negatively correlated (lift &lt; 1 \u2014 genuine substitutes).</p> <pre><code>def generate_basket_data(n_transactions: int = 20_000, seed: int = 42) -&gt; pd.DataFrame:\n    \"\"\"\n    Segment-based basket generator with realistic co-purchase correlations.\n\n    Three segments create strong *positive* correlations (high lift).\n    Two competing cola brands are *negatively* correlated (lift \u2248 0.76, substitutes).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = n_transactions\n\n    cols = [\n        # Tech accessories cluster\n        \"Mouse\", \"Keyboard\", \"USB_Hub\", \"Webcam\",\n        # Barista / coffee cluster\n        \"Espresso_Beans\", \"Milk_Frother\", \"Travel_Mug\",\n        # Home-office cluster\n        \"Notebook\", \"Gel_Pen\", \"Highlighter\",\n        # Competing brands \u2014 negative correlation\n        \"Cola_A\", \"Cola_B\",\n    ]\n    df = pd.DataFrame(False, index=range(n), columns=cols)\n\n    # Tech buyers (40%) cluster\n    seg = rng.random(n) &lt; 0.40\n    for p in [\"Mouse\", \"Keyboard\", \"USB_Hub\", \"Webcam\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.75\n\n    # Coffee buyers (35%) cluster\n    seg = rng.random(n) &lt; 0.35\n    for p in [\"Espresso_Beans\", \"Milk_Frother\", \"Travel_Mug\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.78\n\n    # Home-office buyers (45%) cluster\n    seg = rng.random(n) &lt; 0.45\n    for p in [\"Notebook\", \"Gel_Pen\", \"Highlighter\"]:\n        df.loc[seg, p] = rng.random(seg.sum()) &lt; 0.72\n\n    # Substitutes: Cola_A is popular (~38%)\n    # Cola_B can appear with A at only 16% probability \u2192 co-occurrence ~6%\n    # Independence would predict ~8% \u2192 lift \u2248 0.76\n    a_mask = rng.random(n) &lt; 0.38\n    b_with_a = a_mask &amp; (rng.random(n) &lt; 0.16)\n    b_only = (~a_mask) &amp; (rng.random(n) &lt; 0.24)\n    df[\"Cola_A\"] = a_mask\n    df[\"Cola_B\"] = b_with_a | b_only\n\n    return df\n\n\ndf = generate_basket_data(n_transactions=20_000)\nprint(f\"Dataset: {df.shape[0]:,} baskets \u00d7 {df.shape[1]} products\")\nprint(f\"Avg basket size: {df.sum(axis=1).mean():.1f} items\")\nprint(f\"Cola_A support: {df['Cola_A'].mean():.3f}\")\nprint(f\"Cola_B support: {df['Cola_B'].mean():.3f}\")\nprint(f\"Cola_A &amp; Cola_B co-occurrence: {(df['Cola_A'] &amp; df['Cola_B']).mean():.3f}\")\ndf.head(5)\n</code></pre> <pre><code>Dataset: 20,000 baskets \u00d7 12 products\nAvg basket size: 3.6 items\nCola_A support: 0.380\nCola_B support: 0.212\nCola_A &amp; Cola_B co-occurrence: 0.061\n</code></pre> Mouse Keyboard USB_Hub Webcam Espresso_Beans Milk_Frother Travel_Mug Notebook Gel_Pen Highlighter Cola_A Cola_B 0 False False False False False False False False False False True False 1 False False False False False False False False False False True False 2 False False False False False False False False False False False False 3 False False False False False False False True True True False False 4 True True False False False False False True True False False True"},{"location":"notebooks/1_online_retail_basket_analysis/#2-the-benchmark-rusket-vs-mlxtend","title":"2. The Benchmark: Rusket vs MLxtend","text":"<p>We mine all product combinations appearing in at least 5% of baskets. <code>rusket</code> provides FP-Growth and Eclat \u2014 both written entirely in Rust.</p> <pre><code>min_support = 0.05\n\n# --- Rusket FP-Growth ---\nt0 = time.time()\nrusket_res = mine(df, min_support=min_support, method=\"fpgrowth\", use_colnames=True)\nrusket_time = time.time() - t0\nprint(f\"\ud83d\ude80 Rusket FP-Growth: {rusket_time:.4f}s  ({len(rusket_res):,} itemsets)\")\n\n# --- Rusket Eclat ---\nt0 = time.time()\nrusket_eclat_res = mine(df, min_support=min_support, method=\"eclat\", use_colnames=True)\nrusket_eclat_time = time.time() - t0\nprint(f\"\ud83d\ude80 Rusket Eclat:     {rusket_eclat_time:.4f}s\")\n\n# --- MLxtend FP-Growth ---\nt0 = time.time()\nmlxtend_res = mlxtend_fpgrowth(df, min_support=min_support, use_colnames=True)\nmlxtend_time = time.time() - t0\nprint(f\"\ud83d\udc22 MLxtend FP-Growth:{mlxtend_time:.4f}s  ({len(mlxtend_res):,} itemsets)\")\nprint(\"-\" * 50)\nprint(f\"\ud83c\udfc6 Rusket is {mlxtend_time / rusket_time:.1f}\u00d7 faster than MLxtend!\")\n</code></pre> <pre><code>\ud83d\ude80 Rusket FP-Growth: 0.0129s  (226 itemsets)\n\ud83d\ude80 Rusket Eclat:     0.0027s\n\ud83d\udc22 MLxtend FP-Growth:0.0421s  (226 itemsets)\n--------------------------------------------------\n\ud83c\udfc6 Rusket is 3.3\u00d7 faster than MLxtend!\n</code></pre> <pre><code>fig = px.bar(\n    x=[\"MLxtend (Python)\", \"Rusket Eclat (Rust)\", \"Rusket FP-Growth (Rust)\"],\n    y=[mlxtend_time, rusket_eclat_time, rusket_time],\n    title=\"\u23f1 Execution Time \u2014 Lower is Better\",\n    labels={\"x\": \"Implementation\", \"y\": \"Time (seconds)\"},\n    color=[\"baseline\", \"optimized\", \"optimized\"],\n    color_discrete_map={\"baseline\": \"#EF553B\", \"optimized\": \"#00CC96\"},\n    text_auto=\".2f\",\n)\nfig.update_traces(textfont_size=15)\nfig.update_layout(showlegend=False, title_font_size=20)\nsave_chart(fig, \"benchmark\")\nfig.show()\n</code></pre> <pre><code>Chart saved \u2192 docs/notebooks/charts/benchmark.html\n</code></pre>"},{"location":"notebooks/1_online_retail_basket_analysis/#3-generating-cross-sell-rules","title":"3. Generating Cross-Sell Rules","text":"<p>From the frequent itemsets we generate association rules \u2014 \"If a customer buys A, they will also buy B\" \u2014 ranked by lift (how much more likely the co-purchase is versus random chance). Lift &gt; 1 means a genuine affinity; lift &lt; 1 means the products repel each other.</p> <pre><code>t0 = time.time()\nrules = association_rules(rusket_res, num_itemsets=len(df), min_threshold=0.01)\nprint(f\"Generated {len(rules):,} rules in {time.time() - t0:.5f}s\")\n\n# Top cross-sell rules by lift\n(\n    rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]\n    .sort_values(\"lift\", ascending=False)\n    .head(8)\n    .assign(\n        support=lambda d: d[\"support\"].round(3),\n        confidence=lambda d: d[\"confidence\"].round(3),\n        lift=lambda d: d[\"lift\"].round(2),\n    )\n    .reset_index(drop=True)\n)\n</code></pre> <pre><code>Generated 1,412 rules in 0.00574s\n</code></pre> antecedents consequents support confidence lift 0 frozenset({Gel_Pen, Espresso_Beans}) frozenset({Notebook, Travel_Mug}) 0.051 0.588 6.670 1 frozenset({Notebook, Travel_Mug}) frozenset({Gel_Pen, Espresso_Beans}) 0.051 0.583 6.670 2 frozenset({Notebook, Espresso_Beans}) frozenset({Gel_Pen, Travel_Mug}) 0.051 0.586 6.570 3 frozenset({Gel_Pen, Travel_Mug}) frozenset({Notebook, Espresso_Beans}) 0.051 0.576 6.570 4 frozenset({Milk_Frother, Gel_Pen}) frozenset({Highlighter, Travel_Mug}) 0.050 0.571 6.560 5 frozenset({Highlighter, Travel_Mug}) frozenset({Milk_Frother, Gel_Pen}) 0.050 0.580 6.560 6 frozenset({Milk_Frother, Gel_Pen}) frozenset({Notebook, Travel_Mug}) 0.050 0.571 6.480 7 frozenset({Notebook, Travel_Mug}) frozenset({Milk_Frother, Gel_Pen}) 0.050 0.573 6.480"},{"location":"notebooks/1_online_retail_basket_analysis/#4-assortment-optimization-substitute-detection","title":"4. Assortment Optimization \u2014 Substitute Detection","text":"<p>Most tutorials stop at finding items bought together. But what about items that prevent each other from being bought?</p> <p>If Product A and Product B are both individually popular but their co-occurrence is lower than random chance (lift &lt; 1), they are substitutes \u2014 customers choose one instead of the other. Retailers use this to:</p> <ul> <li>Delist redundant SKUs (reduce warehouse cost)</li> <li>Negotiate better terms with the weaker brand</li> <li>Optimise shelf-space by not displaying competing items side-by-side</li> </ul> <p><code>rusket</code> provides <code>find_substitutes</code> out of the box:</p> <pre><code>substitutes = find_substitutes(rules, max_lift=0.9)\nprint(f\"Found {len(substitutes)} cannibalizing product pair(s).\")\n\n(\n    substitutes[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]\n    .assign(\n        support=lambda d: d[\"support\"].round(3),\n        confidence=lambda d: d[\"confidence\"].round(3),\n        lift=lambda d: d[\"lift\"].round(3),\n    )\n    .reset_index(drop=True)\n)\n</code></pre> <pre><code>Found 2 cannibalizing product pair(s).\n</code></pre> antecedents consequents support confidence lift 0 frozenset({Cola_B}) frozenset({Cola_A}) 0.061 0.290 0.762 1 frozenset({Cola_A}) frozenset({Cola_B}) 0.061 0.162 0.762"},{"location":"notebooks/1_online_retail_basket_analysis/#visualizing-the-product-strategy-quadrant","title":"Visualizing the Product Strategy Quadrant","text":"<p>Plot every rule as a point: Confidence (x-axis) vs Lift (y-axis).</p> <ul> <li>Top-right (high confidence, high lift): perfect cross-sell candidates</li> <li>Below the dashed line (lift &lt; 1): substitutes / cannibalizing products</li> </ul> <pre><code># Plot only singleton\u2192singleton rules for readability\nsingleton_rules = rules[\n    (rules[\"antecedents\"].apply(len) == 1)\n    &amp; (rules[\"consequents\"].apply(len) == 1)\n].copy()\n\nsingleton_rules[\"rule_label\"] = (\n    singleton_rules[\"antecedents\"].apply(lambda x: next(iter(x)))\n    + \" \u2192 \"\n    + singleton_rules[\"consequents\"].apply(lambda x: next(iter(x)))\n)\n\nfig = px.scatter(\n    singleton_rules,\n    x=\"confidence\",\n    y=\"lift\",\n    size=\"support\",\n    color=\"lift\",\n    hover_name=\"rule_label\",\n    hover_data={\"confidence\": \":.3f\", \"lift\": \":.3f\", \"support\": \":.3f\"},\n    color_continuous_scale=\"RdYlGn\",\n    title=\"\ud83d\udcca Product Strategy: Cross-Sells vs Substitutes\",\n    labels={\"confidence\": \"Confidence\", \"lift\": \"Lift\"},\n)\nfig.add_hline(\n    y=1.0, line_dash=\"dash\", line_color=\"white\",\n    annotation_text=\"Lift = 1.0  (independent)\",\n    annotation_position=\"top left\",\n)\nfig.add_annotation(\n    x=0.85, y=singleton_rules[\"lift\"].max() * 0.92,\n    text=\"\u2705 Cross-sell\", showarrow=False,\n    font=dict(color=\"#00CC96\", size=14),\n)\nfig.add_annotation(\n    x=0.18, y=0.60,\n    text=\"\u26a0\ufe0f Substitutes\", showarrow=False,\n    font=dict(color=\"#EF553B\", size=14),\n)\nfig.update_layout(title_font_size=20)\nsave_chart(fig, \"product_strategy\")\nfig.show()\n</code></pre> <pre><code>Chart saved \u2192 docs/notebooks/charts/product_strategy.html\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/","title":"Hybrid Recommendations &amp; User Potential with MovieLens","text":"<p>In this cookbook, we will use the classic MovieLens 100k dataset to showcase <code>rusket</code>'s collaborative filtering models (<code>ALS</code> and <code>BPR</code>) and demonstrate how to deploy the ultimate hybrid engine using <code>NextBestAction</code>.</p> <pre><code>import time\n\nimport numpy as np\nimport pandas as pd\n\nfrom rusket import ALS, BPR, NextBestAction\nfrom rusket.recommend import score_potential\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/#1-load-movielens-data","title":"1. Load MovieLens Data","text":"<p>We will download and parse the MovieLens 100k dataset into a Pandas DataFrame representing user ID, movie ID, and rating (1-5).</p> <pre><code>import os\nimport urllib.request\nimport zipfile\n\nif not os.path.exists(\"ml-100k\"):\n    url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n    urllib.request.urlretrieve(url, \"ml-100k.zip\")\n    with zipfile.ZipFile(\"ml-100k.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(\".\")\n\ncolumns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\ndf = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=columns)\n\nprint(f\"Loaded {len(df):,} ratings!\")\ndf.head()\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/#2-fitting-alternating-least-squares-als","title":"2. Fitting Alternating Least Squares (ALS)","text":"<p><code>rusket</code> provides <code>ALS</code>, a highly optimized collaborative filtering engine that learns latent representations of users based on their interaction histories.</p> <p>We can fit our model directly off the raw Pandas transaction dataframe using <code>fit_transactions</code>:</p> <pre><code>als_model = ALS(factors=64, iterations=15, alpha=15.0, regularization=0.01, seed=42)\n\nt0 = time.time()\nals_model.fit_transactions(df, user_col=\"user_id\", item_col=\"item_id\", rating_col=\"rating\")\nprint(f\"\u26a1 ALS training complete in {time.time() - t0:.4f}s!\")\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/#3-bayesian-personalized-ranking-bpr","title":"3. Bayesian Personalized Ranking (BPR)","text":"<p>While ALS is great for general score predictions, BPR natively optimizes for Rank. This makes it the superior choice when your goal is purely to rank Top-N recommendations based entirely on implicit (binary) views or clicks, ignoring explicit star ratings.</p> <pre><code>bpr_model = BPR(factors=64, iterations=150, learning_rate=0.05, regularization=0.01)\n\n# Notice BPR trains on implicit behavior (user_col and item_col), completely ignoring ratings\nt0 = time.time()\nbpr_model.fit_transactions(df, user_col=\"user_id\", item_col=\"item_id\")\nprint(f\"\u26a1 BPR training complete in {time.time() - t0:.4f}s!\")\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/#4-next-best-action-hybrid-api","title":"4. Next Best Action (Hybrid API)","text":"<p>The <code>NextBestAction</code> engine wraps these complicated matrices and index boundaries into a dead-simple business API for analysts. </p> <p>If we pass it our <code>als_model</code>, we can instantly ask for the top 5 next best products for a handful of target users.</p> <pre><code>nba = NextBestAction(als_model=als_model)\n\n# Target users from our CRM\ntarget_users = pd.DataFrame({\"customer_id\": [1, 5, 25, 42]})\n\n# Predict the best 3 movies for these users to watch next!\nrecommendations = nba.predict_next_chunk(target_users, user_col=\"customer_id\", k=3)\nrecommendations\n</code></pre>"},{"location":"notebooks/2_movielens_recommendations/#5-marketing-potential-score","title":"5. Marketing Potential Score","text":"<p>Want to launch an email marketing campaign for specific movies (say movies <code>10</code>, <code>50</code>, and <code>100</code>), but only want to email users who are highly primed to buy them? </p> <p>We can use the <code>score_potential</code> API to predict their exact likelihood of interaction across the entire customer base.</p> <pre><code>user_histories = df.groupby(\"user_id\")[\"item_id\"].apply(list).tolist()\n\npotential_scores = score_potential(user_history=user_histories, als_model=als_model, target_categories=[10, 50, 100])\n\nprint(\"Top 5 user scores for Movie ID 10:\")\n# Get users with the highest probability to interact with Movie 10\nmovie_10_scores = potential_scores[:, 0]  # First column corresponds to Item 10\nbest_users_for_campaign = np.argsort(movie_10_scores)[::-1][:5]\n\nfor u in best_users_for_campaign:\n    print(f\"User {u}: {movie_10_scores[u]:.2f}\")\n</code></pre>"},{"location":"notebooks/3_ecommerce_lifecycle_mining/","title":"Sequential Pattern Mining (PrefixSpan)","text":"<p>In standard Market Basket Analysis, we look at the items inside a single checkout. However, if we want to model lifecycle purchasing or churn behavior, we need an algorithm that natively understands time.</p> <p>In this cookbook, we will mine Sequential Patterns using <code>rusket</code>'s blazing fast PrefixSpan implementation over an e-commerce clickstream log.</p> <pre><code>import time\n\nimport pandas as pd\n\nfrom rusket import prefixspan, sequences_from_event_log\n</code></pre>"},{"location":"notebooks/3_ecommerce_lifecycle_mining/#1-the-e-commerce-event-log","title":"1. The E-Commerce Event Log","text":"<p>We start with a classic log of distinct user events over time. This could be page views, checkout events, or support tickets.</p> <pre><code>events = pd.DataFrame(\n    {\n        \"user_id\": [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n        \"timestamp\": [\n            \"2024-01-01 10:00\",\n            \"2024-01-05 10:05\",\n            \"2024-01-10 10:10\",\n            \"2024-01-02 11:00\",\n            \"2024-01-07 11:05\",\n            \"2024-01-03 09:00\",\n            \"2024-01-04 09:05\",\n            \"2024-01-09 09:10\",\n            \"2024-01-01 12:00\",\n            \"2024-01-08 12:00\",\n            \"2024-01-15 12:00\",\n        ],\n        \"event_name\": [\n            \"signup\",\n            \"view_product\",\n            \"add_to_cart\",\n            \"signup\",\n            \"view_product\",\n            \"signup\",\n            \"view_product\",\n            \"checkout\",\n            \"view_product\",\n            \"checkout\",\n            \"churn\",\n        ],\n    }\n)\n\n# Ensure correct temporal ordering\nevents[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"])\nevents.sort_values([\"user_id\", \"timestamp\"], inplace=True)\nevents.head()\n</code></pre> user_id timestamp event_name 0 1 2024-01-01 10:00:00 signup 1 1 2024-01-05 10:05:00 view_product 2 1 2024-01-10 10:10:00 add_to_cart 3 2 2024-01-02 11:00:00 signup 4 2 2024-01-07 11:05:00 view_product"},{"location":"notebooks/3_ecommerce_lifecycle_mining/#2-compiling-the-sequential-database","title":"2. Compiling the Sequential Database","text":"<p>Rusket requires data grouped into discrete sequential arrays of integers per user. We provide a <code>sequences_from_event_log</code> helper to automatically convert your Pandas DataFrame into this required zero-copy format.</p> <pre><code>sequences, label_mapping = sequences_from_event_log(\n    events, user_col=\"user_id\", time_col=\"timestamp\", item_col=\"event_name\"\n)\n\nprint(f\"Compiled {len(sequences)} distinct user sequences.\")\nprint(f\"Internal Mapping Table: {label_mapping}\")\n</code></pre> <pre><code>Compiled 4 distinct user sequences.\nInternal Mapping Table: {0: 'signup', 1: 'view_product', 2: 'add_to_cart', 3: 'checkout', 4: 'churn'}\n</code></pre>"},{"location":"notebooks/3_ecommerce_lifecycle_mining/#3-mining-sequential-patterns","title":"3. Mining Sequential Patterns","text":"<p>Now we pass our compiled sequences into the <code>prefixspan</code> model. We will ask for patterns that happen to at least 2 independent users.</p> <pre><code># Mine patterns\nt0 = time.time()\npatterns_df = prefixspan(sequences, min_support=2)\nprint(f\"Found {len(patterns_df)} sequential patterns in {time.time() - t0:.4f}s!\")\n\n# Restore the human-readable labels from our internal `label_mapping`\npatterns_df[\"event_path\"] = patterns_df[\"sequence\"].apply(lambda seq: \" \u2192 \".join([label_mapping[idx] for idx in seq]))\n\n# Display the most frequent sequences\npatterns_df.sort_values(\"support\", ascending=False)[[\"support\", \"event_path\"]]\n</code></pre> <pre><code>Found 5 sequential patterns in 0.0013s!\n</code></pre> support event_path 0 4 view_product 1 3 signup 2 3 signup \u2192 view_product 3 2 view_product \u2192 checkout 4 2 checkout <p>Using these sequential outputs, businesses can automatically map out the 'Happy Path' to <code>checkout</code> vs the 'Failure Path' leading to <code>churn</code>.</p>"}]}